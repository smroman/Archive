TY,T1,AU,JO,VL,SP,PY,DA,SN,DO,UR,KW,AB,EP,IS,T2,We,In
JOUR,Personalized sentiment classification of customer reviews via an interactive attributes attention model,"Zhang, You",Knowledge-Based Systems,226,107135,2021,2021/08/17/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107135,https://www.sciencedirect.com/science/article/pii/S0950705121003981,Sentiment analysis,"Incorporating extra attributes of customer reviews, such as user and product information, to align text representations to each attribute has improved the sentiment polarity classification performance. Existing works only treated such attributes separately thus ignored the interactive information between these attributes. In this paper, we proposed an interactive attributes attention model that considered all attributes to be relevant and investigated the interactive relationships in and across separate features to improve the sentiment classification performance for customer reviews. In addition to the local text encoder, three more interactive attribute encoders, including user–product, user–text, and product–text encoders, are applied to extract implicit information to align attribute features to text representations with a bilinear interaction instead of self-attention. To better integrate different information, a multiloss objective function is used to further improve the performance. The comparative experiments on the IMDB, Yelp, and Amazon datasets show that the proposed model achieves significant improvements in the effects of the bilinear interactions in and across attributes and local text features.",,,,,
,,"Wang, Jin",,,,,,,,,Text classification,,,,,,
,,"Zhang, Xuejie",,,,,,,,,Sentence representation,,,,,,
,,,,,,,,,,,Bilinear attention,,,,,,
JOUR,Multi-source aggregated classification for stock price movement prediction,"Ma, Yu",Information Fusion,91,515,2023,2023/03/01/,1566-2535,https://doi.org/10.1016/j.inffus.2022.10.025,https://www.sciencedirect.com/science/article/pii/S1566253522002019,Stock prediction,"Predicting stock price movements is a challenging task. Previous studies mostly used numerical features and news sentiments of target stocks to predict stock price movements. However, their semantics-based sentiment analysis is sub-optimal to represent real market sentiments. Moreover, only considering the information of target companies is insufficient because the stock prices of target companies can be affected by their related companies. Thus, we propose a novel Multi-source Aggregated Classification (MAC) method for stock price movement prediction. MAC incorporates the numerical features and market-driven news sentiments of target stocks, as well as the news sentiments of their related stocks. To better represent real market sentiments from the news, we pre-train an embedding feature generator by fitting the news to real stock price movements. Embeddings given by the pre-trained sentiment classifier can represent the sentiment information in vector space. Moreover, MAC introduces a graph convolutional network to capture the news effects of related companies on the target stock. Finally, MAC can predict stock price movements for the next trading day based on the aforementioned features. Extensive experiments prove that MAC outperforms state-of-the-art baselines in stock price movement prediction, Sharpe Ratio, and backtesting trading incomes.",528,,,,
,,"Mao, Rui",,,,,,,,,Event-driven investing,,,,,,
,,"Lin, Qika",,,,,,,,,Multi-source aggregating,,,,,,
,,"Wu, Peng",,,,,,,,,Sentiment analysis,,,,,,
,,"Cambria, Erik",,,,,,,,,,,,,,,
JOUR,TraceNet: Tracing and locating the key elements in sentiment analysis,"Zhao, Qinghua",Knowledge-Based Systems,277,110792,2023,2023/10/09/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110792,https://www.sciencedirect.com/science/article/pii/S0950705123005427,Language models,"We study sentiment analysis task where the outcomes are mainly contributed by a few key elements of the inputs. Motivated by the two-streams hypothesis, we explore processing input items and their weights separately by developing a neural architecture, named TraceNet, to address this type of task. It not only learns discriminative representations for the target task via its encoders, but also traces key elements at the same time via its locators. In TraceNet, both encoders and locators are organized in a layer-wise manner, and a smoothness regularization is employed between adjacent encoder-locator combinations. Moreover, a sparsity constraint is enforced on locators for tracing purposes and items are proactively masked according to the item weights output by locators. A major advantage of TraceNet is that the outcomes are easier to understand, as it identifies the key components responsible for the outcomes, making them easier to understand. The experimental results demonstrate its effectiveness in sentiment classification. Furthermore, we present case studies to showcase the interpretability of the model and conduct comprehensive analyses to highlight the impacts of each component.",,,,,
,,"Liu, Junfeng",,,,,,,,,Sentiment analysis,,,,,,
,,"Kang, Zhongfeng",,,,,,,,,Key items,,,,,,
,,"Zhou, Zenghui",,,,,,,,,Interpretability,,,,,,
JOUR,SKEAFN: Sentiment Knowledge Enhanced Attention Fusion Network for multimodal sentiment analysis,"Zhu, Chuanbo",Information Fusion,100,101958,2023,2023/12/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.101958,https://www.sciencedirect.com/science/article/pii/S1566253523002749,Multi-view learning,"Multimodal sentiment analysis is an active research field that aims to recognize the user’s sentiment information from multimodal data. The primary challenge in this field is to develop a high-quality fusion framework that effectively addresses the heterogeneity among different modalities. However, prior research has primarily concentrated on intermodal interactions while neglecting the semantic sentiment information conveyed by words in the text modality. In this paper, we propose the Sentiment Knowledge Enhanced Attention Fusion Network (SKEAFN), a novel end-to-end fusion network that enhances multimodal fusion by incorporating additional sentiment knowledge representations from an external knowledge base. Firstly, we construct an external knowledge enhancement module to acquire additional representations for the text modality. Then, we design a text-guided interaction module that facilitates the interaction between text and the visual/acoustic modality. Finally, we propose a feature-wised attention fusion module that achieves multimodal fusion by dynamically adjusting the weights of the additional and each modality’s representations. We evaluate our method on three challenging multimodal sentiment analysis datasets: CMU-MOSI, CMU-MOSEI, and Twitter2019. The experiment results demonstrate that our model significantly outperforms the state-of-the-art models. The source code is publicly available at https://github.com/doubibobo/SKEAFN.",,,,,
,,"Chen, Min",,,,,,,,,Multiple feature fusion,,,,,,
,,"Zhang, Sheng",,,,,,,,,Multimodal sentiment analysis,,,,,,
,,"Sun, Chao",,,,,,,,,External knowledge,,,,,,
,,"Liang, Han",,,,,,,,,Multi-head attention,,,,,,
,,"Liu, Yifan",,,,,,,,,,,,,,,
,,"Chen, Jincai",,,,,,,,,,,,,,,
JOUR,Enhancing aspect-category sentiment analysis via syntactic data augmentation and knowledge enhancement,"Liu, Bin",Knowledge-Based Systems,264,110339,2023,2023/03/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110339,https://www.sciencedirect.com/science/article/pii/S0950705123000898,Aspect-category sentiment analysis,"The goal of aspect-category sentiment analysis (ACSA) is to predict the sentiment polarity toward a specific aspect category from reviewers’ expressed opinions in a sentence. With the boom of pretrained language models, various relative methods have achieved significant improvements in ACSA. However, two major issues still remain to be solved. First, most of these studies usually follow the canonical method of fine-tuning on limited labeled data, neglecting to leverage external knowledge to further enhance ACSA performance. Second, aspect categories are usually abstract concepts that are mentioned explicitly or implicitly, and the corresponding different sentiment polarities are not easy to accurately recognize. To address these issues, we first transform the ACSA task into a sentence-pair classification task with natural language inference, constructing synthetic sentences as hypotheses based on the predefined aspect categories and the prompt-generation sentence template. Then the model applies a passivization transformation to the synthetic sentences and generates more syntactic data to augment the limited training data. Furthermore, we enhance ACSA with curated knowledge from a common sense knowledge graph. Finally, different representations are synergistically fused with a gating mechanism to output richer sentiment features and enable context-, syntax-, and knowledge-aware predictions. Experimental results on three challenging benchmark datasets show that the proposed model outperforms some competitive baselines.",,,,,
,,"Lin, Tao",,,,,,,,,Data augmentation,,,,,,
,,"Li, Ming",,,,,,,,,Natural language inference,,,,,,
,,,,,,,,,,,Pretrained language model,,,,,,
JOUR,Aspect-based sentiment analysis via multitask learning for online reviews,"Zhao, Guoshuai",Knowledge-Based Systems,264,110326,2023,2023/03/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110326,https://www.sciencedirect.com/science/article/pii/S095070512300076X,Aspect-based sentiment analysis,"Aspect based sentiment analysis(ABSA) aims to identify aspect terms in online reviews and predict their corresponding sentiment polarity. Sentiment analysis poses a challenging fine-grained task. Two typical subtasks are involved: Aspect Term Extraction (ATE) and Aspect Polarity Classification (APC). These two subtasks are usually trained discretely, which ignores the connection between ATE and APC. Concretely, we can relate ATE to APC through aspects and train them concurrently. We mainly use the ATE task as an auxiliary task, allowing the APC to focus more on relevant aspects to facilitate aspect polarity classification. In addition, previous studies have shown that utilizing dependency syntax information with a graph neural network (GNN) also contributes to the performance of the APC task. However, most studies directly input sentence dependency relations into graph neural networks without considering the influence of aspects, which do not emphasize the important dependency relationships. To address these issues, we propose a multitask learning model combining APC and ATE tasks that can extract aspect terms as well as classify aspect polarity simultaneously. Moreover, we exploit multihead attention(MHA) to associate dependency sequences with aspect extraction, which not only combines both ATE and APC tasks but also stresses the significant dependency relations, enabling the model to focus more on words closely related to aspects. According to our experiments on three benchmark datasets, we demonstrate that the connection between ATE and APC can be better established by our model, which enhances aspect polarity classification performance significantly. The source code has been released on GitHub https://github.com/winder-source/MTABSA.",,,,,
,,"Luo, Yiling",,,,,,,,,Aspect polarity classification,,,,,,
,,"Chen, Qiang",,,,,,,,,Aspect term extraction,,,,,,
,,"Qian, Xueming",,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Relational graph attention network,,,,,,
,,,,,,,,,,,Multi-head attention,,,,,,
JOUR,Leveraging hierarchical language models for aspect-based sentiment analysis on financial data,"Lengkeek, Matteo",Information Processing & Management,60,103435,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103435,https://www.sciencedirect.com/science/article/pii/S0306457323001723,Text data,"Every day millions of news articles and (micro)blogs that contain financial information are posted online. These documents often include insightful financial aspects with associated sentiments. In this paper, we predict financial aspect classes and their corresponding polarities (sentiment) within sentences. We use data from the Financial Question & Answering (FiQA) challenge, more precisely the aspect-based financial sentiment analysis task. We incorporate the hierarchical structure of the data by using the parent aspect class predictions to improve the child aspect class prediction (two-step model). Furthermore, we incorporate model output from the child aspect class prediction when predicting the polarity. We improve the F1 score by 7.6% using the two-step model for aspect classification over direct aspect classification in the test set. Furthermore, we improve the state-of-the-art test F1 score of the original aspect classification challenge from 0.46 to 0.70. The model that incorporates output from the child aspect classification performs up to par in polarity classification with our plain RoBERTa model. In addition, our plain RoBERTa model outperforms all the state-of-the-art models, lowering the MSE score by at least 28% and 33% for the cross-validation set and the test set, respectively.",,5,,,
,,"van der Knaap, Finn",,,,,,,,,Financial aspect classes,,,,,,
,,"Frasincar, Flavius",,,,,,,,,Polarity,,,,,,
,,,,,,,,,,,Hierarchical structure of data,,,,,,
JOUR,HOMOCHAR: A novel adversarial attack framework for exposing the vulnerability of text based neural sentiment classifiers,"Bajaj, Ashish",Engineering Applications of Artificial Intelligence,126,106815,2023,2023/11/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106815,https://www.sciencedirect.com/science/article/pii/S0952197623009995,Adversarial attack,"State-of-the-art deep learning algorithms have demonstrated remarkable proficiency in the task of text classification. Despite the widespread use of deep learning-based language models, there remains much work to be done in order to improve the security of these models. This is particularly concerning for their growing use in sensitive applications, such as sentiment analysis. This study demonstrates that language models possess inherent susceptibility to textual adversarial attacks, wherein a small number of words or characters are modified to produce an adversarial text that deceives the machine into producing erroneous predictions while maintaining its true meaning for human readers. The current study offers HOMOCHAR, a novel textual adversarial attack that operates within a black box setting. The proposed method generates more robust adversarial examples by considering the task of perturbing a text input with transformations at the character level. The objective is to deceive a target NLP model while adhering to specific linguistic constraints in a way such that the perturbations are imperceptible to humans. Comprehensive experiments are performed to assess the effectiveness of the proposed attack method against several popular models, including Word-CNN, Word-LSTM along with five powerful transformer models on two benchmark datasets, i.e., MR & IMDB utilized for sentiment analysis task. Empirical findings indicate that the proposed attack model consistently attains significantly greater attack success rates (ASR) and generates high-quality adversarial examples when compared to conventional methods. The results indicate that text-based sentiment prediction techniques can be circumvented, leading to potential consequences for existing policy measures.",,,,,
,,"Vishwakarma, Dinesh Kumar",,,,,,,,,Vulnerability,,,,,,
,,,,,,,,,,,Transformers,,,,,,
,,,,,,,,,,,Natural language processing) NLP,,,,,,
,,,,,,,,,,,Sentiment classification,,,,,,
JOUR,Quantum Fuzzy Neural Network for multimodal sentiment and sarcasm detection,"Tiwari, Prayag",Information Fusion,,102085,2023,2023/10/20/,1566-2535,https://doi.org/10.1016/j.inffus.2023.102085,https://www.sciencedirect.com/science/article/pii/S1566253523004013,Multimodal fusion,"Sentiment and sarcasm detection in social media language contribute to judging social opinion trends. Over the years, most artificial intelligence (AI) methods have used real values to characterize the sentimental and sarcastic features in language. These methods overlook the complexity and uncertainty of sentimental and sarcastic elements in human language. Therefore, this paper proposes the Quantum Fuzzy Neural Network (QFNN), a multimodal fusion and multitask learning algorithm with a Seq2Seq structure that combines Classical and Quantum Neural Networks. QFNN integrates fuzzy logic and Quantum Neural Networks. Complex numbers are used for the Fuzzifier to capture sentiment and sarcasm features, and Quantum Neural Networks are used for the Defuzzifier to get the predicted results. The experiments were conducted on classical computers and constructed quantum circuits in a simulated noisy environment. The results show the performance of QFNN in sarcasm and sentiment detection outperforms RCNN-RoBERTa and UPB-MTL on the Mustard and Memotion datasets. Moreover, by detecting the fidelity of quantum circuits in a noisy environment, QFNN was found to have excellent robustness.",,,,,
,,"Zhang, Lailei",,,,,,,,,Quantum Neural Networks,,,,,,
,,"Qu, Zhiguo",,,,,,,,,Sarcasm and sentiment detection,,,,,,
,,"Muhammad, Ghulam",,,,,,,,,Fuzzy logic,,,,,,
JOUR,BERT base model for toxic comment analysis on Indonesian social media,"Nabiilah, Ghinaa Zain",Procedia Computer Science,216,714,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2022.12.188,https://www.sciencedirect.com/science/article/pii/S1877050922022669,Toxic Comment,"Social media is an online media that functions as a platform for users to participate, share, create, and exchange information through various forums and social networks. The rapid increase in social media activity causes an increase in the number of comments on social media. This is prone to triggering debate due to the easy formation of open discussions between social media users. However, the debate often triggers the emergence of negative things, causing great fights on social media. Social media users often use comments containing toxic words to argue and corner a party or group. This study conducted an experiment to detect comments containing toxic sentences on social media in Indonesia using a Pre-Trained Model that was trained for Indonesian. This study performed a multilabel classification and evaluated the classification results generated by the Multilingual BERT (MBERT), IndoBERT, and Indo Roberta Small models. The optimal result of this study is to use the IndoBERT model with an F1 Score of 0.8897.",721,,7th International Conference on Computer Science and Computational Intelligence 2022,,
,,"Prasetyo, Simeon Yuda",,,,,,,,,MBERT,,,,,,
,,"Izdihar, Zahra Nabila",,,,,,,,,IndoBERT,,,,,,
,,"Girsang, Abba Suganda",,,,,,,,,IndoRoBERTa Small,,,,,,
,,,,,,,,,,,Indonesian social media,,,,,,
JOUR,Real-time social media sentiment analysis for rapid impact assessment of floods,"Bryan-Smith, Lydia",Computers & Geosciences,178,105405,2023,2023/09/01/,0098-3004,https://doi.org/10.1016/j.cageo.2023.105405,https://www.sciencedirect.com/science/article/pii/S0098300423001097,Social media,"Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.",,,,,
,,"Godsall, Jake",,,,,,,,,Sentiment analysis,,,,,,
,,"George, Franky",,,,,,,,,Flooding,,,,,,
,,"Egode, Kelly",,,,,,,,,Artificial Intelligence,,,,,,
,,"Dethlefs, Nina",,,,,,,,,,,,,,,
,,"Parsons, Dan",,,,,,,,,,,,,,,
JOUR,Comparison of neutrosophic approach to various deep learning models for sentiment analysis,"Sharma, Mayukh",Knowledge-Based Systems,223,107058,2021,2021/07/08/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107058,https://www.sciencedirect.com/science/article/pii/S095070512100321X,Neutrosophy,"Deep learning has been widely used in numerous real-world engineering applications and for classification problems. Real-world data is present with neutrality and indeterminacy, which neutrosophic theory captures clearly. Though both are currently developing research areas, there has been little study on their interlinking. We have proposed a novel framework to implement neutrosophy in deep learning models. Instead of just predicting a single class as output, we have quantified the sentiments using three membership functions to understand them better. Our proposed model consists of two blocks, feature extraction, and feature classification. Having a separate feature extraction block enables us to use any model as a feature extractor. We experimented with BiLSTM using GloVe (Global Vectors for word representation), BERT (Bidirectional Encoder Representations from Transformers), ALBERT (A Lite BERT), RoBERTa (Robustly optimized BERT approach), MPNet, and stacked ensemble models. Feature classification performs prediction and dimensionality reduction of features. Experimental analysis was done on the SemEval 2017 Task 4 dataset (Subtask A). We used the intermediate layer features to define membership functions of Single Valued Neutrosophic Sets (SVNS). We used these membership functions for prediction as well. We have compared our models with the top five teams of the task and recent state-of-the-art systems. Our proposed stacked ensemble model achieved the best recall (0.733) score.",,,,,
,,"Kandasamy, Ilanthenral",,,,,,,,,Sentiment analysis,,,,,,
,,"Vasantha, W.B.",,,,,,,,,BiLSTM,,,,,,
,,,,,,,,,,,ALBERT,,,,,,
,,,,,,,,,,,RoBERTa,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,MPNet,,,,,,
,,,,,,,,,,,Stacked ensemble,,,,,,
JOUR,Contrastive learning from label distribution: A case study on text classification,"Qian, Tao",Neurocomputing,507,208,2022,2022/10/01/,0925-2312,https://doi.org/10.1016/j.neucom.2022.07.076,https://www.sciencedirect.com/science/article/pii/S0925231222009523,Text classification,"State-of-the-art text classification models are dominated by deep neural networks, but they still struggle to the issue of poor generalization ability when using cross entropy loss for training. One of the reasons is the training samples are usually annotated with hard labels, ignoring the inter-label relationship among these labels. This results in the outputted distributions to violate the label-correlation. Although the widely used contrastive learning is able to learn highly-expressive feature representations that generalize well across the training and test sets, contrastive learning in the embedding space is difficult to model the label correlation and may still output unreasonable label distributions. In this paper, we suggest contrastive learning using label distributions and present a novel label-level contrastive learning (LLCL) paradigm, which can constrain the unreasonable label distributions from model outputs. We hypothesize that the label distributions of the instances in the same class are more similar than those from other classes. We introduce two label-level contrastive learning losses, namely supervised contrastive learning and self-supervised contrastive learning. After adding our proposed losses to the cross-entropy loss as regularizer for the training text classification model, our model obtains the average improvement of 0.74% over the strong RoBERTa-Large baseline on ten datasets. Particularly, our contrastive learning in the label space can effectively capture label correlations better than that in the embedding space, achieving an improvement of 6.2% in the top-2 accuracy on the SST-5 dataset. We also demonstrate that our proposed method is more effective, especially in the text classification tasks with a large label space or limited labeled data. Last but not least, our model does not rely on any kind of specialized architectures, data augmentation methods, or additional unsupervised data.",220,,,,
,,"Li, Fei",,,,,,,,,Contrastive learning,,,,,,
,,"Zhang, Meishan",,,,,,,,,Label distribution,,,,,,
,,"Jin, Guonian",,,,,,,,,Deep neural network,,,,,,
,,"Fan, Ping",,,,,,,,,,,,,,,
,,"Dai, Wenhua",,,,,,,,,,,,,,,
JOUR,WikiDes: A Wikipedia-based dataset for generating short descriptions from paragraphs,"Ta, Hoang Thang",Information Fusion,90,265,2023,2023/02/01/,1566-2535,https://doi.org/10.1016/j.inffus.2022.09.022,https://www.sciencedirect.com/science/article/pii/S1566253522001610,Text summarization,"As free online encyclopedias with massive volumes of content, Wikipedia and Wikidata are key to many Natural Language Processing (NLP) tasks, such as information retrieval, knowledge base building, machine translation, text classification, and text summarization. In this paper, we introduce WikiDes, a novel dataset to generate short descriptions of Wikipedia articles for the problem of text summarization. The dataset consists of over 80k English samples on 6987 topics. We set up a two-phase summarization method — description generation (Phase I) and candidate ranking (Phase II) — as a strong approach that relies on transfer and contrastive learning. For description generation, T5 and BART show their superiority compared to other small-scale pre-trained models. By applying contrastive learning with the diverse input from beam search, the metric fusion-based ranking models outperform the direct description generation models significantly up to ≈ 22 ROUGE in topic-exclusive split and topic-independent split. Furthermore, the outcome descriptions in Phase II are supported by human evaluation in over 45.33% chosen compared to 23.66% in Phase I against the gold descriptions. In the aspect of sentiment analysis, the generated descriptions cannot effectively capture all sentiment polarities from paragraphs while doing this task better from the gold descriptions. The automatic generation of new descriptions reduces the human efforts in creating them and enriches Wikidata-based knowledge graphs. Our paper shows a practical impact on Wikipedia and Wikidata since there are thousands of missing descriptions. Finally, we expect WikiDes to be a useful dataset for related works in capturing salient information from short paragraphs. The curated dataset is publicly available at: https://github.com/declare-lab/WikiDes.",282,,,,
,,"Rahman, Abu Bakar Siddiqur",,,,,,,,,Contrastive learning,,,,,,
,,"Majumder, Navonil",,,,,,,,,Sentiment analysis,,,,,,
,,"Hussain, Amir",,,,,,,,,Metric fusion,,,,,,
,,"Najjar, Lotfollah",,,,,,,,,Wikipedia,,,,,,
,,"Howard, Newton",,,,,,,,,Wikidata,,,,,,
,,"Poria, Soujanya",,,,,,,,,,,,,,,
,,"Gelbukh, Alexander",,,,,,,,,,,,,,,
JOUR,Crossmodal bipolar attention for multimodal classification on social media,"Cheung, Tsun-hin",Neurocomputing,514,1,2022,2022/12/01/,0925-2312,https://doi.org/10.1016/j.neucom.2022.09.140,https://www.sciencedirect.com/science/article/pii/S092523122201236X,Multimodal classification,"Multimodal classification of social media is used to classify data from different modalities into different categories, which is essential for understanding user behavior on the web. In this paper, we focus on classifying image-text pairs, specifically user-generated content on social media. Recently, the transformer network, a kind of self-attention network, has been widely studied in the disciplines of visual computing and language processing. In the attention mechanism, positive correlation is considered. However, multimedia content posted on social media is diverse. Images and text are not always consistent, and contrary information is also helpful for representation. Therefore, it is equally important to detect conflicts based on negative or inverse attention. Inspired by the attention mechanism, we propose a novel model, namely Crossmodal Bipolar Attention Network (CBAN). Different from existing positive dot-product and additive attention mechanisms, we propose a bipolar attention mechanism, which fuses visual and textual information through their direct and inverse semantic relationships to classify multimodal data. We conducted experiments on multiple multimodal classification data sets, for performing sentiment analysis, sarcasm detection, crisis categorization and hate-speech detection. Experimental results show that our proposed CBAN consistently outperforms state-of-the-art methods in all classification tasks.",12,,,,
,,"Lam, Kin-man",,,,,,,,,Attention mechanism,,,,,,
,,,,,,,,,,,Social media mining,,,,,,
JOUR,Unified benchmark for zero-shot Turkish text classification,"Çelik, Emrecan",Information Processing & Management,60,103298,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103298,https://www.sciencedirect.com/science/article/pii/S0306457323000353,Text classification,"Effective learning schemes such as fine-tuning, zero-shot, and few-shot learning, have been widely used to obtain considerable performance with only a handful of annotated training data. In this paper, we presented a unified benchmark to facilitate the problem of zero-shot text classification in Turkish. For this purpose, we evaluated three methods, namely, Natural Language Inference, Next Sentence Prediction and our proposed model that is based on Masked Language Modeling and pre-trained word embeddings on nine Turkish datasets for three main categories: topic, sentiment, and emotion. We used pre-trained Turkish monolingual and multilingual transformer models which can be listed as BERT, ConvBERT, DistilBERT and mBERT. The results showed that ConvBERT with the NLI method yields the best results with 79% and outperforms previously used multilingual XLM-RoBERTa model by 19.6%. The study contributes to the literature using different and unattempted transformer models for Turkish and showing improvement of zero-shot text classification performance for monolingual models over multilingual models.",,3,,,
,,"Dalyan, Tuğba",,,,,,,,,Zero-shot learning,,,,,,
,,,,,,,,,,,Next sentence prediction,,,,,,
,,,,,,,,,,,Natural language inference,,,,,,
,,,,,,,,,,,Masked language modeling,,,,,,
JOUR,ASK-RoBERTa: A pretraining model for aspect-based sentiment classification via sentiment knowledge mining,"You, Lan",Knowledge-Based Systems,253,109511,2022,2022/10/11/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109511,https://www.sciencedirect.com/science/article/pii/S0950705122007584,Aspect-based sentiment classification,"The main objective of aspect-based sentiment classification (ABSC) is to predict sentiment polarities of different aspects from sentences or documents. Recent research integrates sentiment terms into pretraining models whose accuracy impacts the ABSC performance. This paper introduces a sentiment knowledge-adaptive pretraining model (ASK-RoBERTa). A sentiment word dictionary is first built from general and field sentiment words. We develop a series of term and sentiment mining rules based on part-of-speech tagging and sentence dependency grammar. These mining rules consider word dependencies, compounding, and conjunctions. The pretraining model optimizes the mining rules to capture the dependency between aspects and sentiment words. Experimental results on multiple public benchmark datasets demonstrate the satisfactory performance of ASK-RoBERTa.",,,,,
,,"Han, Fanyu",,,,,,,,,RoBERTa,,,,,,
,,"Peng, Jiaheng",,,,,,,,,Sentiment knowledge,,,,,,
,,"Jin, Hong",,,,,,,,,Dependency grammar,,,,,,
,,"Claramunt, Christophe",,,,,,,,,Knowledge mining,,,,,,
JOUR,"A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations","Zhang, Yazhou",Information Fusion,93,282,2023,2023/05/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.01.005,https://www.sciencedirect.com/science/article/pii/S1566253523000040,Multimodal sarcasm recognition,"Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address the three problems with a multimodal joint framework. We thus propose a multimodal multitask learning model based on the encoder–decoder architecture, termed M2Seq2Seq. At the heart of the encoder module are two attention mechanisms, i.e., intramodal (Ia) attention and intermodal (Ie) attention. Ia attention is designed to capture the contextual dependency between adjacent utterances, while Ie attention is designed to model multimodal interactions. In contrast, we design two kinds of multitask learning (MTL) decoders, i.e., single-level and multilevel decoders, to explore their potential. More specifically, the core of a single-level decoder is a masked outer-modal (Or) self-attention mechanism. The main motivation of Or attention is to explicitly model the interdependence among the tasks of sarcasm, sentiment and emotion recognition. The core of the multilevel decoder contains the shared gating and task-specific gating networks. Comprehensive experiments on four bench datasets, MUStARD, Memotion, CMU-MOSEI and MELD, prove the effectiveness of M2Seq2Seq over state-of-the-art baselines (e.g., CM-GCN, A-MTL) with significant improvements of 1.9%, 2.0%, 5.0%, 0.8%, 4.3%, 3.1%, 2.8%, 1.0%, 1.7% and 2.8% in terms of Micro F1.",301,,,,
,,"Wang, Jinglin",,,,,,,,,Sentiment analysis,,,,,,
,,"Liu, Yaochen",,,,,,,,,Emotion recognition,,,,,,
,,"Rong, Lu",,,,,,,,,Multitask learning,,,,,,
,,"Zheng, Qian",,,,,,,,,Affective computing,,,,,,
,,"Song, Dawei",,,,,,,,,,,,,,,
,,"Tiwari, Prayag",,,,,,,,,,,,,,,
,,"Qin, Jing",,,,,,,,,,,,,,,
JOUR,D-NEXUS: Defending text networks using summarization,"Gupta, Anup Kumar",Electronic Commerce Research and Applications,54,101171,2022,2022/07/01/,1567-4223,https://doi.org/10.1016/j.elerap.2022.101171,https://www.sciencedirect.com/science/article/pii/S1567422322000552,Sentiment analysis,"Sentiment analysis is an important tool for understanding consumer sentiment in e-commerce platforms. Usually, it is performed using Deep Neural Networks (DNNs), owing to their strong predictive and generalization capabilities. Unfortunately, DNNs are prone to adversarial attacks, which involve introducing imperceptible changes in the data with the deliberate motive of “fooling” the target model. It can lead to far-reaching consequences and pose an alarming issue to the credibility of e-commerce platforms. The existing text-based defenses, such as spelling correction and adversarial training, are largely ineffective against state-of-the-art adversarial attacks, most of which deal in word replacement, insertion, and deletion. We introduce an effective transformation-based defense strategy, D-NEXUS (DefeNding tEXt networks Using Summarization). It overcomes the drawbacks of existing defenses by summarising the input text before feeding it into the target model. Our extensive experiments on publicly available datasets show that D-NEXUS successfully defends against state-of-the-art attacks, in a time-efficient manner.",,,,,
,,"Rastogi, Aryan",,,,,,,,,Natural language processing,,,,,,
,,"Paliwal, Vardhan",,,,,,,,,Adversarial defense,,,,,,
,,"Nassar, Fyse",,,,,,,,,Transformers,,,,,,
,,"Gupta, Puneet",,,,,,,,,Adversarial attack,,,,,,
,,,,,,,,,,,Language summarization,,,,,,
JOUR,Towards sentiment and Temporal Aided Stance Detection of climate change tweets,"Upadhyaya, Apoorva",Information Processing & Management,60,103325,2023,2023/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103325,https://www.sciencedirect.com/science/article/pii/S0306457323000626,Climate change,"Climate change has become one of the most significant crises of our time. Public opinion on climate change is influenced by social media platforms such as Twitter, often divided into believers and deniers. In this paper, we propose a framework to classify a tweet’s stance on climate change (denier/believer). Existing approaches to stance detection and classification of climate change tweets either have paid little attention to the characteristics of deniers’ tweets or often lack an appropriate architecture. However, the relevant literature reveals that the sentimental aspects and time perspective of climate change conversations on Twitter have a major impact on public attitudes and environmental orientation. Therefore, in our study, we focus on exploring the role of temporal orientation and sentiment analysis (auxiliary tasks) in detecting the attitude of tweets on climate change (main task). Our proposed framework STASY integrates word- and sentence-based feature encoders with the intra-task and shared-private attention frameworks to better encode the interactions between task-specific and shared features. We conducted our experiments on our novel curated climate change CLiCS dataset (2465 denier and 7235 believer tweets), two publicly available climate change datasets (ClimateICWSM-2022 and ClimateStance-2022), and two benchmark stance detection datasets (SemEval-2016 and COVID-19-Stance). Experiments show that our proposed approach improves stance detection performance (with an average improvement of 12.14% on our climate change dataset, 15.18% on ClimateICWSM-2022, 12.94% on ClimateStance-2022, 19.38% on SemEval-2016, and 35.01% on COVID-19-Stance in terms of average F1 scores) by benefiting from the auxiliary tasks compared to the baseline methods.",,4,,,
,,"Fisichella, Marco",,,,,,,,,Stance detection,,,,,,
,,"Nejdl, Wolfgang",,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Temporal orientation,,,,,,
JOUR,Sentiment Analysis of Customer Feedback and Reviews for Airline Services using Language Representation Model,"PATEL, AKSH",Procedia Computer Science,218,2459,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2023.01.221,https://www.sciencedirect.com/science/article/pii/S1877050923002211,Sentiment Analysis,"The competitive airline sector has grown at a breakneck pace in the last two decades. A useful source for collecting consumer feedback and performing various forms of analysis on it is proper data collection. This collection of data can be used for sentiment analysis. Sentiment analysis is a type of analysis that involves extracting sentiment to find attitudes and emotions associated with the text or data supplied. It's a classification approach in which machine learning techniques are used to identify positive and negative words or reviews in text-driven databases. Further to explain the reasons for negative comments, a word cloud and a bar graph are used. Sentiment analysis is used to analyze the Airline reviews dataset in this paper. To test the performance of sentiment analysis, many Machine Learning (ML) algorithms have been utilized, such as Naive Bayes, Support Vector Machine, and Decision Tree (DT), and each of these approaches has produced distinct results. The performance of Google's BERT algorithm has been evaluated to that of other machine learning algorithms in our research. Furthermore, this paper explores the Bert architecture, which has been pre-trained on two NLP tasks: Masked language modeling and Sentence prediction. The ”Random Forest” is used as a baseline against which the results of the ”BERT Model” are compared because its performance is the best among the machine learning models. In terms of performance criteria such as accuracy, precision, recall, and F1-score, it is discovered that BERT outperformed the other ML techniques.",2467,,International Conference on Machine Learning and Data Engineering,,
,,"OZA, PARITA",,,,,,,,,BERT,,,,,,
,,"AGRAWAL, SMITA",,,,,,,,,Classification,,,,,,
,,,,,,,,,,,Review,,,,,,
,,,,,,,,,,,Machine Learning,,,,,,
JOUR,A framework for investigating the dynamics of user and community sentiments in a social platform,"Bonifazi, Gianluca",Data & Knowledge Engineering,146,102183,2023,2023/07/01/,0169-023X,https://doi.org/10.1016/j.datak.2023.102183,https://www.sciencedirect.com/science/article/pii/S0169023X23000435,Social platforms,"Social platforms are the preferred medium for many people to express their opinions on many topics. This has led many professionals from various fields (marketing, politics, research and development, etc.) to demand increasingly advanced approaches capable of analyzing the evolution of user or community sentiments on particular topics. In this paper, we want to make a contribution to addressing this issue. Specifically, we propose a model and a framework to analyze the dynamics of user and community sentiments in a social platform. In particular, our framework currently focuses on three activities, namely: (i) finding users capable of creating and maintaining a community that reflects their sentiment on a topic; (ii) studying how a user or community sentiment on a topic evolves over time; and (iii) investigating the cross-contamination between a user community and its neighborhood. We tested our framework by means of an extensive experimental campaign that we describe in the paper. Our framework is extremely scalable, and further activities can be easily implemented in it in the near future.",,,,,
,,"Cauteruccio, Francesco",,,,,,,,,User and community sentiments,,,,,,
,,"Corradini, Enrico",,,,,,,,,Model and framework for sentiment evolution,,,,,,
,,"Marchetti, Michele",,,,,,,,,Cross-contamination of users,,,,,,
,,"Terracina, Giorgio",,,,,,,,,,,,,,,
,,"Ursino, Domenico",,,,,,,,,,,,,,,
,,"Virgili, Luca",,,,,,,,,,,,,,,
JOUR,Towards comparable ratings: Exploring bias in German physician reviews,"Kersting, Joschka",Data & Knowledge Engineering,148,102235,2023,2023/11/01/,0169-023X,https://doi.org/10.1016/j.datak.2023.102235,https://www.sciencedirect.com/science/article/pii/S0169023X23000952,Language model fairness,"In this study, we evaluate the impact of gender-biased data from German-language physician reviews on the fairness of fine-tuned language models. For two different downstream tasks, we use data reported to be gender biased and aggregate it with annotations. First, we propose a new approach to aspect-based sentiment analysis that allows identifying, extracting, and classifying implicit and explicit aspect phrases and their polarity within a single model. The second task we present is grade prediction, where we predict the overall grade of a review on the basis of the review text. For both tasks, we train numerous transformer models and evaluate their performance. The aggregation of sensitive attributes, such as a physician’s gender and migration background, with individual text reviews allows us to measure the performance of the models with respect to these sensitive groups. These group-wise performance measures act as extrinsic bias measures for our downstream tasks. In addition, we translate several gender-specific templates of the intrinsic bias metrics into the German language and evaluate our fine-tuned models. Based on this set of tasks, fine-tuned models, and intrinsic and extrinsic bias measures, we perform correlation analyses between intrinsic and extrinsic bias measures. In terms of sensitive groups and effect sizes, our bias measure results show different directions. Furthermore, correlations between measures of intrinsic and extrinsic bias can be observed in different directions. This leads us to conclude that gender-biased data does not inherently lead to biased models. Other variables, such as template dependency for intrinsic measures and label distribution in the data, must be taken into account as they strongly influence the metric results. Therefore, we suggest that metrics and templates should be chosen according to the given task and the biases to be assessed.",,,,,
,,"Maoro, Falk",,,,,,,,,Aspect phrase classification,,,,,,
,,"Geierhos, Michaela",,,,,,,,,Grade prediction,,,,,,
,,,,,,,,,,,Physician reviews,,,,,,
JOUR,Financial sentiment classification with fresh and hot public opinions,"Cao, Shiyang",Computers and Electrical Engineering,111,108955,2023,2023/11/01/,0045-7906,https://doi.org/10.1016/j.compeleceng.2023.108955,https://www.sciencedirect.com/science/article/pii/S0045790623003798,Financial sentiment analysis,"Financial sentiment analysis aims to extract public opinion about an institution to help financial researchers make better decisions. To predict sentiment more accurately, it is necessary for models to improve their capability to capture long-term temporal information and support multi-user interaction. However, existing methods only analyze sentiment based on one comment from a user, which fails to fully exploit the latent emotions of the public, and they lack effective temporal modeling and interaction capabilities. In this paper, we analyze a company from two perspectives to alleviate the above issues: (1) the fresh opinions can reflect timely public attitudes towards a company, while (2) the hot opinions provide the most influential views. A comprehensive exploration of fresh and hot financial sentiment can help researchers make more accurate determinations. To this end, we propose a novel financial sentiment classification framework (FSCN), that can capture temporal information and interact with the opinions of users to make a more comprehensive decision. Our approach takes into account the inherent temporal dependencies in public opinions and combines both views of information to achieve an accurate classification of financial sentiment. Specifically, the FSCN contains (1) a multi-opinion extractor to filter and extract features from massively fresh and hot opinions, respectively. (2) a fresh-hot bilinear pooling (FHBP) module to effectively fuse fresh and hot features. Additionally, to verify the effectiveness of the proposed method, we crawl data from the Internet and create a real-world public opinion dataset that consists of 79,350 comments from 837 companies. Extensive experiments demonstrate that our framework achieves state-of-the-art results on this real-world dataset and is capable of providing reliable service in the financial system. Codes will be released at https://github.com/zjfgh2015/FSCN.",,,,,
,,"Ma, Xiao",,,,,,,,,Fresh and hot opinions,,,,,,
,,"Zeng, Jiangfeng",,,,,,,,,Temporal modeling,,,,,,
,,"Yi, Ming",,,,,,,,,Fresh-hot bilinear pooling,,,,,,
JOUR,A prompt model with combined semantic refinement for aspect sentiment analysis,"Zhu, Xinhua",Information Processing & Management,60,103462,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103462,https://www.sciencedirect.com/science/article/pii/S0306457323001991,Aspect-based sentiment analysis,"Recently, pre-trained language models (PLMs), especially pre-trained bidirectional encoder representations from transformers (BERT), have improved the performance of aspect-based sentiment analysis (ABSA) tasks to some extent. However, due to the imbalance of training data in different polarities, the following shortcomings remain in PLM-based ABSA methods: (1) for small corpus scenarios with polarized emotions, an unbalanced performance problem exists; and (2) for delicate and obscure scenes dominated by neutral emotions, PLM-based performance gains are limited. To address these shortcomings, we use BERT as an instance of PLMs to propose a general-purpose prompt model with combined semantic refinement for ABSA. First, we utilize a BERT without fine-tuning to automatically induce prompts for various ABSA datasets to enhance the adaptability of the model to different application scenarios. We then leverage multi-prompt learning to propose a data augmentation method to address the imbalance of training data in different polarities. Moreover, to further deepen the model's understanding and analysis of reviews with prompts, we also propose an improved BERT semantic refinement method that combines global semantic refinement and local semantic extraction. Experiments on five public datasets show that compared with existing methods, our macro-average F1 improvement is over 10% on polarized small datasets and over 7% on an emotionally delicate and obscure dataset.",,5,,,
,,"Kuang, Zhongjie",,,,,,,,,Pre-trained language models,,,,,,
,,"Zhang, Lanfang",,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Prompt learning,,,,,,
,,,,,,,,,,,Semantic refinement,,,,,,
,,,,,,,,,,,Data augmentation,,,,,,
JOUR,A cognitive emotion model enhanced sequential method for social emotion cause identification,"Xiao, Xinglin",Information Processing & Management,60,103305,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103305,https://www.sciencedirect.com/science/article/pii/S0306457323000420,Social emotion mining,"Social emotion refers to the emotion evoked to the reader by a textual document. In contrast to the emotion cause extraction task which analyzes the cause of the author's sentiments based on the expressions in text, identifying the causes of social emotion evoked to the reader from text has not been explored previously. Social emotion mining and its cause analysis is not only an important research topic in Web-based social media analytics and text mining but also has a number of applications in multiple domains. As the focus of social emotion cause identification is on analyzing the causes of the reader's emotions elicited by a text that are not explicitly or implicitly expressed, it is a challenging task fundamentally different from the previous research. To tackle this, it also needs a deeper level understanding of the cognitive process underlying the inference of social emotion and its cause analysis. In this paper, we propose the new task of social emotion cause identification (SECI). Inspired by the cognitive structure of emotions (OCC) theory, we present a Cognitive Emotion model Enhanced Sequential (CogEES) method for SECI. Specifically, based on the implications of the OCC model, our method first establishes the correspondence between words/phrases in text and emotional dimensions identified in OCC and builds the emotional dimension lexicons with 1,676 distinct words/phrases. Then, our method utilizes lexicons information and discourse coherence for the semantic segmentation of document and the enhancement of clause representation learning. Finally, our method combines text segmentation and clause representation into a sequential model for cause clause prediction. We construct the SECI dataset for this new task and conduct experiments to evaluate CogEES. Our method outperforms the baselines and achieves over 10% F1 improvement on average, with better interpretability of the prediction results.",,3,,,
,,"Mao, Wenji",,,,,,,,,Cause identification,,,,,,
,,"Sun, Ying",,,,,,,,,Cognitive structure of emotions model,,,,,,
,,"Zeng, Daniel",,,,,,,,,Emotional dimension lexicon,,,,,,
,,,,,,,,,,,Discourse unit,,,,,,
,,,,,,,,,,,Sequential neural network,,,,,,
JOUR,Sentiment analysis tools in software engineering: A systematic mapping study,"Obaidi, Martin",Information and Software Technology,151,107018,2022,2022/11/01/,0950-5849,https://doi.org/10.1016/j.infsof.2022.107018,https://www.sciencedirect.com/science/article/pii/S0950584922001422,Social software engineering,Context:,,,,"aim to help developers or stakeholders in their choice of sentiment analysis tools for their specific purpose. Therefore, we conducted a systematic mapping study (SMS).",
,,"Nagel, Lukas",,,,,,,,,Sentiment analysis,"Software development is a collaborative task. Previous research has shown social aspects within development teams to be highly relevant for the success of software projects. A team’s mood has been proven to be particularly important. It is paramount for project managers to be aware of negative moods within their teams, as such awareness enables them to intervene. Sentiment analysis tools offer a way to determine the mood of a team based on textual communication.",,,,Methods:,
,,"Specht, Alexander",,,,,,,,,Machine learning,Objective:,,,,"present the results of our SMS of sentiment analysis tools developed for or applied in the context of software engineering (SE). Our results summarize insights from 106 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools, (5) the usage of already existing tools, and (6) the difficulties researchers face. We analyzed in more detail which tools and approaches perform how in terms of their performance.",
,,"Klünder, Jil",,,,,,,,,Systematic mapping study,,,,,Results:,
,,,,,,,,,,,,,,,,"According to our results, sentiment analysis is frequently applied to open-source software projects, and most approaches are neural networks or support-vector machines. The best performing approach in our analysis is neural networks and the best tool is BERT. Despite the frequent use of sentiment analysis in SE, there are open issues, e.g. regarding the identification of irony or sarcasm, pointing to future research directions.",
,,,,,,,,,,,,,,,,Conclusion:,
,,,,,,,,,,,,,,,,conducted an SMS to gain an overview of the current state of sentiment analysis in order to help developers or stakeholders in this matter. Our results include interesting findings e.g. on the used tools and their difficulties. We present several suggestions on how to solve these identified problems.,
JOUR,Content still matters. A machine learning model for predicting news longevity from textual and context features,"Rybinski, Krzysztof",Information Processing & Management,60,103398,2023,2023/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103398,https://www.sciencedirect.com/science/article/pii/S0306457323001358,News longevity,"There is an ongoing debate about what is more important in the modern online media newsroom, whether it is the news content and worthiness, or the audience clicks. Using a dataset of over one million articles from five countries (Belarus, Kazakhstan, Poland, Russia, and Ukraine) and a novel machine learning methodology, I demonstrate that the content of news articles has a significant impact on their lifespan. My findings show that articles with positive sentiment tend to be displayed longer, and that high fear emotion scores can extend the lifespan of news articles in autocratic regimes, and the impact is substantial in magnitude. This paper proposes four new methods for improving information management methodology: a flexible version of Latent Dirichlet Allocation (LDA), a technique for performing relative sentiment analysis, a method for determining semantic similarity between a news article and a newspaper's dominant narrative, and a novel approach to unsupervised model validation based on inter-feature consistency.",,4,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Emotion detection,,,,,,
,,,,,,,,,,,Topic modelling,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
JOUR,An intelligent conversational agent for educating the general public about HIV,"Moreno, Joan C.",Neurocomputing,563,126902,2024,2024/01/01/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126902,https://www.sciencedirect.com/science/article/pii/S0925231223010251,Conversational agent,"The article presents a Spanish conversational agent that focuses on raising awareness about HIV. The agent aims to provide natural communication, personalized information based on user requests, and a centralized source of information about HIV. The core of the agent’s logic is formed by a natural language understanding conversational model, supported by a knowledge base of medical responses and real conversations with users. An empirical study was conducted with 71 users to evaluate the agent’s effectiveness as a sexual health educational tool. The results show that HIV knowledge raised by 18.44% after using the agent. That, and the positive user experience support the agent’s role as a tool for raising HIV prevention and awareness.",,,,,
,,"Sánchez-Anguix, Victor",,,,,,,,,Natural language understanding,,,,,,
,,"Alberola, Juan M.",,,,,,,,,HIV,,,,,,
,,"Julián, Vicente",,,,,,,,,Health informatics,,,,,,
,,"Botti, Vicent",,,,,,,,,Human–computer interaction,,,,,,
,,,,,,,,,,,Empirical study,,,,,,
JOUR,Deep neural networks and weighted word embeddings for sentiment analysis of drug product reviews,"Suhartono, Derwin",Procedia Computer Science,216,664,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2022.12.182,https://www.sciencedirect.com/science/article/pii/S1877050922022608,transformers,"One of the major tasks of natural language processing is sentiment analysis. The web is a source of unstructured and rich informa-tion with thousands of opinions and reviews. Individuals, businesses, and governments can all benefit from recognizing sentiment. As part of this study, we propose a deep learning-based approach for sentiment analysis on drug product review data obtained from the UCI machine learning repository. As an alternative to deep learning models, this architecture integrates glove word embedding with convolutional neural networks (CNN). Word2vec and GloVe word embedding schemes have been evaluated empirically for their predictive performance in CNN architectures. Based on a comparison of the deep learning architecture with RoBERTa, itcan be seen that BERT architecture outperforms both of them in training and validation. However, CNN models using Glove word embedding provided superior results in testing.",671,,7th International Conference on Computer Science and Computational Intelligence 2022,,
,,"Purwandari, Kartika",,,,,,,,,machine learning,,,,,,
,,"Jeremy, Nicholaus Hendrik",,,,,,,,,sentiment analysis,,,,,,
,,"Philip, Samuel",,,,,,,,,weighted word embeddings,,,,,,
,,"Arisaputra, Panji",,,,,,,,,,,,,,,
,,"Parmonangan, Ivan Halim",,,,,,,,,,,,,,,
JOUR,Text-based neural networks for question intent recognition,"Trewhela, Alvaro",Engineering Applications of Artificial Intelligence,121,105933,2023,2023/05/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.105933,https://www.sciencedirect.com/science/article/pii/S0952197623001173,Question intent,"An effective organization of their knowledge bases is pivotal in keeping social networks vibrant, namely in designing successful personalization and contextualization strategies. This way, enhancing dedicated displays and encouraging the production of better content. Particularly for question answering communities, splitting archived material according to their intent is essential to reuse their knowledge and social capital. Recently, deep neural networks have shown breakthrough capabilities on multiple tasks related to language understanding. Thus the main contribution of this work is a thorough comparison of assorted architectures applied to the detection of question intents (i.e., informational and conversational). Evaluated on two collections, DEBERTA and RoBERTa demonstrated to be the best options by finishing with an accuracy of 71.19% and 74.10%, respectively. As for conventional neural networks, RCNNs proven to be the most effective technique. Overall, best models signal the usefulness of both question titles and bodies, and that fusing diverse learning strategies hold promise since they focus on learning different discriminative patterns.",,,,,
,,"Figueroa, Alejandro",,,,,,,,,Question analysis,,,,,,
,,,,,,,,,,,Community question answering,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Pre-trained models,,,,,,
JOUR,Finding hate speech with auxiliary emotion detection from self-training multi-label learning perspective,"Min, Changrong",Information Fusion,96,214,2023,2023/08/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.03.015,https://www.sciencedirect.com/science/article/pii/S1566253523001045,Hate speech detection,"Hate Speech Detection (HSD) aims to identify whether a text contains hate speech content, which often refers to discrimination and is even associated with a hate crime. The mainstream methods jointly train the HSD problem with relevant auxiliary problems, e.g., emotion detection and sentiment analysis, under the paradigm of Multi-Task Learning (MTL). In this paper, we improve HSD by integrating it with emotion detection, since we take inspiration from the potential correlations between hate speech and certain negative emotion states, which have been studied theoretically and empirically. To be specific, we can concatenate their hateful labels and predicted emotion states as pseudo-multiple labels for hate speech samples, formulating a pseudo-Multi-Label Learning (MLL) problem. Beyond the existing MTL-HSD methods, we further incorporate this pseudo-MLL problem and solve it by capturing the correlations between hate speech and negative emotion states, so as to improve the performance of HSD. Based on these ideas, we propose a novel HSD method named the Emotion-correlated Hate Speech DetectOR (EHSor). We conduct extensive experiments to evaluate EHSor, and the results show that it can consistently outperform the existing HSD methods across benchmark datasets.",223,,,,
,,"Lin, Hongfei",,,,,,,,,Emotion detection,,,,,,
,,"Li, Ximing",,,,,,,,,Multi-label learning,,,,,,
,,"Zhao, He",,,,,,,,,Multi-task learning,,,,,,
,,"Lu, Junyu",,,,,,,,,,,,,,,
,,"Yang, Liang",,,,,,,,,,,,,,,
,,"Xu, Bo",,,,,,,,,,,,,,,
JOUR,PyFin-sentiment: Towards a machine-learning-based model for deriving sentiment from financial tweets,"Wilksch, Moritz",International Journal of Information Management Data Insights,3,100171,2023,2023/04/01/,2667-0968,https://doi.org/10.1016/j.jjimei.2023.100171,https://www.sciencedirect.com/science/article/pii/S2667096823000186,Sentiment analysis,"Responding to the poor performance of generic automated sentiment analysis solutions on domain-specific texts, we collect a dataset of 10,000 tweets discussing the topics of finance and investing. We manually assign each tweet its market sentiment, i.e., the investor’s anticipation of a stock’s future return. Using this data, we show that all existing sentiment models trained on adjacent domains struggle with accurate market sentiment analysis due to the task’s specialized vocabulary. Consequently, we design, train, and deploy our own sentiment model. It outperforms all previous models (VADER, NTUSD-Fin, FinBERT, TwitterRoBERTa) when evaluated on Twitter posts. On posts from a different platform, our model performs on par with BERT-based large language models. We achieve this result at a fraction of the training and inference costs due to the model’s simple design. We publish the artifact as a python library to facilitate its use by future researchers and practitioners.",,1,,,
,,"Abramova, Olga",,,,,,,,,Financial market sentiment,,,,,,
,,,,,,,,,,,Opinion mining,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
JOUR,User Sentiment Analysis in Conversational Systems Based on Augmentation and Attention-based BiLSTM,"Jbene, Mourad",Procedia Computer Science,207,4106,2022,2022/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2022.09.473,https://www.sciencedirect.com/science/article/pii/S1877050922013680,Sentiment analysis,"Conversational Systems are increasingly substituting humans in many service industries. They aim to provide human-like interaction with users for task completion or chitchat in a conversation style. User sentiment analysis is an important task that can help better understand users’ behavior and satisfaction in conversations. Although some researchers have studied the problem of sentiment analysis, most of the existing methods are oriented toward general felds. To overcome the challenges of sentiment analysis, we propose a BE-Att-BiLSTM, which stands for an attention-based Bidirectional Long Short-Term Memory (BiLSTM) model. The proposed model uses pre-trained BERT, contextual embeddings and a combination of BiLSTM and attention mechanism for efficient sentiment analysis in conversations. In addition, text-augmentation techniques are leveraged to enhance the performance of the proposed model. Experimental results on a public benchmark dataset show an improved accuracy of 68.00% and an F1-score of 67.50%.",4112,,Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022,,
,,"raif, Mourad",,,,,,,,,Conversational systems,,,,,,
,,"Tigani, Smail",,,,,,,,,Recurrent neural network,,,,,,
,,"Chehri, Abdellah",,,,,,,,,text-augmentation,,,,,,
,,"Saadane, Rachid",,,,,,,,,,,,,,,
JOUR,ABCDM: An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis,"Basiri, Mohammad Ehsan",Future Generation Computer Systems,115,279,2021,2021/02/01/,0167-739X,https://doi.org/10.1016/j.future.2020.08.005,https://www.sciencedirect.com/science/article/pii/S0167739X20309195,Sentiment analysis,"Sentiment analysis has been a hot research topic in natural language processing and data mining fields in the last decade. Recently, deep neural network (DNN) models are being applied to sentiment analysis tasks to obtain promising results. Among various neural architectures applied for sentiment analysis, long short-term memory (LSTM) models and its variants such as gated recurrent unit (GRU) have attracted increasing attention. Although these models are capable of processing sequences of arbitrary length, using them in the feature extraction layer of a DNN makes the feature space high dimensional. Another drawback of such models is that they consider different features equally important. To address these problems, we propose an Attention-based Bidirectional CNN-RNN Deep Model (ABCDM). By utilizing two independent bidirectional LSTM and GRU layers, ABCDM will extract both past and future contexts by considering temporal information flow in both directions. Also, the attention mechanism is applied on the outputs of bidirectional layers of ABCDM to put more or less emphasis on different words. To reduce the dimensionality of features and extract position-invariant local features, ABCDM utilizes convolution and pooling mechanisms. The effectiveness of ABCDM is evaluated on sentiment polarity detection which is the most common and essential task of sentiment analysis. Experiments were conducted on five review and three Twitter datasets. The results of comparing ABCDM with six recently proposed DNNs for sentiment analysis show that ABCDM achieves state-of-the-art results on both long review and short tweet polarity classification.",294,,,,
,,"Nemati, Shahla",,,,,,,,,Deep learning,,,,,,
,,"Abdar, Moloud",,,,,,,,,Convolutional neural network,,,,,,
,,"Cambria, Erik",,,,,,,,,Long short-term memory,,,,,,
,,"Acharya, U. Rajendra",,,,,,,,,Attention mechanism,,,,,,
JOUR,AsU-OSum: Aspect-augmented unsupervised opinion summarization,"Zhang, Mengli",Information Processing & Management,60,103138,2023,2023/01/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103138,https://www.sciencedirect.com/science/article/pii/S0306457322002394,Natural language processing,"Opinion summarization can facilitate user’s decision-making by mining the salient review information. However, due to the lack of sufficient annotated data, most of the early works are based on extractive methods, which restricts the performance of opinion summarization. In this work, we aim to improve the informativeness of opinion summarization to provide better guidance to users. We consider the setting with only reviews without corresponding summaries, and propose an aspect-augmented model for unsupervised abstractive opinion summarization, denoted as AsU-OSum. We first employ an aspect-based sentiment analysis system to extract opinion phrases from reviews. Then, we construct a heterogeneous graph consisting of reviews and opinion clusters as nodes, which is used to enhance the Transformer-based encoder–decoder framework. Furthermore, we design a novel cascaded attention mechanism to prompt the decoder to pay more attention to the aspects that are more likely to appear in summary. During training, we introduce a sentiment accuracy reward that further enhances the learning ability of our model. We conduct comprehensive experiments on the Yelp, Amazon, and Rotten Tomatoes datasets. Automatic evaluation results show that our model is competitive and performs better than the state-of-the-art (SOTA) models on some ROUGE metrics. Human evaluation results further verify that our model can generate more informative summaries and reduce redundancy.",,1,,,
,,"Zhou, Gang",,,,,,,,,Opinion summarization,,,,,,
,,"Huang, Ningbo",,,,,,,,,Aspect-augmented,,,,,,
,,"He, Peng",,,,,,,,,Heterogeneous graph,,,,,,
,,"Yu, Wanting",,,,,,,,,Attention mechanism,,,,,,
,,"Liu, Wenfen",,,,,,,,,,,,,,,
JOUR,Generating paraphrase sentences for multimodal entity-category-sentiment triple extraction,"Yang, Li",Knowledge-Based Systems,278,110823,2023,2023/10/25/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110823,https://www.sciencedirect.com/science/article/pii/S0950705123005737,Entity-based sentiment analysis,"Multimodal entity-based sentiment analysis (MEBSA) is an emerging task in sentiment analysis that aims to identify three key elements (entity, entity category, and sentiment polarity) from a pair of sentence and image. However, most existing studies have primarily focused on one or two MEBSA subtasks, ignoring the fact that these subtasks are closely related with one another. Moreover, previous studies focused on the detection of coarse-grained entity categories, which failed to provide sufficient information to disambiguate entities. To address these two issues, we introduced a new task called multimodal entity-category-sentiment triple extraction (MECSTE) to extract entities, their corresponding fine-grained entity categories, and sentiment polarities simultaneously. We constructed two datasets for this task based on two existing Twitter corpora. Moreover, we developed a generative multimodal approach based on a pre-trained sequence-to-sequence model that formulates the MECSTE task as a paraphrase generation problem by linearising all the entity-category-sentiment triples into a natural language sentence. Extensive experiments on the annotated Twitter datasets demonstrate the superiority of the proposed method.",,,,,
,,"Wang, Jieming",,,,,,,,,Fine-grained opinion mining,,,,,,
,,"Na, Jin-Cheon",,,,,,,,,Multimodal sentiment analysis,,,,,,
,,"Yu, Jianfei",,,,,,,,,,,,,,,
JOUR,Zero-shot cross-lingual transfer language selection using linguistic similarity,"Eronen, Juuso",Information Processing & Management,60,103250,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103250,https://www.sciencedirect.com/science/article/pii/S030645732200351X,Multilingual natural language processing,"We study the selection of transfer languages for different Natural Language Processing tasks, specifically sentiment analysis, named entity recognition and dependency parsing. In order to select an optimal transfer language, we propose to utilize different linguistic similarity metrics to measure the distance between languages and make the choice of transfer language based on this information instead of relying on intuition. We demonstrate that linguistic similarity correlates with cross-lingual transfer performance for all of the proposed tasks. We also show that there is a statistically significant difference in choosing the optimal language as the transfer source instead of English. This allows us to select a more suitable transfer language which can be used to better leverage knowledge from high-resource languages in order to improve the performance of language applications lacking data. For the study, we used datasets from eight different languages from three language families.",,3,,,
,,"Ptaszynski, Michal",,,,,,,,,Zero-shot learning,,,,,,
,,"Masui, Fumito",,,,,,,,,Transfer learning,,,,,,
,,,,,,,,,,,Linguistics,,,,,,
,,,,,,,,,,,Language similarity,,,,,,
JOUR,Multitasking of sentiment detection and emotion recognition in code-mixed Hinglish data,"Ghosh, Soumitra",Knowledge-Based Systems,260,110182,2023,2023/01/25/,0950-7051,https://doi.org/10.1016/j.knosys.2022.110182,https://www.sciencedirect.com/science/article/pii/S0950705122012783,Emotion analysis,"As the number of non-native English speakers on social media has skyrocketed in recent years, sentiment and emotion analysis on regional languages and code-mixed data has gained traction. Despite extensive research on English, the area of Hindi–English code-mixed texts is still relatively new and understudied. We create an emotion annotated Hindi–English (Hinglish) code-mixed dataset by performing emotion annotation on the benchmark SentiMix dataset to solve this problem and enable future researchers to contribute to this domain. We propose an end-to-end transformer-based multitask framework for sentiment detection and emotion recognition from the SentiMix code-mixed dataset. We fine-tune the pre-trained cross-lingual embedding model, XLMR, using task-specific data to further exploit the efficacy of transfer learning to improve the overall efficiency of our methods. Our proposed multi-task solution outperforms the state-of-the-art single-task and multitask baselines by a considerable margin, implying that the auxiliary task (i.e. emotion recognition) increases the efficiency of the primary task (i.e. sentiment detection) in a multi-task environment. It should be noted that the reported findings were obtained without the use of any ensemble techniques, thereby adhering to a model of effective and production-ready NLP.",,,,,
,,"Priyankar, Amit",,,,,,,,,Sentiment analysis,,,,,,
,,"Ekbal, Asif",,,,,,,,,Multitasking,,,,,,
,,"Bhattacharyya, Pushpak",,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Transfer learning,,,,,,
,,,,,,,,,,,Code-mixed data,,,,,,
JOUR,Pseudo dense counterfactual augmentation for aspect-based sentiment analysis,"Ouyang, Jihong",Neurocomputing,561,126869,2023,2023/12/07/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126869,https://www.sciencedirect.com/science/article/pii/S092523122300992X,Aspect-based sentiment analysis,"Aspect-based sentiment analysis (ABSA) is a fine-grained text classification task, and the cutting-edge ABSA models have achieved outstanding performance. Unfortunately, the robustness of these ABSA models is neglected. ABSA models must face numerous challenges to be robust, and we concentrate on one of these challenges caused by negation words, such as “not”, “un-”. In the actual context, these negation words intuitively result in two problems: negative sensitivity and spurious correlation. First, a negation word tends to reverse the sentiment polarity of a sentence. Meanwhile, in the ABSA datasets, most sentences containing negation words express Negative polarities, which will lead the predictive model to learn the spurious correlation between negation words and polarities. To resolve these ambiguous issues, we are inspired by causal inference and propose a novel data augmentation framework, namely Pseudo Dense Counterfactual Augmentation (PDCaug) for ABSA. Specifically, we initialize a pseudo sequence and employ a multi-head multi-layer attention network to achieve counterfactual augmentation for a vanilla sentence in the hidden space. This pseudo sequence will be adversarially trained. PDCaug is a plug-and-play method for various ABSA models, so we evaluate it on discriminative models and generative prompt-based models. Our extensive experiments show that our PDCaug can significantly and consistently outperform several data augmentation methods and ABSA models.",,,,,
,,"Feng, Shi",,,,,,,,,Encoder-decoder,,,,,,
,,"Wang, Bing",,,,,,,,,Data augmentation,,,,,,
,,"Yang, Zhiyao",,,,,,,,,,,,,,,
JOUR,An automated multi-web platform voting framework to predict misleading information proliferated during COVID-19 outbreak using ensemble method,"Varshney, Deepika",Data & Knowledge Engineering,143,102103,2023,2023/01/01/,0169-023X,https://doi.org/10.1016/j.datak.2022.102103,https://www.sciencedirect.com/science/article/pii/S0169023X22000945,COVID-19,"The spreading of misleading information on social web platforms has fuelled massive panic and confusion among the public regarding the Corona disease, the detection of which is of paramount importance. Previous studies mainly relied on a specific web platform to collect crucial evidence to detect fake content. The analysis identifies that retrieving clues from two or more different sources/web platforms gives more reliable prediction and confidence concerning a specific claim. This study proposed a novel multi-web platform voting framework that incorporates 4 sets of novel features: content, linguistic, similarity, and sentiments. The features have been gathered from each web-platforms to validate the news. To validate the fact/claim, a unique source platform is designed to collect relevant clues/headlines from two web platforms (YouTube, Google) based on specific queries and extracted features concerning each clue/headline. The proposed idea is to incorporate a unique platform to assist researchers in gathering relevant and vital evidence from diverse web platforms. After evaluation and validation, it has been identified that the built model is quite intelligent, gives promising results, and effectively predicts misleading information. The model correctly detected about 98% of the COVID misinformation on the constraint Covid-19 fake news dataset. Furthermore, it is observed that it is efficient to gather clues from multiple web platforms for more reliable predictions to validate the news. The suggested work depicts numerous practical applications for health policy-makers and practitioners that could be useful in safeguarding and implicating awareness among society from misleading information dissemination during this pandemic.",,,,,
,,"Vishwakarma, Dinesh Kumar",,,,,,,,,Misleading information,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Fake news,,,,,,
,,,,,,,,,,,Multi-web platforms,,,,,,
,,,,,,,,,,,YouTube,,,,,,
,,,,,,,,,,,Google,,,,,,
JOUR,Sentiment overflow in the testing stack: Analyzing software testing posts on Stack Overflow,"Swillus, Mark",Journal of Systems and Software,205,111804,2023,2023/11/01/,0164-1212,https://doi.org/10.1016/j.jss.2023.111804,https://www.sciencedirect.com/science/article/pii/S0164121223001991,Stack Overflow,"Software testing is an integral part of modern software engineering practice. Past research has not only underlined its significance, but also revealed its multi-faceted nature. The practice of software testing and its adoption is influenced by many factors that go beyond tools or technology. This paper sets out to investigate the context of software testing from the practitioners’ point of view by mining and analyzing sentimental posts on the widely used question and answer website Stack Overflow. By qualitatively analyzing sentimental expressions of practitioners, which we extract from the Stack Overflow dataset using sentiment analysis tools, we discern factors that help us to better understand the lived experience of software engineers with regards to software testing. Grounded in the data that we have analyzed, we argue that sentiments like insecurity, despair and aspiration, have an impact on practitioners’ attitude towards testing. We suggest that they are connected to concrete factors like the level of complexity of projects in which software testing is practiced. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.",,,,,
,,"Zaidman, Andy",,,,,,,,,Software testing,,,,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Grounded theory,,,,,,
JOUR,Cross-Modal Multitask Transformer for End-to-End Multimodal Aspect-Based Sentiment Analysis,"Yang, Li",Information Processing & Management,59,103038,2022,2022/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103038,https://www.sciencedirect.com/science/article/pii/S0306457322001479,Fine-grained opinion mining,"As an emerging task in opinion mining, End-to-End Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract all the aspect-sentiment pairs mentioned in a pair of sentence and image. Most existing methods of MABSA do not explicitly incorporate aspect and sentiment information in their textual and visual representations and fail to consider the different contributions of visual representations to each word or aspect in the text. To tackle these limitations, we propose a multi-task learning framework named Cross-Modal Multitask Transformer (CMMT), which incorporates two auxiliary tasks to learn the aspect/sentiment-aware intra-modal representations and introduces a Text-Guided Cross-Modal Interaction Module to dynamically control the contributions of the visual information to the representation of each word in the inter-modal interaction. Experimental results demonstrate that CMMT consistently outperforms the state-of-the-art approach JML by 3.1, 3.3, and 4.1 absolute percentage points on three Twitter datasets for the End-to-End MABSA task, respectively. Moreover, further analysis shows that CMMT is superior to comparison systems in both aspect extraction (AE) and sentiment classification (SC), which would move the development of multimodal AE and SC algorithms forward with improved performance.",,5,,,
,,"Na, Jin-Cheon",,,,,,,,,Aspect-Based Sentiment Analysis,,,,,,
,,"Yu, Jianfei",,,,,,,,,Multimodal Sentiment Analysis,,,,,,
JOUR,"EMOCOV: Machine learning for emotion detection, analysis and visualization using COVID-19 tweets","Kabir, Md. Yasin",Online Social Networks and Media,23,100135,2021,2021/05/01/,2468-6964,https://doi.org/10.1016/j.osnem.2021.100135,https://www.sciencedirect.com/science/article/pii/S2468696421000197,COVID-19 data,"The adversarial impact of the Covid-19 pandemic has created a health crisis globally all over the world. This unprecedented crisis forced people to lockdown and changed almost every aspect of the regular activities of the people. Thus, the pandemic is also impacting everyone physically, mentally, and economically, and it, therefore, is paramount to analyze and understand emotional responses during the crisis affecting mental health. Negative emotional responses at fine-grained labels like anger and fear during the crisis might also lead to irreversible socio-economic damages. In this work, we develop a neural network model and train it using manually labeled data to detect various emotions at fine-grained labels in the Covid-19 tweets automatically. We present a manually labeled tweets dataset on COVID-19 emotional responses along with regular tweets data. We created a custom Q&A roBERTa model to extract phrases from the tweets that are primarily responsible for the corresponding emotions. None of the existing datasets and work currently provide the selected words or phrases denoting the reason for the corresponding emotions. Our classification model outperforms other systems and achieves a Jaccard score of 0.6475 with an accuracy of 0.8951. The custom RoBERTa Q&A model outperforms other models by achieving a Jaccard score of 0.7865. Further, we present a historical emotion analysis using COVID-19 tweets over the USA including each state level analysis.",,,,,
,,"Madria, Sanjay",,,,,,,,,Coronavirus,,,,,,
,,,,,,,,,,,Twitter Data,,,,,,
,,,,,,,,,,,Data analytics,,,,,,
,,,,,,,,,,,Topics tracker,,,,,,
,,,,,,,,,,,Emotion analysis,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
JOUR,Efficient dynamic feature adaptation for cross language sentiment analysis with biased adversarial training,"Li, Rui",Knowledge-Based Systems,279,110957,2023,2023/11/04/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110957,https://www.sciencedirect.com/science/article/pii/S0950705123007074,Pre-trained language model,"Fine-tuning a large multi-lingual pretrained language model demonstrates impressive results in cross-language understanding. However, it still suffers when the training and test data have different distributions owing to various languages and domains. On one hand, annotating target data for different languages or domains is time-consuming or infeasible. On the other hand, fine-tuning a large language model often incurs high computational costs. In this paper, we aim to develop an efficient and effective adaptation framework for cross-language sentiment analysis based on a fixed pretrained multi-lingual model. Specifically, we propose a Dynamic Feature Adaptation (DFA) module to fully leverage the features from different layers of the pretrained model such that its large backbone is not involved during adaptation training. Furthermore, we observe that traditional adversarial domain adaptation training could compromise the discriminative information of the model by pushing source and target features towards each other. The source features obtained with supervised training preserved the discriminability of the model, which should be less affected. Therefore, we propose a novel Biased Adversarial Training (BAT) method, that encourages only the target features towards source features. Extensive experimental results on various cross-lingual and cross-lingual-and-domain sentiment analysis tasks demonstrate the superiority of the proposed framework. Additionally, several ablation studies are conducted to validate the effectiveness of each proposed module.",,,,,
,,"Liu, Cheng",,,,,,,,,Cross-language understanding,,,,,,
,,"Jiang, Dazhi",,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Efficient domain adaptation,,,,,,
,,,,,,,,,,,Adversarial training,,,,,,
JOUR,MHaDiG: A Multilingual Humor-aided Multiparty Dialogue Generation in multimodal conversational setting,"Chauhan, Dushyant Singh",Knowledge-Based Systems,278,110840,2023,2023/10/25/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110840,https://www.sciencedirect.com/science/article/pii/S0950705123005907,Multimodal humor,"Building a multimodal humor-aided dialogue generation system in a multilingual setting is a very challenging task and it could even be more challenging in the Hindi language because of its morphological richness and free word order. Existing works only solved the problem of unimodal humorous dialogue generation, not humor-aided, in a monolingual setting. However, due to the tremendous growth in multimodal and multilingual content, there is a great demand to build multimodal humor-aided systems that support multilingual information access. To this end, in this paper, we propose a deep learning-based multimodal sentiment and emotion-aware humor-aided multiparty dialogue generation in multilingual (Hindi and English) setting. We evaluate our proposed approach (MHaDiG) on the recently released Sentiment, Humor, and Emotion-aware Multilingual Multimodal Multiparty Dataset (SHEMuD). Experimental results show the efficacy of MHaDiG and show the effect of multilingualism over monolingualism in the multimodal conversational setting.",,,,,
,,"Singh, Gopendra Vikram",,,,,,,,,Multimodal sentiment,,,,,,
,,"Ekbal, Asif",,,,,,,,,Multimodal emotion,,,,,,
,,"Bhattacharyya, Pushpak",,,,,,,,,Multilingual dataset,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
JOUR,A novel fusion-based deep learning model for sentiment analysis of COVID-19 tweets,"Basiri, Mohammad Ehsan",Knowledge-Based Systems,228,107242,2021,2021/09/27/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107242,https://www.sciencedirect.com/science/article/pii/S0950705121005049,Deep learning,"Undoubtedly, coronavirus (COVID-19) has caused one of the biggest challenges of all times. The ongoing COVID-19 pandemic has caused more than 150 million infected cases and one million deaths globally as of May 5, 2021. Understanding the sentiment of people expressed in their social media comments can help in monitoring, controlling, and ultimately eradicating the disease. This is a sensitive matter as the threat of infectious disease significantly affects the way people think and behave in various ways. In this study, we proposed a novel method based on the fusion of four deep learning and one classical supervised machine learning model for sentiment analysis of coronavirus-related tweets from eight countries. Also, we analyzed coronavirus-related searches using Google Trends to better understand the change in the sentiment pattern at different times and places. Our findings reveal that the coronavirus attracted the attention of people from different countries at different times in varying intensities. Also, the sentiment in their tweets is correlated to the news and events that occurred in their countries including the number of newly infected cases, number of recoveries and deaths. Moreover, common sentiment patterns can be observed in various countries during the spread of the virus. We believe that different social media platforms have great impact on raising people’s awareness about the importance of this disease as well as promoting preventive measures among people in the community.",,,,,
,,"Nemati, Shahla",,,,,,,,,Coronavirus (COVID-19),,,,,,
,,"Abdar, Moloud",,,,,,,,,Sentiment analysis,,,,,,
,,"Asadi, Somayeh",,,,,,,,,Information fusion,,,,,,
,,"Acharrya, U. Rajendra",,,,,,,,,Tweet analysis,,,,,,
JOUR,Investigating ChatGPT and cybersecurity: A perspective on topic modeling and sentiment analysis,"Okey, Ogobuchi Daniel",Computers & Security,135,103476,2023,2023/12/01/,0167-4048,https://doi.org/10.1016/j.cose.2023.103476,https://www.sciencedirect.com/science/article/pii/S0167404823003863,ChatGPT,"In early 2023, the Artificial Intelligence (AI) industry experienced a significant advancement with the emergence of OpenAI's ChatGPT, a research product that demonstrated remarkable capabilities and garnered widespread attention. ChatGPT is an advanced chatbot powered by the Generative Pretrained Transformers (GPT) architecture, designed to generate human-like conversations encompassing a wide range of knowledge domains. Many AI researchers are currently engaging with the new technology to understand its functionality and limitations. Various expressions across a range of social media platforms, including Twitter, YouTube, Facebook, and numerous others, are currently under investigation. This research seeks to analyze the opinions of ChatGPT users as it regards cybersecurity. This research is important due to its contribution towards gaining enhanced understanding and devising intricate improvements for the chatbot. The Latent Dirichlet Allocation (LDA) algorithm is utilized to extract relevant topics from the texts. Additionally, to analyze user opinions and decipher the sentiments as either positive, negative, or neutral, we use the Natural language tool kit Valence Aware Dictionary for sEntiment Reasoning (NLTK's VADER) and Robustly Optimized BERT Pretraining Approach (roBERTa) libraries. The data used is obtained from Twitter via the SNScrape library, which aided in the retrieval of over 700,000 tweets via the search terms #chatgptsecurity, #chatgpthackers, #chatgptcybersecurity, and #chatgptcyberthreats. The analysis of the results by the VADER model shows 43.8% positive, 36.3% neutral, and 19.9% negative sentiments. Similarly, the roBERTa model shows 14.1% positive, 53.2% neutral, and 32.7% negative. These results show that there is an ongoing concern about ChatGPT and cybersecurity, especially in malware code generation, hacking, intelligence gathering, and phishing attacks.",,,,,
,,"Udo, Ekikere Umoren",,,,,,,,,Cybersecurity,,,,,,
,,"Rosa, Renata Lopes",,,,,,,,,Sentiment analysis,,,,,,
,,"Rodríguez, Demostenes Zegarra",,,,,,,,,Generative pre-trained transformers,,,,,,
,,"Kleinschmidt, João Henrique",,,,,,,,,Artificial intelligence,,,,,,
,,,,,,,,,,,Data security,,,,,,
JOUR,An ALBERT-based TextCNN-Hatt hybrid model enhanced with topic knowledge for sentiment analysis of sudden-onset disasters,"Zhang, Xinsheng",Engineering Applications of Artificial Intelligence,123,106136,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106136,https://www.sciencedirect.com/science/article/pii/S0952197623003202,Deep learning,"Sudden-onset disasters put forward new requirements for on the state authorities’ ability to analyze public opinion sentiment. However, traditional sentiment analysis methods ignore the contextual semantic relationships and out-of-vocabulary words, and their computational resource utilization is excessive compared to their expected accuracy. In this paper, an ALBERT-based model combined with a text convolution neural network, a hierarchical attention mechanism and the latent Dirichlet allocation is proposed to create a hybrid model enhanced with topic knowledge for sentiment analysis of sudden-onset disasters. Weibo text data from a rainstorm disaster in China are used to evaluate the model’s performance. Compared with the XLNet, DistilBERT and RoBERTa models, the experimental results demonstrate that the proposed approach is capable of achieving better performance by incorporating external topic knowledge into the language representation model to compensate for the limited vocabulary data.",,,,,
,,"Ma, Yulong",,,,,,,,,Sudden-onset disaster,,,,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,ALBERT model,,,,,,
,,,,,,,,,,,Latent Dirichlet Allocation (LDA),,,,,,
JOUR,DeepEmotionNet: Emotion mining for corporate performance analysis and prediction,"Wang, Qiping",Information Processing & Management,60,103151,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103151,https://www.sciencedirect.com/science/article/pii/S0306457322002527,Emotion mining,"Since previous studies in cognitive psychology show that individuals’ affective states can help analyze and predict their future behaviors, researchers have explored emotion mining for predicting online activities, firm profitability, and so on. Existing emotion mining methods are divided into two categories: feature-based approaches that rely on handcrafted annotations and deep learning-based methods that thrive on computational resources and big data. However, neither category can effectively detect emotional expressions captured in text (e.g., social media postings). In addition, the utilization of these methods in downstream explanatory and predictive applications is also rare. To fill the aforementioned research gaps, we develop a novel deep learning-based emotion detector named DeepEmotionNet that can simultaneously leverage contextual, syntactic, semantic, and document-level features and lexicon-based linguistic knowledge to bootstrap the overall emotion detection performance. Based on three emotion detection benchmark corpora, our experimental results confirm that DeepEmotionNet outperforms state-of-the-art baseline methods by 4.9% to 29.8% in macro-averaged F-score. For the downstream application of DeepEmotionNet to a real-world financial application, our econometric analysis highlights that top executives’ emotions of fear and anger embedded in their social media postings are significantly associated with corporate financial performance. Furthermore, these two emotions can significantly improve the predictive power of corporate financial performance when compared to sentiments. To the best of our knowledge, this is the first study to develop a deep learning-based emotion detection method and successfully apply it to enhance corporate performance prediction.",,3,,,
,,"Su, Tingxuan",,,,,,,,,Deep learning,,,,,,
,,"Lau, Raymond Yiu Keung",,,,,,,,,Predictive analytics,,,,,,
,,"Xie, Haoran",,,,,,,,,Econometric analysis,,,,,,
,,,,,,,,,,,Corporate financial performance,,,,,,
JOUR,Language-agnostic deep learning framework for automatic monitoring of population-level mental health from social networks,"Noraset, Thanapon",Journal of Biomedical Informatics,133,104145,2022,2022/09/01/,1532-0464,https://doi.org/10.1016/j.jbi.2022.104145,https://www.sciencedirect.com/science/article/pii/S1532046422001587,Mental health,"In many countries, mental health issues are among the most serious public health concerns. National mental health statistics are frequently collected from reported patient cases or government-sponsored surveys, which have restricted coverage, frequency, and timeliness. Many domains of study, including public healthcare and biomedical informatics, have recently adopted social media data as a feasible real-time alternative to traditional methods of gathering representative information at the population level in a variety of contexts. However, because of the limits of fundamental natural language processing tools and labeled corpora in countries with limited natural language resources, such as Thailand, implementing social media systems to monitor mental health signals could be challenging. This paper presents LAPoMM, a novel framework for monitoring real-time mental health indicators from social media data without using labeled datasets in low-resource languages. Specifically, we use cross-lingual methods to train language-agnostic models and validate our framework by examining cross-correlations between the aggregate predicted mental signals and real-world administrative data from Thailand’s Department of Mental Health, which includes monthly depression patients and reported cases of suicidal attempts. A combination of a language-agnostic representation and a deep learning classification model outperforms all other cross-lingual techniques for recognizing various mental signals in tweets, such as emotions, sentiments, and suicidal tendencies. The correlation analyses discover a strong positive relationship between actual depression cases and the predicted negative sentiment signals as well as suicide attempts and negative signals (e.g., fear, sadness, and disgust) and suicidal tendency. These findings establish the effectiveness of our proposed framework and its potential applications in monitoring population-level mental health using large-scale social media data. Furthermore, because the language-agnostic model utilized in the methodology is capable of supporting a wide range of languages, the proposed LAPoMM framework can be easily generalized for analogous applications in other countries with limited language resources.",,,,,
,,"Chatrinan, Krittin",,,,,,,,,Large-scale social media analysis,,,,,,
,,"Tawichsri, Tanisa",,,,,,,,,Deep learning,,,,,,
,,"Thaipisutikul, Tipajin",,,,,,,,,Cross lingual models,,,,,,
,,"Tuarob, Suppawong",,,,,,,,,Social networks,,,,,,
JOUR,Artificial cheerleading in IEO: Marketing campaign or pump and dump scheme,"Tian, Yingjie",Information Processing & Management,61,103537,2024,2024/01/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103537,https://www.sciencedirect.com/science/article/pii/S0306457323002741,Deep learning,"Initial coin offerings (ICOs) have been vulnerable to pump-and-dump schemes, where fraudsters spread false information on social media to inflate coin prices. However, initial exchange offerings (IEOs) are considered more reliable as exchanges conduct due diligence on projects. This study investigates the effectiveness of IEOs in mitigating pump-and-dump schemes by analyzing over 11,000 tweets posted by different accounts from 54 IEO projects one month prior to the IEO. We select 22 projects with at least 1000 tweets and use topic modeling to perform a descriptive analysis. We use botometer to produce a bot score for each tweet account and a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model for sentiment analysis. We introduce penalized sentiment scores and interaction term and use an OLS regression model to identify the relationship between bot scores, sentiment values, and post-IEO performance. Our research finds that bot score is negatively associated with 30-day return and is positively associated with 30-day volatility, indicating the presence of pump-and-dump schemes in IEOs. We suggest that investors, exchanges, and regulators should take steps to assess the risks associated with IEOs and design appropriate interventions to prevent market manipulation.",,1,,,
,,"Xie, Yuhao",,,,,,,,,Social media analysis,,,,,,
,,,,,,,,,,,Initial exchange offering,,,,,,
,,,,,,,,,,,Bot detection,,,,,,
JOUR,Credit default prediction from user-generated text in peer-to-peer lending using deep learning,"Kriebel, Johannes",European Journal of Operational Research,302,309,2022,2022/10/01/,0377-2217,https://doi.org/10.1016/j.ejor.2021.12.024,https://www.sciencedirect.com/science/article/pii/S037722172101078X,OR in banking,"Digital technologies produce vast amounts of unstructured data that can be stored and accessed by traditional banks and fintech companies. We employ deep learning and several other techniques to extract credit-relevant information from user-generated text on Lending Club. Our results show that even short pieces of user-generated text can improve credit default predictions significantly. The importance of text is further supported by an information fusion analysis. Compared with other approaches that use text, deep learning outperforms them in almost all cases. However, machine learning models combined with word frequencies or topic models also extract substantial credit-relevant information. A comparison of six deep neural network architectures, including state-of-the-art transformer models, finds that the architectures mostly provide similar performance. This means that simpler methods (such as average embedding neural networks) offer performance comparable to more complex methods (such as the transformer networks BERT and RoBERTa) in this credit scoring setting.",323,1,,,
,,"Stitz, Lennart",,,,,,,,,Peer-to-peer lending,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Textual data,,,,,,
,,,,,,,,,,,Credit risk,,,,,,
JOUR,TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis,"Wang, Di",Pattern Recognition,136,109259,2023,2023/04/01/,0031-3203,https://doi.org/10.1016/j.patcog.2022.109259,https://www.sciencedirect.com/science/article/pii/S0031320322007385,Multimodal sentiment analysis,"Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment-related information and simultaneously preserving inter- and intra-modality relationships becomes a significant challenge. To this end, we propose a novel method named Text Enhanced Transformer Fusion Network (TETFN), which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. In particular, it incorporates textual information in learning sentiment-related nonlinguistic representations through text-based multi-head attention. In addition to preserving consistency information by cross-modal mappings, it also retains the differentiated information among modalities through unimodal label prediction. Furthermore, the vision pre-trained model Vision-Transformer is utilized to extract visual features from the original videos to preserve both global and local information of a human face. Extensive experiments on benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the superior performance of the proposed TETFN over state-of-the-art methods.",,,,,
,,"Guo, Xutong",,,,,,,,,Transformer,,,,,,
,,"Tian, Yumin",,,,,,,,,Text-oriented pairwise cross-modal mappings,,,,,,
,,"Liu, Jinhui",,,,,,,,,,,,,,,
,,"He, LiHuo",,,,,,,,,,,,,,,
,,"Luo, Xuemei",,,,,,,,,,,,,,,
JOUR,Data augmentation for sentiment classification with semantic preservation and diversity,"Chao, Guoqing",Knowledge-Based Systems,280,111038,2023,2023/11/25/,0950-7051,https://doi.org/10.1016/j.knosys.2023.111038,https://www.sciencedirect.com/science/article/pii/S0950705123007888,Deep learning,"Data augmentation is a commonly-used technique to avoid over-fitting in deep learning. However, the mechanism behind effective data augmentation methods is unclear. To address this issue, we explore and identify two critical factors: semantic preservation and diversity to assess the quality of data augmentation in natural language processing. Our study focus on text sentiment classification and examines these two factors on two commonly-used data augmentation methods: synonym replacement and random deletion. Based on the discovery, we propose two new augmentation methods: TF-IDF word dropout and adaptive synonym replacement. Experimental results demonstrate that these two new data augmentation methods are effective. Moreover, with further experiments, we summarize three strategies for improving data augmentation methods in sentiment classification task. These strategies are employing online augmentation, introducing word importance into word sampling process, and filtering augmented data based on the current model state. We hope that our study will inspire some new perspectives on the underlying principles of data augmentation’s effectiveness and contribute to a systematic study of data augmentation methods in future.",,,,,
,,"Liu, Jingyao",,,,,,,,,Data augmentation,,,,,,
,,"Wang, Mingyu",,,,,,,,,Natural language processing,,,,,,
,,"Chu, Dianhui",,,,,,,,,Sentiment classification,,,,,,
JOUR,On the instability of further pre-training: Does a single sentence matter to BERT?,"Bacco, Luca",Natural Language Processing Journal,,100037,2023,2023/10/27/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100037,https://www.sciencedirect.com/science/article/pii/S2949719123000341,Natural language processing,"We observe a remarkable instability in BERT-like models: minimal changes in the internal representations of BERT, as induced by one-step further pre-training with even a single sentence, can noticeably change the behaviour of subsequently fine-tuned models. While the pre-trained models seem to be essentially the same, also by means of established similarity assessment techniques, the measurable tiny changes appear to substantially impact the models’ tuning path, leading to significantly different fine-tuned systems and affecting downstream performance. After testing a very large number of combinations, which we briefly summarise, the experiments reported in this short paper focus on an intermediate phase consisting of a single-step and single-sentence masked language modeling stage and its impact on a sentiment analysis task. We discuss a series of unexpected findings which leave some open questions over the nature and stability of further pre-training.",,,,,
,,"Minnema, Gosse",,,,,,,,,Large language models,,,,,,
,,"Caselli, Tommaso",,,,,,,,,Further pre-training,,,,,,
,,"Dell’Orletta, Felice",,,,,,,,,Transformers,,,,,,
,,"Merone, Mario",,,,,,,,,Instability,,,,,,
,,"Nissim, Malvina",,,,,,,,,BERT,,,,,,
JOUR,Combining dynamic local context focus and dependency cluster attention for aspect-level sentiment classification,"Xu, Mayi",Neurocomputing,478,49,2022,2022/03/14/,0925-2312,https://doi.org/10.1016/j.neucom.2021.12.084,https://www.sciencedirect.com/science/article/pii/S0925231221019391,Aspect-level sentiment classification,"Aspect-level sentiment classification (ASC), as a subtask of Aspect-based sentiment analysis (ABSA), aims to analyze the sentiment polarity of different aspect terms in a sentence. Although the existing methods have been proven to be effective, they fail to effectively identify the range of local context and fully leverage the nonequivalent property of dependency relation. Hence, we propose a concept of dependency cluster and design two modules named Dynamic Local Context Focus (DLCF) and Dependency Cluster Attention (DCA) respectively. The DLCF can dynamically capture the range of local context based on the different max distance from the target aspect term to its context words and the DCA allows the model to pay more attention to the cluster which is more critical for sentiment classification. Together with the two modules, we then propose the DLCF-DCA model, in which the DLCF is equipped before DCA. Considering the DLCF has masked or weighted down the less-semantic-relative words, the semantic information can therefore be better extracted in DCA. Experiments conducted on six benchmark datasets demonstrate that DLCF-DCA achieves the state-of-the-art results. Moreover, the ablation experiment results also verify the effectiveness of each part in DLCF-DCA.",69,,,,
,,"Zeng, Biqing",,,,,,,,,Dynamic local context focus,,,,,,
,,"Yang, Heng",,,,,,,,,Dependency cluster attention,,,,,,
,,"Chi, Junlong",,,,,,,,,,,,,,,
,,"Chen, Jiatao",,,,,,,,,,,,,,,
,,"Liu, Hongye",,,,,,,,,,,,,,,
JOUR,Predictive intelligence in harmful news identification by BERT-based ensemble learning model with text sentiment analysis,"Lin, Szu-Yin",Information Processing & Management,59,102872,2022,2022/03/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.102872,https://www.sciencedirect.com/science/article/pii/S0306457322000073,Information disorder,"In an environment full of disordered information, the media spreads fake or harmful information into the public arena with a speed which is faster than ever before. A news report should ideally be neutral and factual. Excessive personal emotions or viewpoints should not be included. News articles ought not to be intentionally or maliciously written or create a media framing. A harmful news is defined as those explicit or implicit harmful speech in news text that harms people or affects readers’ perception. However, in the current situation, it is difficult to effectively identify and predict fake or harmful news in advance, especially harmful news. Therefore, in this study, we propose a Bidirectional Encoder Representation from Transformers (BERT) based model which applies ensemble learning methods with a text sentiment analysis to identify harmful news, aiming to provide readers with a way to identify harmful news content so as to help them to judge whether the information provided is in a more neutral manner. The working model of the proposed system has two phases. The first phase is collecting harmful news and establishing a development model for analyzing the correlation between text sentiment and harmful news. The second phase is identifying harmful news by analyzing text sentiment with an ensemble learning technique and the BERT model. The purpose is to determine whether the news has harmful intentions. Our experimental results show that the F1-score of the proposed model reaches 66.3%, an increase of 7.8% compared with that of the previous term frequency-inverse document frequency approach which adopts a Lagrangian Support Vector Machine (LSVM) model without using a text sentiment. Moreover, the proposed method achieves a better performance in recognizing various cases of information disorder.",,2,,,
,,"Kung, Yun-Ching",,,,,,,,,Harmful news analysis,,,,,,
,,"Leu, Fang-Yie",,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,News sentiment analysis,,,,,,
,,,,,,,,,,,Ensemble learning,,,,,,
,,,,,,,,,,,BERT model,,,,,,
JOUR,Affective awareness in neural sentiment analysis,"Xiang, Rong",Knowledge-Based Systems,226,107137,2021,2021/08/17/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107137,https://www.sciencedirect.com/science/article/pii/S0950705121004007,Deep neural network,"Sentiment analysis is helpful to bestow ability of understanding human’s attitude in texts on artificial intelligence systems. In this area, text sentiment is usually signaled by a few indicative words that convey affective meanings and arouse readers’ collective emotions. However, most existing sentiment analysis models have predominantly featured through neural network architectures with end-to-end training manner and limited awareness of affective knowledge, which, as a result, often fails to pinpoint the essential features for sentiment prediction. In this work, we present a novel approach for sentiment analysis by fusing external affective knowledge into neural networks. The affective knowledge is distilled from two sentiment lexicons grounded by two psychological theories, e.g., the Affect Control Theory and word affections in terms of Valence, Arousal, and Dominance. To examine the effects of affective knowledge over sentiment analysis, we conduct cross-dataset and cross-model experiments along with a detailed ablation analysis. Results show that our proposed method outperforms trendy neural networks in all the five benchmarks with consistent and significant improvement (1.4% Accuracy in average). Further discussions demonstrate that all affective attributes exhibit positive effects to model enhancement and our model is robust to the change of lexicon size.",,,,,
,,"Li, Jing",,,,,,,,,Sentiment analysis,,,,,,
,,"Wan, Mingyu",,,,,,,,,Affective knowledge,,,,,,
,,"Gu, Jinghang",,,,,,,,,Sentiment lexicon,,,,,,
,,"Lu, Qin",,,,,,,,,,,,,,,
,,"Li, Wenjie",,,,,,,,,,,,,,,
,,"Huang, Chu-Ren",,,,,,,,,,,,,,,
JOUR,Emotion quantification and classification using the neutrosophic approach to deep learning,"Sharma, Mayukh",Applied Soft Computing,148,110896,2023,2023/11/01/,1568-4946,https://doi.org/10.1016/j.asoc.2023.110896,https://www.sciencedirect.com/science/article/pii/S1568494623009146,Neutrosophy,"Advancements in the rapidly evolving specialization of deep learning have aided in improving several natural language understanding tasks. Sentiment and emotion classification models have improved, but when it comes to fine-grained sentiment analysis, these models can perform better. Human sentiment in natural language is generally an intricate combination of emotions, which can sometimes be indeterminate, neutral, or ambiguous. In the case of fine-grained sentiment analysis, the sentiments can be very similar to each other and interconnected, e.g., anger and fear. Most deep learning systems try to solve the problem of fine-grained sentiment analysis as a classification problem. However, fine-grained sentiments might combine similar emotions with one primary emotion. Trying to solve the problem as a classification task can result in better performance on benchmarks but does not ensure a better understanding and representation of language. The proposed work explores applying neutrosophy for fine-grained sentiment analysis using large language models. Neutrosophy identifies neutralities and employs membership functions (neutral, positive, negative) to quantify an instance into Single Valued Neutrosophic Sets (SVNS). This paper introduces Refined Emotion Neutrosophic Sets (RENS) for emotions (with four emotions) and Refined Ekman’s Emotion Neutrosophic Sets (REENS) with seven emotions. In this paper, refined neutrosophic sets with membership functions are employed for each sentiment across a given taxonomy and assigned their values using the Neutrosophic Iterative Neural Clustering (NINC) algorithm proposed in this paper. It facilitates not only classifying sentiments but also quantifying the presence of each sentiment present in a given sample. It aids in better understanding and representation of samples across multiple sentiments, as in fine-grained sentiment analysis, experiments are performed on the GoEmotions dataset. The proposed approach performs on par with cross-entropy deep learning classifiers and is reproducible across different pre-trained language models.",,,,,
,,"Kandasamy, Ilanthenral",,,,,,,,,Sentiment analysis,,,,,,
,,"Vasantha, W.B.",,,,,,,,,Refined emotion neutrosophic set,,,,,,
,,,,,,,,,,,Emotion quantification,,,,,,
,,,,,,,,,,,Neutrosophic iterative neural clustering (NINC),,,,,,
JOUR,An adversarial training method for text classification,"Liu, Xiaoyang",Journal of King Saud University - Computer and Information Sciences,35,101697,2023,2023/09/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101697,https://www.sciencedirect.com/science/article/pii/S1319157823002513,Gradient reversal,"Text classification is an emerging topic in the field of text data mining, but the current methods of deducing sentence polarity have two major shortcomings: on the one hand, there is currently a lack of a large and well-curated corpus; on the other hand, current solutions based on deep learning are particularly vulnerable to attacks from adversarial samples. To overcome the limitations above, we propose an adversarial training method HNN-GRAT (Hierarchical Neural Network and Gradient Reversal) for text classification. Firstly, A Robustly Optimized BERT Pretraining Approach (RoBERTa) pretraining model is used to extract text features and feature gradient information; secondly, the original gradient information is passed through the gradient reversal layer designed to obtain the inverted gradient information; finally, the original gradient information and the inverted gradient information are fused to obtain the new gradient of the model. HNN-GRAT method are tested on three real datasets and five attack methods, compared with RoBERTa pretraining model, HNN-GRAT improves the robustness accuracy and reduces the probability of the model being attacked. In addition, using six text defense methods, HNN-GRAT achieves the best Boa and Succ (Such as, DeepWordBug attack, for AGNEWS, IMDB, SST-2 datasets, with an improvement of Boa up to 41.50%, 67.50%, 28.15% and Succ drop to 55.90%, 27.45%, 69.89%, respectively).",,8,,,
,,"Dai, Shanghong",,,,,,,,,Adversarial training,,,,,,
,,"Fiumara, Giacomo",,,,,,,,,Text classification,,,,,,
,,"De Meo, Pasquale",,,,,,,,,Neural networks,,,,,,
,,,,,,,,,,,Adversarial sample,,,,,,
JOUR,Isomer: Transfer enhanced dual-channel heterogeneous dependency attention network for aspect-based sentiment classification,"Cao, Yukun",Knowledge-Based Systems,256,109879,2022,2022/11/28/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109879,https://www.sciencedirect.com/science/article/pii/S0950705122009728,Aspect-based sentiment analysis,"Aspect-based sentiment classification aims to predict the sentiment polarity of a specific aspect in a sentence. However, most existing methods attempt to construct dependency relations into a homogeneous dependency graph with sparsity and ambiguity, which only considers one type of node and one type of edge, thus cannot cover the comprehensive contextualized features of short texts or consider any additional node types or semantic relation information. To solve those issues, we present a sentiment analysis model, Isomer, which performs dual-channel attention on heterogeneous dependency graphs incorporating external knowledge to integrate additional information effectively. Specifically, a transfer-enhanced dual-channel heterogeneous dependency attention network is designed in Isomer for modeling short texts by heterogeneous dependency graphs. These heterogeneous dependency graphs not only consider different types of information but also incorporate external knowledge. Experiments studies show that Isomer outperforms the-state-of-arts on diverse datasets. Furthermore, the results suggest that Isomer captures the importance of various information features to focus on informative contextual words.",,,,,
,,"Tang, Yijia",,,,,,,,,Knowledge graph,,,,,,
,,"Du, Haizhou",,,,,,,,,Heterogeneous graph,,,,,,
,,"Xu, Feifei",,,,,,,,,Graph convolutional network,,,,,,
,,"Wei, Ziyue",,,,,,,,,,,,,,,
,,"Jin, Chengkun",,,,,,,,,,,,,,,
JOUR,Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis,"Xiao, Luwei",Information Processing & Management,60,103508,2023,2023/11/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103508,https://www.sciencedirect.com/science/article/pii/S0306457323002455,Cross-modal alignment and fusion,"Multi-modal Aspect-based Sentiment Analysis (MABSA) aims to forecast the polarity of sentiment concerning aspects within a given sentence based on the correlation between the sentence and its accompanying image. Comprehending multi-modal sentiment expression requires strong cross-modal alignment and fusion ability. Previous state-of-the-art (SOTA) models fail to explicitly align valuable visual clues with aspect and sentiment information in textual representations and overlook the utilization of syntactic dependency information in the accompanying text modality. We present CoolNet (Cross-modal Fine-grained Alignment and Fusion Network) to boost the performance of visual-language models in seamlessly integrating vision and language information. Specifically, CoolNet starts by transforming an image into a textual caption and a graph structure, then dynamically aligns the semantic and syntactic information from both the input sentence and the generated caption, as well as models the object-level visual features. Finally, a cross-modal transformer is employed to fuse and model the inter-modality dynamics.This network boasts advanced cross-modal fine-grained alignment and fusion capabilities. On standard benchmarks such as Twitter-2015 and Twitter-2017, CoolNet consistently outperforms state-of-the-art algorithm FITE with notable improvements in accuracy and Macro-F1 values. Specifically, we observe an improvement in accuracy and Macro-F1 values by 1.43% and 1.38% for Twitter-2015, and 0.74% and 0.88% for Twitter-2017, respectively, demonstrating the superiority of our CoolNet architecture.",,6,,,
,,"Wu, Xingjiao",,,,,,,,,Multi-modal sentiment analysis,,,,,,
,,"Yang, Shuwen",,,,,,,,,Aspect-based sentiment analysis,,,,,,
,,"Xu, Junjie",,,,,,,,,Graph structure,,,,,,
,,"Zhou, Jie",,,,,,,,,,,,,,,
,,"He, Liang",,,,,,,,,,,,,,,
JOUR,Sentiment Lexical Strength Enhanced Self-supervised Attention Learning for sentiment analysis,"Wang, Xi",Knowledge-Based Systems,252,109335,2022,2022/09/27/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109335,https://www.sciencedirect.com/science/article/pii/S0950705122006694,Sentiment lexical strength,"In Natural Language Processing (NLP), attention mechanism is often used to quantify the importance of the context word in sentiment prediction. However, it tends to focus on high-frequency words, while ignoring low-frequency words that have an active effect in some positions. In this paper, we propose a Sentiment Lexical Strength Enhanced Self-supervised Attention Learning (SLS-ESAL) approach. Specifically, we iteratively mine attention supervision information from all input sentences. Then we use weights quantified by sentiment lexical strength to enhance attention learning in final training, which enables our model to continue to focus on the active context words in different positions and eliminate the effects of the misleading context ones. Experiments on three datasets show that our approach can improve sentiment analysis performance and verify attention weights can be used as an explanation for text classification.",,,,,
,,"Fan, Mengmeng",,,,,,,,,Attention supervision information,,,,,,
,,"Kong, Mingming",,,,,,,,,Sentiment analysis,,,,,,
,,"Pei, Zheng",,,,,,,,,,,,,,,
JOUR,Contextual sentiment embeddings via bi-directional GRU language model,"Wang, Jin",Knowledge-Based Systems,235,107663,2022,2022/01/10/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107663,https://www.sciencedirect.com/science/article/pii/S0950705121009254,Contextual sentiment embeddings,"Compared with conventional word embeddings, sentiment embeddings can distinguish words with similar contexts but opposite sentiment. They can be used to incorporate sentiment information from labeled corpora or lexicons by either end-to-end training or sentiment refinement. However, these methods present two major limitations. First, traditional approaches provide a fixed representation to each word but ignore the alternation of word meaning in different contexts. As a result, the polarity of a certain emotional word may vary with context, but will be assigned with a same representation. Another problem is the handling of out-of-vocabulary (OOV) or informal-writing sentiment words that would be assigned generic vectors (e.g., <UNK>). In addition, if affective words are not included in affective corpora or lexicons, they would be treated as neutral. Using such low-quality embeddings for building a neural model will reduce performance. This study proposes a training model of contextual sentiment embeddings. A stacked two-layer GRU model was used as the language model, simultaneously trained to incorporate semantic and sentiment information from labeled corpora and lexicons. To deal with OOV or informal-writing sentiment words, the WordPiece tokenizer was used to divide the text into subwords. The resulting model can be transferred to downstream applications by either feature extractor or fine-tuning. The results show that the proposed model can handle unseen or informal writing sentiment words and thus outperforms previously proposed methods.",,,,,
,,"Zhang, You",,,,,,,,,Sentiment analysis,,,,,,
,,"Yu, Liang-Chih",,,,,,,,,Pre-trained language model,,,,,,
,,"Zhang, Xuejie",,,,,,,,,Gated recurrent unit,,,,,,
JOUR,COVID-19 outbreak: An ensemble pre-trained deep learning model for detecting informative tweets,"Malla, SreeJagadeesh",Applied Soft Computing,107,107495,2021,2021/08/01/,1568-4946,https://doi.org/10.1016/j.asoc.2021.107495,https://www.sciencedirect.com/science/article/pii/S156849462100418X,COVID-19,"On 11 March 2020, the (WHO) World Health Organization declared COVID-19 (CoronaVirus Disease 2019) as a pandemic. A further crisis has manifested mass fear and panic, driven by lack of information, or sometimes outright misinformation, alongside the coronavirus pandemic. Twitter is one of the prominent and trusted social media in this current outbreak. Over time, boundless COVID-19 headlines and vast awareness have been spreading, with tweets, updates, videos, and explosive posts. Few studies have been performed on the pandemic to detect and interrelate various disease types, including current coronavirus. However, it is pretty tricky to discriminate and detect a specific category. This work is motivated by the need to inform society about limiting irrelevant information and avoiding spreading negative emotions. In this context, the current work focuses on informative tweet detection in the pandemic to provide relevant information to the government, medical organizations, victims services, etc. This paper used a Majority Voting technique-based Ensemble Deep Learning (MVEDL) model. This MVEDL model is used to identify COVID-19 related (INFORMATIVE) tweets. The state-of-art deep learning models RoBERTa, BERTweet, and CT-BERT are used for best performance with the MVEDL model. The “COVID-19 English labeled tweets” dataset is used for training and testing the MVEDL model. The MVEDL model has shown 91.75 percent accuracy, 91.14 percent F1-score and outperforms the traditional machine learning and deep learning models. We also investigate how to use the MVEDL model for sentiment analysis on 226668 unlabeled COVID-19 tweets and their informative tweets. The application section discussed a comprehensive analysis of both actual and informative tweets. According to our knowledge, this is the first work on COVID-19 sentiment analysis using a deep learning ensemble model.",,,,,
,,"P.J.A., Alphonse",,,,,,,,,Informative tweets,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,RoBERTa,,,,,,
,,,,,,,,,,,CT-BERT,,,,,,
,,,,,,,,,,,BERTweet,,,,,,
,,,,,,,,,,,Majority voting,,,,,,
,,,,,,,,,,,Health emergency,,,,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
JOUR,A comparative analysis of transformer based models for figurative language classification,"Junaid, Taha",Computers and Electrical Engineering,101,108051,2022,2022/07/01/,0045-7906,https://doi.org/10.1016/j.compeleceng.2022.108051,https://www.sciencedirect.com/science/article/pii/S0045790622003123,Figurative language,"Efficient and effective methods are required to construct a model to rapidly extractdifferent sentiments from large volumes of text. To augment the performance of the models, contemporary developments in Natural Language Processing (NLP) have been utilized by researchers to work on several model architecture and pretraining tasks. This work explores several models based on transformer architecture and analyses its performance. In this work, the researchersusea dataset to answer the question of whether or not transformers work significantly well for figurative language and not just literal language classification. The results of various models are compared and have come up as a result of research over time. The studyexplains why it is necessary for computers to understand the occurrence of figurative language, why it is yet a challenge and is being intensively worked on to date, and how it is different from literal language classification. This research also covers how well these models train on a specific type of figurative language and generalize on a few other similar types.",,,,,
,,"Sumathi, D.",,,,,,,,,Sentiment analysis,,,,,,
,,"Sasikumar, A.N.",,,,,,,,,Sarcasm,,,,,,
,,"Suthir, S.",,,,,,,,,Natural language processing,,,,,,
,,"Manikandan, J.",,,,,,,,,Long Short Term Memory (LSTM),,,,,,
,,"Khilar, Rashmita",,,,,,,,,Transformers,,,,,,
,,"Kuppusamy, P.G.",,,,,,,,,Architecture,,,,,,
,,"Janardhana Raju, M.",,,,,,,,,Fine tuning,,,,,,
,,,,,,,,,,,Hyperbole,,,,,,
,,,,,,,,,,,Rhetorical questions,,,,,,
JOUR,A holistic AI-based approach for pharmacovigilance optimization from patients behavior on social media,"Roche, Valentin",Artificial Intelligence in Medicine,144,102638,2023,2023/10/01/,0933-3657,https://doi.org/10.1016/j.artmed.2023.102638,https://www.sciencedirect.com/science/article/pii/S0933365723001525,Social network analysis,"In this paper, we propose a holistic AI-based pharmacovigilance optimization approach using patient’s social media data. Instead of focusing on the detection and identification of Adverse Drug Events (ADE) in social media posts in single time points, we propose a holistic approach that looks at the evolution of different user behavior indicators in time. We examine various NLP-based indicators such as word frequency, semantic similarity, Adverse Drug Reactions mentions, and sentiment analysis. We introduce a classification approach to identify normal vs. abnormal time periods based on patient comments. This approach, along with user behavior indicators, can optimize the pharmacovigilance process by flagging the need for immediate attention and further investigation. We specifically focus on the Levothyrox® case in France, which sparked media attention due to changes in the medication formula and affected patient behavior on medical forums. For classification, we propose a deep learning architecture called Word Cloud Convolutional Neural Network (WC-CNN), trained on word clouds from patient comments. We evaluate different temporal resolutions and NLP pre-processing techniques, finding that monthly resolution and the proposed indicators can effectively detect new safety signals, with an accuracy of 75%. We have made the code open source, available via github.",,,,,
,,"Robert, Jean-Philippe",,,,,,,,,Drug safety,,,,,,
,,"Salam, Hanan",,,,,,,,,Pharmacovigilance,,,,,,
,,,,,,,,,,,AI for healthcare,,,,,,
,,,,,,,,,,,Natural Language Processing,,,,,,
JOUR,Hindi fake news detection using transformer ensembles,"Praseed, Amit",Engineering Applications of Artificial Intelligence,119,105731,2023,2023/03/01/,0952-1976,https://doi.org/10.1016/j.engappai.2022.105731,https://www.sciencedirect.com/science/article/pii/S0952197622007217,Fake news,"In the past few decades, due to the growth of social networking sites such as Whatsapp and Facebook, information distribution has been at a level never seen before. Knowing the integrity of information has been a long-standing problem, even more so for the regional languages. Regional languages, such as Hindi, raise challenging problems for fake news detection as they tend to be resource constrained. This limits the amount of data available to efficiently train models for these languages. Most of the existing techniques to detect fake news is targeted towards the English language or involves the manual translation of the language to the English language and then proceeding with Deep Learning methods. Pre-trained transformer based models such as BERT are fine-tuned for the task of fake news detection and are commonly employed for detecting fake news. Other pre-trained transformer models, such as ELECTRA and RoBERTa have also been shown to be able to detect fake news in multiple languages after suitable fine-tuning. In this work, we propose a method for detecting fake news in resource constrained languages such as Hindi more efficiently by using an ensemble of pre-trained transformer models, all of which are individually fine-tuned for the task of fake news detection. We demonstrate that the use of such a transformer ensemble consisting of XLM-RoBERTa, mBERT and ELECTRA is able to improve the efficiency of fake news detection in Hindi by overcoming the drawbacks of individual transformer models.",,,,,
,,"Rodrigues, Jelwin",,,,,,,,,Transformer,,,,,,
,,"Thilagam, P. Santhi",,,,,,,,,Hindi fake news,,,,,,
,,,,,,,,,,,mBERT,,,,,,
,,,,,,,,,,,ELECTRA,,,,,,
,,,,,,,,,,,XLM-RoBERTa,,,,,,
,,,,,,,,,,,Ensemble,,,,,,
JOUR,Syntax-enhanced aspect-based sentiment analysis with multi-layer attention,"Shi, Jingli",Neurocomputing,557,126730,2023,2023/11/07/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126730,https://www.sciencedirect.com/science/article/pii/S0925231223008536,Aspect-based sentiment analysis,"As a key task of fine-grained sentiment analysis, aspect-based sentiment analysis aims to analyse people’s opinions at the aspect level from user-generated texts. Various sub-tasks have been defined according to different scenarios, extracting aspect terms, opinion terms, and the corresponding sentiment. However, most existing studies merely focus on a specific sub-task or a subset of sub-tasks, having many complicated models designed and developed. This hinders the practical applications of aspect-based sentiment analysis. Therefore, some unified frameworks are proposed to handle all the subtasks, but most of them suffer from two limitations. First, the syntactic features are neglected, but such features have been proven effective for aspect-based sentiment analysis. Second, very few efficient mechanisms are developed to leverage important syntactic features, e.g., dependency relations, dependency relation types, and part-of-speech tags. To address these challenges, in this paper, we propose a novel unified framework to handle all defined sub-tasks for aspect-based sentiment analysis. Specifically, based on the graph convolutional network, a multi-layer semantic model is designed to capture the semantic relations between aspect and opinion terms. Moreover, a multi-layer syntax model is proposed to learn explicit dependency relations from different layers. To facilitate the sub-tasks, the learned semantic features are propagated to the syntax model with better semantic guidance to learn the syntactic representations comprehensively. Different from the conventional syntactic model, the proposed framework introduces two attention mechanisms. One is to model dependency relation and type, and the other is to encode part-of-speech tags for detecting aspect and opinion term boundaries. Extensive experiments are conducted to evaluate the proposed novel unified framework, and the experimental results on four groups of real-world datasets explicitly demonstrate the superiority of the proposed framework over a range of baselines.",,,,,
,,"Li, Weihua",,,,,,,,,Graph convolutional network,,,,,,
,,"Bai, Quan",,,,,,,,,Aspect sentiment triplet,,,,,,
,,"Yang, Yi",,,,,,,,,Syntactic structure,,,,,,
,,"Jiang, Jianhua",,,,,,,,,,,,,,,
JOUR,Multilingual hope speech detection: A Robust framework using transfer learning of fine-tuning RoBERTa model,"Malik, Muhammad Shahid Iqbal",Journal of King Saud University - Computer and Information Sciences,35,101736,2023,2023/09/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101736,https://www.sciencedirect.com/science/article/pii/S1319157823002902,Transfer learning,"Hope Speech Detection (HSD) from social media is a new direction for promoting and supporting positive content to encourage harmony and positivity in society. As users of social media belong to different linguistic communities, hope speech detection is rarely studied as a multilingual task considering low-resource languages. Moreover, prior studies explored only monolingual techniques, and the Russian language is not addressed. This study tackles the issue of Multi-lingual Hope Speech Detection (MHSD) in English and Russian languages using the transfer learning paradigm with fine-tuning approach. We explore joint multi-lingual and translation-based approaches to tackle the task of multilingualism, where the latter approach adopts the translation mechanism to transform all content into one language and then classify them. The joint multi-lingual method handles it by designing a universal classifier for various languages. We explore the strengths of the Robustly Optimized BERT Pre-Training Approach (RoBERTa) that showed a benchmark in capturing the semantics and contextual information within the content. The proposed framework consists of several stages: 1) data preprocessing, 2) representation of data using RoBERTa models, 3) fine-tuning phase, and 4) classification of hope speech into two labels. A new Russian corpus for hope speech detection is built, containing YouTube comments. Several experiments are conducted in English and Russian languages by using semi-supervised bilingual English and Russian datasets. The findings show that the proposed framework demonstrated benchmark performance and outperformed the baselines. Furthermore, the translation-based approach (Russian-RoBERTa) offered the best performance by achieving 94% accuracy and 80.24% f1-score.",,8,,,
,,"Nazarova, Anna",,,,,,,,,Russian,,,,,,
,,"Jamjoom, Mona Mamdouh",,,,,,,,,XLM-RoBERTa,,,,,,
,,"Ignatov, Dmitry I.",,,,,,,,,Hope speech,,,,,,
,,,,,,,,,,,Translation-based,,,,,,
,,,,,,,,,,,Multi-lingual,,,,,,
JOUR,Boosting aspect category detection with inference heuristics and knowledge enhancement,"Liu, Bin",Knowledge-Based Systems,256,109855,2022,2022/11/28/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109855,https://www.sciencedirect.com/science/article/pii/S0950705122009480,Sentiment analysis,"Aspect category detection (ACD) aims to identify the aspect categories from reviewers’ expressed opinions in a given sentence, where one or multiple predefined aspect categories are mentioned explicitly or implicitly. With the boom of the pretrained language model, related studies have achieved significant improvements in ACD. However, the studies usually follow the canonical method of fine-tuning and neglect deeply mining internal knowledge or incorporating external knowledge, which could lead to suboptimal results. To address this issue, we propose a novel multilevel knowledge-aware ACD model by innovatively converting ACD to a binary sentence-pair classification task from the viewpoint of natural language inference, which is effective and consists of four key components. The model first expands the predefined aspect categories by introducing terms with high semantic similarity scores from commonsense knowledge bases. Next, the model generates synthetic premise-hypothesis sentence pairs based on the aspect categories and an inference heuristic template. Then, the training data are effectively augmented and used for fine-tuning the proposed model. Moreover, the model designs a pooling strategy to mine the rich syntactic and semantic knowledge encoded in the internal layers of BERT. Finally, the pooled low-dimensional representation is fed to a linear classifier to detect aspect categories. Experimental results on the SemEval-2014 and SemEval-2016 benchmark datasets achieve F1-scores of 92.75% and 83.58%, respectively, which demonstrate the superiority of our proposed model compared with some strong baselines.",,,,,
,,"Lin, Tao",,,,,,,,,Aspect category detection,,,,,,
,,"Li, Ming",,,,,,,,,Natural language inference,,,,,,
,,,,,,,,,,,Pretrained language model,,,,,,
JOUR,VABDC-Net: A framework for Visual-Caption Sentiment Recognition via spatio-depth visual attention and bi-directional caption processing,"Pandey, Ananya",Knowledge-Based Systems,269,110515,2023,2023/06/07/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110515,https://www.sciencedirect.com/science/article/pii/S0950705123002654,Visual-Caption Sentiment Recognition (VCSR),"People are becoming accustomed to posting images and captions on social media platforms to express their opinions. Hence, Visual-Caption Sentiment Recognition (VCSR) has been a subject of growing attention recently. Thus, the correlation between visual and caption modalities is crucial for VCSR. However, most recent VCSR strategies concatenate features from the visual and caption modalities with the help of pre-trained deep learning models containing millions of trainable parameters without adding a dedicated attention module, ultimately leading to less desirable results. Motivated by this observation, we have proposed a novel model VABDC-Net, that integrates an attention module with the convolutional neural network to focus on the most relevant information from the visual modality and attentional tokenizer-based method to extract the most relevant contextual information from the caption modality. Demanding to this dire need, the following are the significant contributions of our experimentation: (1) an attentional tokenizer-based bi-directional caption branch to retrieve useful textual features from the captions, (2) an attentional visual branch to retrieve appropriate visual features, and (3) a cross-domain feature fusion to merge multi-modal features and predict sentiment. Thorough experimentation on two benchmark datasets, Twitter-15, with an accuracy of 83.80%, and Twitter-17, with an accuracy of 72.42%, indicates that our technique outperforms existing methods for VCSR.",,,,,
,,"Vishwakarma, Dinesh Kumar",,,,,,,,,BERT,,,,,,
,,,,,,,,,,,ConvNet,,,,,,
,,,,,,,,,,,Sentiment Recognition (SR),,,,,,
,,,,,,,,,,,Multi-Modal Sentiment Recognition (MMSR),,,,,,
JOUR,What users’ musical preference on Twitter reveals about psychological disorders,"Alavijeh, Soroush Zamani",Information Processing & Management,60,103269,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103269,https://www.sciencedirect.com/science/article/pii/S0306457323000067,Psychological disorders,"Previous research found a strong relation between the users’ psychological disorders and their language use in social media posts in terms of vocabulary selection, emotional expressions, and psychometric attributes. However, although studying the association between psychological disorders and musical preference is considered as rather an old tradition in the clinical analysis of health data, it is not explored through the lens of social media analytics. In this study, we investigate which attributes of the music posted on social media are associated with mental health conditions of Twitter users. We created a large-scale dataset of 1519 Twitter users with six self-reported psychological disorders (depression, bipolar, anxiety, panic, post-traumatic stress disorder, and borderline) and matched with 2480 control users. We then conduct an observational study to investigate the relationship between the users’ psychological disorders and their musical preference by analyzing lyrics of the music tracks that the users shared on Twitter from multiple dimensions including word usage, linguistic style, sentiment and emotion patterns, topical interests and underlying semantics. Our findings reveal descriptive differences on the linguistic and semantic features of music tracks of affected users compared to control individuals and among users from different psychological disorders. Additionally, we build a feature-based and an (explainable) deep learning-based binary classifiers trained on disorder and control users and demonstrate that lyrics of the music tracks of users on Twitter can be considered as complementary information to their published posts to improve the accuracy of the disorder detection task. Overall, we find that the music attributes of users on Twitter allow inferences about their mental health status.",,3,,,
,,"Zarrinkalam, Fattane",,,,,,,,,Music preference,,,,,,
,,"Noorian, Zeinab",,,,,,,,,Social media,,,,,,
,,"Mehrpour, Anahita",,,,,,,,,Natural language processing,,,,,,
,,"Etminani, Kobra",,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Twitter,,,,,,
JOUR,Balancing between holistic and cumulative sentiment classification,"Agathangelou, Pantelis",Online Social Networks and Media,29,100199,2022,2022/05/01/,2468-6964,https://doi.org/10.1016/j.osnem.2022.100199,https://www.sciencedirect.com/science/article/pii/S2468696422000039,Sentiment analysis,"Sentiment analysis is a fast-accelerating discipline that develops algorithms for knowledge discovery from opinionated content. The challenges however, when it comes to analyzing user reviews are plenty. Bad-quality, informal use of language and lack of labels, are only a few obstacles. Most importantly, users, consciously or subconsciously, use different approaches for expressing their opinion about a product or a service. Some of them go sentence by sentence mentioning some positive and negative aspects whereas others provide a mixed piece of text where the reader is supposed to see the big picture to understand the message. In this work, we propose a novel neural network that deals with both situations. Our method, by combining convolutional, recurrent and attention neural networks can extract rich linguistic patterns that reveal the user’s sentiment towards the entity under review. We evaluate our method in nine datasets that represent both binary and multi-class classification tasks. Experimental evaluation indicates that our method outperforms well-established deep learning approaches. Our approach outperformed the competitive methods in 8 out of 9 cases.",,,,,
,,"Katakis, Ioannis",,,,,,,,,Opinion mining,,,,,,
,,,,,,,,,,,Text classification,,,,,,
JOUR,FABSA: An aspect-based sentiment analysis dataset of user reviews,"Kontonatsios, Georgios",Neurocomputing,562,126867,2023,2023/12/28/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126867,https://www.sciencedirect.com/science/article/pii/S0925231223009906,ABSA,"Aspect-based sentiment analysis (ABSA) aims at automatically extracting aspects of entities and classifying the polarity of each extracted aspect. The majority of available ABSA systems heavily rely on manually annotated datasets to train supervised machine learning models. However, the development of such manually curated datasets is a labour-intensive process and therefore existing ABSA datasets cover only a few domains and they are limited in size. In response, we present FABSA (Feedback ABSA), a new large-scale and multi-domain ABSA dataset of feedback reviews. FABSA consists of approximately 10,500 reviews which span across 10 domains. We conduct a number of experiments to evaluate the performance of state-of-the-art deep learning models when applied to the FABSA dataset. Our results demonstrate that ABSA models can generalise across different domains when trained on our FABSA dataset while the performance of the models is enhanced when using a larger training dataset. Our FABSA dataset is publicly available.11https://github.com/kontonag86/fabsa-dataset.",,,,,
,,"Clive, Jordan",,,,,,,,,Multi-domain dataset,,,,,,
,,"Harrison, Georgia",,,,,,,,,Deep learning,,,,,,
,,"Metcalfe, Thomas",,,,,,,,,,,,,,,
,,"Sliwiak, Patrycja",,,,,,,,,,,,,,,
,,"Tahir, Hassan",,,,,,,,,,,,,,,
,,"Ghose, Aji",,,,,,,,,,,,,,,
JOUR,PecidRL: Petition expectation correction and identification based on deep reinforcement learning,"Li, Ying",Information Processing & Management,60,103285,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103285,https://www.sciencedirect.com/science/article/pii/S0306457323000225,Petition expectation,"Identifying petition expectation for government response plays an important role in government administrative service. Although some petition platforms allow citizens to label the petition expectation when they submit e-petitions, the misunderstanding and misselection of petition labels still has necessitated manual classification involved. Automatic petition expectation identification has faced challenges in poor context information, heavy noise and casual syntactic structure of the petition text. In this paper we propose a novel deep reinforcement learning based method for petition expectation (citizens’ demands for the level of government response) correction and identification named PecidRL. We collect a dataset from Message Board for Leaders, the largest official petition platform in China, containing 237,042 petitions. Firstly, we introduce a deep reinforcement learning framework to automatically correct the mislabeled and ambiguous labels of the petitions. Then, multi-view textual features, including word-level and document-level semantic features, sentiment features and different textual graph representations are extracted and integrated to enrich more auxiliary information. Furthermore, based on the corrected petitions, 19 novel petition expectation identification models are constructed by extending 11 popular machine learning models for petition expectation detection. Finally, comprehensive comparison and evaluation are conducted to select the final petition expectation identification model with the best performance. After performing correction by PecidRL, each metric on all extended petition expectation identification models improves by an average of 8.3% with the highest increase ratio reaching 14.2%. The optimal model is determined as Peti-SVM-bert with the highest accuracy 93.66%. We also analyze the petition expectation label variation of the dataset by using PecidRL. We derive that 16.9% of e-petitioners tend to exaggerate the urgency of their petitions to make the government pay high attention to their appeals and 4.4% of the petitions urgency are underestimated. This study has substantial academic and practical value in improving government efficiency. Additionally, a web-server is developed to facilitate government administrators and other researchers, which can be accessed at http://www.csbg-jlu.info/PecidRL/.",,3,,,
,,"Fang, Wensi",,,,,,,,,Multi-view textual features,,,,,,
,,"Sun, Hang",,,,,,,,,Reinforcement learning,,,,,,
,,"Liu, Xiangyu",,,,,,,,,Textual graph representation,,,,,,
,,"Du, Wei",,,,,,,,,Correction and identification,,,,,,
,,"Liu, Yijun",,,,,,,,,,,,,,,
,,"Li, Qianqian",,,,,,,,,,,,,,,
JOUR,Novel fuzzy deep learning approach for automated detection of useful COVID-19 tweets,"Malla, SreeJagadeesh",Artificial Intelligence in Medicine,143,102627,2023,2023/09/01/,0933-3657,https://doi.org/10.1016/j.artmed.2023.102627,https://www.sciencedirect.com/science/article/pii/S0933365723001410,COVID-19,"Coronavirus (COVID-19) is a newly discovered viral disease from the SARS-CoV-2 family. This has caused a moral panic resulting in the spread of informative and uninformative information about COVID-19 and its effects. Twitter is a popular social media platform used extensively during the current outbreak. This paper aims to predict informative tweets related to COVID-19 on Twitter using a novel set of fuzzy rules involving deep learning techniques. This study focuses on identifying informative tweets during the pandemic to provide the public with trustworthy information and forecast how quickly diseases could spread. In this case, we have implemented RoBERTa and CT-BERT models using the fuzzy methodology to identify COVID-19 patient tweets. The proposed architecture combines deep learning transformer models RoBERTa and CT-BERT with the fuzzy technique to categorize posts as INFORMATIVE or UNINFORMATIVE. We performed a comparative analysis of our method with machine learning models and deep learning approaches. The results show that our proposed model can classify informative and uninformative tweets with an accuracy of 91.40% and an F1-score of 91.94% using the COVID-19 English tweet dataset. The proposed model is accurate and ready for real-world application.",,,,,
,,"Kumar, Lella Kranthi",,,,,,,,,Informative tweets,,,,,,
,,"Alphonse, P.J.A.",,,,,,,,,CT-BERT,,,,,,
,,,,,,,,,,,RoBERTa,,,,,,
,,,,,,,,,,,Health emergency,,,,,,
,,,,,,,,,,,Fuzzy system,,,,,,
JOUR,Pre-trained ensemble model for identification of emotion during COVID-19 based on emergency response support system dataset,"Nimmi, K.",Applied Soft Computing,122,108842,2022,2022/06/01/,1568-4946,https://doi.org/10.1016/j.asoc.2022.108842,https://www.sciencedirect.com/science/article/pii/S1568494622002368,Emergency response support system (ERSS),"The COVID-19 precautions, lockdown, and quarantine implemented throughout the epidemic resulted in a worldwide economic disaster. People are facing unprecedented levels of intense threat, necessitating professional, systematic psychiatric intervention and assistance. New psychological services must be established as quickly as possible to support the mental healthcare needs of people in this pandemic condition. This study examines the contents of calls landed in the emergency response support system (ERSS) during the pandemic. Furthermore, a combined analysis of Twitter patterns connected to emergency services could be valuable in assisting people in this pandemic crisis and understanding and supporting people’s emotions. The proposed Average Voting Ensemble Deep Learning model (AVEDL Model) is based on the Average Voting technique. The AVEDL Model is utilized to classify emotion based on COVID-19 associated emergency response support system calls (transcribed) along with tweets. Pre-trained transformer-based models BERT, DistilBERT, and RoBERTa are combined to build the AVEDL Model, which achieves the best results. The AVEDL Model is trained and tested for emotion detection using the COVID-19 labeled tweets and call content of the emergency response support system. This is the first deep learning ensemble model using COVID-19 emotion analysis to the best of our knowledge. The AVEDL Model outperforms standard deep learning and machine learning models by attaining an accuracy of 86.46 percent and Macro-average F1-score of 85.20 percent.",,,,,
,,"Janet, B.",,,,,,,,,Deep learning,,,,,,
,,"Selvan, A. Kalai",,,,,,,,,DistilBERT,,,,,,
,,"Sivakumaran, N.",,,,,,,,,RoBERTa,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Health emergency,,,,,,
,,,,,,,,,,,Emotion detection.,,,,,,
,,,,,,,,,,,COVID-19,,,,,,
,,,,,,,,,,,Ensemble model,,,,,,
JOUR,Adversarial attacks on a lexical sentiment analysis classifier,"de Oliveira, Gildásio Antonio",Computer Communications,174,154,2021,2021/06/01/,0140-3664,https://doi.org/10.1016/j.comcom.2021.04.026,https://www.sciencedirect.com/science/article/pii/S0140366421001705,Adversarial attacks,"Social media has become a relevant information source for several decision-making processes and for the definition of business strategies. As various sentiment analysis techniques are used to transform collected data into intelligence information, the sentiment classifiers used in these collection environments must be carefully studied and observed before being considered trustful and ready to be installed in decision support systems. An important research area concerns the robustness of sentiment classifiers in view of new adversarial attacks, in which small perturbations may be created by malicious users to deceive the sentiment classifiers, generating a perception different from the one that should be observed in the environment. Thus, it is important to identify and analyze the vulnerabilities of these classifiers under different strategies of adversarial attacks to propose countermeasures that can be used to mitigate such attacks. In this context, this work presents adversarial attacks related to a lexical natural language classifier. Being the target of the attacks, this classifier is used to calculate the sentiment of collected data as posted by users in various social media applications. The results indicate that the found vulnerabilities, if exploited by malicious users in applications that use the same lexical classifier, could invert or cancel the classifiers’ perception, thus generating perceptions that do not correspond to the reality for decision making. This work also proposes some countermeasures that might mitigate the implemented attacks.",171,,,,
,,"de Sousa, Rafael Timóteo",,,,,,,,,Natural language processing,,,,,,
,,"de Oliveira Albuquerque, Robson",,,,,,,,,Reverse engineering,,,,,,
,,"García Villalba, Luis Javier",,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Text mining,,,,,,
JOUR,Negativity spreads faster: A large-scale multilingual twitter analysis on the role of sentiment in political communication,"Antypas, Dimosthenis",Online Social Networks and Media,33,100242,2023,2023/01/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100242,https://www.sciencedirect.com/science/article/pii/S2468696423000010,Politics,"Social media has become extremely influential when it comes to policy making in modern societies, especially in the western world, where platforms such as Twitter allow users to follow politicians, thus making citizens more involved in political discussion. In the same vein, politicians use Twitter to express their opinions, debate among others on current topics and promote their political agendas aiming to influence voter behaviour. In this paper, we attempt to analyse tweets of politicians from three European countries and explore the virality of their tweets. Previous studies have shown that tweets conveying negative sentiment are likely to be retweeted more frequently. By utilising state-of-the-art pre-trained language models, we performed sentiment analysis on hundreds of thousands of tweets collected from members of parliament in Greece, Spain and the United Kingdom, including devolved administrations. We achieved this by systematically exploring and analysing the differences between influential and less popular tweets. Our analysis indicates that politicians’ negatively charged tweets spread more widely, especially in more recent times, and highlights interesting differences between political parties as well as between politicians and the general population.",,,,,
,,"Preece, Alun",,,,,,,,,Twitter,,,,,,
,,"Camacho-Collados, Jose",,,,,,,,,NLP,,,,,,
JOUR,COVID-19 analytics: Towards the effect of vaccine brands through analyzing public sentiment of tweets,"Shahriar, Khandaker Tayef",Informatics in Medicine Unlocked,31,100969,2022,2022/01/01/,2352-9148,https://doi.org/10.1016/j.imu.2022.100969,https://www.sciencedirect.com/science/article/pii/S2352914822001149,Data analytics,"The COVID-19 outbreak has created effects on everyday life worldwide. Many research teams at major pharmaceutical companies and research institutes in various countries have been producing vaccines since the beginning of the outbreak. There is an impact of gender on vaccine responses, acceptance, and outcomes. Worldwide promotion of the COVID-19 vaccine additionally generates a huge amount of discussions on social media platforms about diverse factors of vaccines including protection and efficacy. Twitter is considered one of the most well-known social media platforms which have been widely used to share a public opinion on vaccine-related problems in the COVID-19 pandemic. However, there is a lack of research work to analyze the public perception of COVID-19 vaccines systematically from a gender perspective. In this paper, we perform an in-depth analysis of the coronavirus vaccine-related tweets to understand the people’s sentiment towards various vaccine brands corresponding to the gender level. The proposed method focuses on the effect of COVID-19 vaccines on gender by taking into account descriptive, diagnostic, predictive, and prescriptive analytics on the Twitter dataset. We also conduct experiments with deep learning models to determine the sentiment polarities of tweets, which are positive, neutral, and negative. The results reveal that LSTM performs better compared to other models with an accuracy rate of 85.7%.",,,,,
,,"Islam, Muhammad Nazrul",,,,,,,,,Covid-19 vaccine,,,,,,
,,"Anwar, Md. Musfique",,,,,,,,,Tweet,,,,,,
,,"Sarker, Iqbal H.",,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
JOUR,"Prediction of the customers' interests using sentiment analysis in e-commerce data for comparison of Arabic, English, and Turkish languages","Savci, Pinar",Journal of King Saud University - Computer and Information Sciences,35,227,2023,2023/03/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.02.017,https://www.sciencedirect.com/science/article/pii/S131915782300054X,Sentiment analysis,"In the business world, large companies that can achieve continuity in innovation gain a significant competitive advantage. The sensitivity of these companies to follow and monitor news sources in e-commerce, social media, and forums provides important information to businesses in the decision-making process. With the large amount of data shared in these resources, sentiment analysis can be made from people's comments about services and products, users' emotions can be extracted and important feedback can be obtained. All of this is of course possible with accurate sentiment analysis. In this study, new data sets were created for Turkish, English, and Arabic, and for the first time, comparative sentiment analysis was performed from texts in three different languages. In addition, a very comprehensive study was presented to the researchers by comparing the performances of both the pre-trained language models for Turkish, Arabic, and English, as well as the deep learning and machine learning models. Our paper will guide researchers working on sentiment analysis about which methods will be more successful in texts written in different languages, which contain different types and spelling mistakes, which factors will affect the success, and how much these factors will affect the performance.",237,3,,,
,,"Das, Bihter",,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Pre-trained language models,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,E-commerce,,,,,,
JOUR,Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks,"Aslam, Ajwa",Applied Soft Computing,144,110494,2023,2023/09/01/,1568-4946,https://doi.org/10.1016/j.asoc.2023.110494,https://www.sciencedirect.com/science/article/pii/S1568494623005124,Sentiment analysis,"There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85% and 93% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80% and 93.14% accuracy, respectively.",,,,,
,,"Sargano, Allah Bux",,,,,,,,,Emotion recognition,,,,,,
,,"Habib, Zulfiqar",,,,,,,,,Multimodal attention,,,,,,
,,,,,,,,,,,Deep neural networks,,,,,,
JOUR,Breaking down linguistic complexities: A structured approach to aspect-based sentiment analysis,"Ahmed, Kanwal",Journal of King Saud University - Computer and Information Sciences,35,101651,2023,2023/09/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101651,https://www.sciencedirect.com/science/article/pii/S1319157823002057,Social media sentiment analysis,"Aspect-based sentiment analysis refers to the task of determining the sentiment polarity associated with particular aspects mentioned in a sentence or document. Previous studies have used attention-based neural network models to connect aspect terms with context words, but these models often perform poorly due to limited interaction between aspect terms and opinion words. Furthermore, these models typically focus only on explicitly stated aspect objects, which can be overly restrictive in certain scenarios. Current sentiment analysis methods that rely on aspect categories also often fail to consider the implicit placement of aspect-category information within the context. While existing models may produce strong results, they often lack domain knowledge. To address these issues, this study proposes an Aspect-position and Entity-oriented Knowledge Convolutional Graph (APEKCG) consisting of two modules: the Aspect position-aware module (APA) and the Entity oriented Knowledge Dependency Convolutional Graph (EKDCG). The APA module is designed to integrate aspect-specific sentiment features for sentiment classification by incorporating information about aspect categories into different parts of the context. The EKDCG module incorporates entity-oriented knowledge, dependency labels, and syntactic path using a dependence graph. Experimental results on five benchmarks Natural Language Processing (NLP) datasets of the English language demonstrate the effectiveness of the proposed APEKCG framework. Furthermore, the APEKCG outperformed previous state-of-the-art models with its accuracy, achieving 89.13%, 84.32%, 89.02%, 79.64%, and 90.22% on the MAMS, Laptop, Restaurant, AWARE, and SemEval-15&16 datasets, respectively.",,8,,,
,,"Nadeem, Muhammad Imran",,,,,,,,,Aspect-based sentiment Analysis,,,,,,
,,"Zheng, Zhiyun",,,,,,,,,Entity-oriented knowledge dependency convolutional graph,,,,,,
,,"Li, Dun",,,,,,,,,,,,,,,
,,"Ullah, Inam",,,,,,,,,,,,,,,
,,"Assam, Muhammad",,,,,,,,,,,,,,,
,,"Ghadi, Yazeed Yasin",,,,,,,,,,,,,,,
,,"Mohamed, Heba G.",,,,,,,,,,,,,,,
JOUR,A deep neural network model for coreference resolution in geological domain,"Wan, Bo",Information Processing & Management,60,103268,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103268,https://www.sciencedirect.com/science/article/pii/S0306457323000055,Geological text mining,"Coreference resolution of geological entities is an important task in geological information mining. Although the existing generic coreference resolution models can handle geological texts, a dramatic decline in their performance can occur without sufficient domain knowledge. Due to the high diversity of geological terminology, coreference is intricately governed by the semantic and expressive structure of geological terms. In this paper, a framework CorefRoCNN based on RoBERTa and convolutional neural network (CNN) for end-to-end coreference resolution of geological entities is proposed. Firstly, the fine-tuned RoBERTa language model is used to transform words into dynamic vector representations with contextual semantic information. Second, a CNN-based multi-scale structure feature extraction module for geological terms is designed to capture the invariance of geological terms in length, internal structure, and distribution. Thirdly, we incorporate the structural feature and word embedding for further determinations of coreference relations. In addition, attention mechanisms are used to improve the ability of the model to capture valid information in geological texts with long sentence lengths. To validate the effectiveness of the model, we compared it with several state-of-the-art models on the constructed dataset. The results show that our model has the optimal performance with an average F1 value of 79.78%, which is a 1.22% improvement compared to the second-ranked method.",,3,,,
,,"Dong, Shuai",,,,,,,,,Coreference resolution,,,,,,
,,"Chu, Deping",,,,,,,,,Deeping learning,,,,,,
,,"Li, Hong",,,,,,,,,Chinese geological texts,,,,,,
,,"Liu, Yiyang",,,,,,,,,,,,,,,
,,"Fu, Jinming",,,,,,,,,,,,,,,
,,"Fang, Fang",,,,,,,,,,,,,,,
,,"Li, Shengwen",,,,,,,,,,,,,,,
,,"Zhou, Dan",,,,,,,,,,,,,,,
JOUR,A disentangled linguistic graph model for explainable aspect-based sentiment analysis,"Mei, Xiaoyong",Knowledge-Based Systems,260,110150,2023,2023/01/25/,0950-7051,https://doi.org/10.1016/j.knosys.2022.110150,https://www.sciencedirect.com/science/article/pii/S0950705122012461,Aspect-based sentiment analysis (ABSA),"Aspect-based sentiment analysis (ABSA) aims to use interactions between aspect terms and their contexts to predict sentiment polarity for given aspects in sentences. Current mainstream approaches use deep neural networks (DNNs) combined with additional linguistic information to improve performance. DNN-based methods, however, lack explanation and transparency to support predictions, and no existing model completely solves the trade-off between explainability and performance. In contrast, most previous studies explain the relationship between input and output by attribution; however, this approach is insufficient to mine hidden semantics from abstract features. To overcome the aforementioned limitations, we propose a disentangled linguistic graph model (DLGM) to enhance transparency and performance by guiding the signal flow. First, we propose a disentangled linguistic representation learning module that extracts a specific linguistic property via neurons to help capture finer feature representations. To further boost explainability, we propose a supervised disentangling module, in which labeled linguistic data help reduce information redundancy. Finally, a cross-linguistic routing mechanism is introduced into the signal propagation of linguistic chunks to overcome the defect of distilling information in an intralinguistic property. Quantitative and qualitative experiments verify the effectiveness and superiority of the proposed DLGM in sentiment polarity classification and explainability.",,,,,
,,"Zhou, Yougen",,,,,,,,,Explainability,,,,,,
,,"Zhu, Chenjing",,,,,,,,,Disentangled graph representation learning,,,,,,
,,"Wu, Mengting",,,,,,,,,,,,,,,
,,"Li, Ming",,,,,,,,,,,,,,,
,,"Pan, Shirui",,,,,,,,,,,,,,,
JOUR,Aspect-based sentiment analysis with attention-assisted graph and variational sentence representation,"Feng, Shi",Knowledge-Based Systems,258,109975,2022,2022/12/22/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109975,https://www.sciencedirect.com/science/article/pii/S0950705122010681,Aspect-based sentiment analysis,"Aspect-based sentiment analysis (ABSA) is a fine-grained task that detects the sentiment polarities of particular aspect words in a sentence. With the rise of graph convolution networks (GCNs), current ABSA models mostly use graph-based methods. These methods construct a dependency tree for each sentence, and regard each word as a unique node. To be more specific, they conduct classification using aspect representations instead of sentence representations, and update them with GCNs. However, this kind of method relies too much on the quality of the dependency tree and may lose the global sentence information, which is also helpful for classification. To deal with these, we design a new ABSA model AG-VSR. Two kinds of representations are proposed to perform the final classification, Attention-assisted Graph-based Representation (A2GR) and Variational Sentence Representation (VSR). A2GR is produced by the GCN module, which inputs a dependency tree modified by the attention mechanism. Furthermore, VSR is sampled from a distribution learned by a VAE-like encoder–decoder structure. Extensive experiments show that our model AG-VSR achieves competitive results. Our code and data have been released in https://github.com/wangbing1416/VAGR.",,,,,
,,"Wang, Bing",,,,,,,,,Graph neural network,,,,,,
,,"Yang, Zhiyao",,,,,,,,,Encoder–decoder,,,,,,
,,"Ouyang, Jihong",,,,,,,,,Self-attention,,,,,,
JOUR,On the class separability of contextual embeddings representations – or “The classifier does not matter when the (text) representation is so good!”,"de Andrade, Claudio M.V.",Information Processing & Management,60,103336,2023,2023/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103336,https://www.sciencedirect.com/science/article/pii/S0306457323000730,Transformers,"The literature has not fully and adequately explained why contextual (e.g., BERT-based) representations are so successful to improve the effectiveness of some Natural Language Processing tasks, especially Automatic Text Classifications (ATC). In this article, we evince that such representations, when properly tuned to a target domain, produce an extremely separable space that makes the classification task very effective, independently of the classifier employed for solving the ATC task. To demonstrate our hypothesis, we perform a thorough class separability analysis in order to visualize and measure how well BERT-based embeddings separate documents of different classes in comparison with other widely used representation approaches, e.g., TFIDF BoW, static embeddings (e.g., fastText) and zero-shot (non-tuned) contextual embeddings. We also analyze separability in the context of transfer learning and compare BERT-based representations with those obtained from other transformers (e.g., RoBERTa, XLNET). Our experiments covering sixteen datasets in topic and sentiment classification, eight classification methods and three class separability metrics show that the fine-tuned BERT embeddings are highly separable in the corresponding space (e.g., they are 67% more separable than the static embeddings). As a consequence, they allow the simplest classifiers to achieve similar effectiveness as the most complex methods. We also find moderate to high correlations between separability and effectiveness in all experimented scenarios. Overall, our main finding is that more discriminative (i.e., separable) textual representations constitute a critical part of the ATC solutions that, given the current state-of-the-art in classification algorithms, are more prominent than the algorithmic (classifier) method for solving the task.",,4,,,
,,"Belém, Fabiano M.",,,,,,,,,Text representations,,,,,,
,,"Cunha, Washington",,,,,,,,,Separability of representations,,,,,,
,,"França, Celso",,,,,,,,,Transfer learning,,,,,,
,,"Viegas, Felipe",,,,,,,,,,,,,,,
,,"Rocha, Leonardo",,,,,,,,,,,,,,,
,,"Gonçalves, Marcos André",,,,,,,,,,,,,,,
JOUR,Sentiment analysis of imbalanced datasets using BERT and ensemble stacking for deep learning,"Habbat, Nassera",Engineering Applications of Artificial Intelligence,126,106999,2023,2023/11/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106999,https://www.sciencedirect.com/science/article/pii/S0952197623011831,BERT,"The Internet is a crucial way to share information in both personal and professional areas. Sentiment analysis attracts great interest in marketing, research, and business today. The instability faced by imbalanced datasets on sentiment analysis is examined in this research. Balancing the datasets using techniques based on under-sampling and over-sampling is examined to achieve more efficient classification results as the effects of using BERT as word embedding and ensemble learning methods for classification. The effects of the resampling training set algorithms on different deep learning classifiers were investigated using BERT as a word embedding model and Cohen's kappa, accuracy, ROC-AUC curve, and MCC as evaluation metrics with k-fold validation on three sentiment analysis datasets containing English, Arabic, and Moroccan Arabic Dialect texts. Also, we did those performance metrics for all models when scaling the dataset for training and testing, and we calculated the memory and the execution time for each model. Finally, we analyzed the National Office of Railways of Morocco (ONCF) customers' Facebook comments in Modern Standard Arabic (MSA) and MD to determine customer satisfaction as positive, negative, and neutral comments.",,,,,
,,"Nouri, Hicham",,,,,,,,,Deep learning,,,,,,
,,"Anoun, Houda",,,,,,,,,Ensemble learning,,,,,,
,,"Hassouni, Larbi",,,,,,,,,Imbalanced datasets,,,,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,SMOTE,,,,,,
JOUR,An ensemble model for classifying idioms and literal texts using BERT and RoBERTa,"Briskilal, J",Information Processing & Management,59,102756,2022,2022/01/01/,0306-4573,https://doi.org/10.1016/j.ipm.2021.102756,https://www.sciencedirect.com/science/article/pii/S0306457321002375,BERT,"An idiom is a common phrase that means something other than its literal meaning. Detecting idioms automatically is a serious challenge in natural language processing (NLP) domain applications like information retrieval (IR), machine translation and chatbot. Automatic detection of Idioms plays an important role in all these applications. A fundamental NLP task is text classification, which categorizes text into structured categories known as text labeling or categorization. This paper deals with idiom identification as a text classification task. Pre-trained deep learning models have been used for several text classification tasks; though models like BERT and RoBERTa have not been exclusively used for idiom and literal classification. We propose a predictive ensemble model to classify idioms and literals using BERT and RoBERTa, fine-tuned with the TroFi dataset. The model is tested with a newly created in house dataset of idioms and literal expressions, numbering 1470 in all, and annotated by domain experts. Our model outperforms the baseline models in terms of the metrics considered, such as F-score and accuracy, with a 2% improvement in accuracy.",,1,,,
,,"Subalalitha, C.N.",,,,,,,,,RoBERTa,,,,,,
,,,,,,,,,,,Ensemble model,,,,,,
,,,,,,,,,,,Idiom,,,,,,
,,,,,,,,,,,Literal classification,,,,,,
JOUR,Complete quadruple extraction using a two-stage neural model for aspect-based sentiment analysis,"Zhang, Hua",Neurocomputing,492,452,2022,2022/07/01/,0925-2312,https://doi.org/10.1016/j.neucom.2022.04.027,https://www.sciencedirect.com/science/article/pii/S0925231222003939,Aspect-based sentiment analysis,"Aspect-based sentiment analysis (ABSA) is a fine-grained task which aims to identify the emotional polarity of a specific aspect in a text or sentence. Aspect term extraction (ATE), opinion term extraction (OTE) and aspect polarity classification (APC) are three main subtasks of the ABSA task. Nowadays, researchers mainly focus on a single task or a joint task composed of these three subtasks, and such investigation on the sentiment analysis is not sufficient. In this paper, we firstly introduce a complete aspect sentiment analysis task, called Aspect Sentiment Quadruple Extraction, which also includes the category detection beside ATE, OTE and APC. Then we propose a two-stage neural network model composed of several modules, including BiLSTM, simple gated self-attention and position encoding for this joint task. In the first stage, the proposed model extracts aspect and opinion terms as well as their categories and polarities. Moreover, the second stage mainly includes a relation classifier to validate the aspect-opinion pairs and then finalizes the complete quadruple extraction. The experimental results, evaluated on a benchmark dataset of Chinese product reviews, show that our proposed model outperforms other baseline methods and achieves the start-of-art performance.",463,,,,
,,"Chen, Zeqi",,,,,,,,,Sentiment quadruple extraction,,,,,,
,,"Chen, Bi",,,,,,,,,Two-stage neural model,,,,,,
,,"Hu, Biao",,,,,,,,,Deep learning,,,,,,
,,"Li, Mian",,,,,,,,,Sequence labeling,,,,,,
,,"Yang, Cheng",,,,,,,,,Relation classifier,,,,,,
,,"Jiang, Bo",,,,,,,,,,,,,,,
JOUR,A method for reducing the impact of information risks on a megaproject life cycle based on a semantic information field,"Kuznetsov, Igor A.",Procedia Computer Science,190,500,2021,2021/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2021.06.108,https://www.sciencedirect.com/science/article/pii/S1877050921013661,decision support system,"This paper consider a comprehensive method for devising loyalty programs based on the stages of a life cycle of an international megaproject. The method is based on the analysis of information risks and their management. The method is focused on aggregation and processing of data from various sources of textual information, which demonstrates the attitudes of key categories of individuals regarding the implementation of a megaproject at its numerous life cycle stages. The semantic information field is formed using hardware and software based on Neural Network Technologies. Authors examine the most popular neural network architectures that are used in the sentiment analysis. The paper describes a comparative analysis of classification accuracy of neural network architectures based on volume of texts and neural network profitability to sentiment analysis of large and small volumes of text. The method is aimed at managing and influencing information flow that accompanies the implementation of a megaproject stages. The application of semantic information field makes it possible to account for the informational risks of a megaproject and to prepare an effective set of measures to counteract these risks in a timely manner. This work was supported by RFBR grant № 20-010-00708\20.",507,,2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society,,
,,"Koptelov, Matvey V.",,,,,,,,,information risks,,,,,,
,,"Kovtun, Dmitriy A.",,,,,,,,,neural network,,,,,,
,,"Guseva, Anna I.",,,,,,,,,sentiment analysis,,,,,,
,,,,,,,,,,,natural language processing,,,,,,
JOUR,Back to common sense: Oxford dictionary descriptive knowledge augmentation for aspect-based sentiment analysis,"Jin, Weiqiang",Information Processing & Management,60,103260,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103260,https://www.sciencedirect.com/science/article/pii/S0306457322003612,Natural language understanding,"Aspect-based Sentiment Analysis (ABSA) is a crucial natural language understanding (NLU) research field which aims to accurately recognize reviewers’ opinions on different aspects of products and services. Despite the prominence of recent ABSA applications, mainstream ABSA approaches inevitably rely on large-scale supervised corpora, and their final performances is susceptible to the quality of the training datasets. However, annotating sufficient data is labour intensive, which presents a significant barrier for generalizing a high-quality sentiment analysis model. Nonetheless, humans can make more accurate judgement based on their external background knowledge, such as factoid triples knowledge and event causality. Inspired by the investigations on external knowledge enhancement strategies in other popular NLP research, we propose a novel knowledge augmentation framework for ABSA, named the Oxford Dictionary descriptive knowledge-infused aspect-based sentiment analysis (DictABSA). Comprehensive experiments with many state-of-the-art approaches on several widely used benchmarks demonstrate that our proposed DictABSA significantly outperforms previous mainstream ABSA methods. For instance, compared with the baselines, our BERT-based knowledge infusion strategy achieves a substantial 6.42% and 5.26% absolute accuracy gain when adopting BERT-SPC on SemEval2014 and ABSA-DeBERTa on ACLShortData, respectively. Furthermore, to effectively make use of dictionary knowledge we devise several alternative knowledge infusion strategies. Extensive experiments using different knowledge infused strategies further demonstrate that the proposed knowledge infusion strategies effectively enhance the sentiment polarity identification capability. The Python implementation of our DictABSA is publicly available at https://github.com/albert-jin/DictionaryFused-E2E-ABSA.",,3,,,
,,"Zhao, Biao",,,,,,,,,Aspect-based sentiment analysis,,,,,,
,,"Zhang, Liwen",,,,,,,,,Knowledge infusion mechanisms,,,,,,
,,"Liu, Chenxing",,,,,,,,,Pre-trained language models,,,,,,
,,"Yu, Hang",,,,,,,,,Model hot-plugging technique,,,,,,
JOUR,Construction of an aspect-level sentiment analysis model for online medical reviews,"Zhao, Yuehua",Information Processing & Management,60,103513,2023,2023/11/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103513,https://www.sciencedirect.com/science/article/pii/S0306457323002509,Online medical review,"Online medical services have become increasingly popular, and patient feedback can significantly influence other patients’ medical decision-making. This study utilizes a double-layer domain ontology for conducting aspect-level sentiment analysis of reviews from online medical platforms. A double-layer aspect recognition model (OMR-ARM), aggregating the knowledge of the domain ontology, is built to identify the aspects of online medical reviews. The proposed model outperforms baseline models by up to 23.12%. Incorporating this model into a series of state-of-the-art models, the resultant OMR-ALSA model achieves a F1-score of 93.53% for aspect-level sentiment analysis of online medical reviews. Additionally, this study develops an object-aspect-sentiment knowledge graph of online medical reviews (OMR-KG) that can classify patients’ sentimental polarities towards the different aspects of online medical reviews. The proposed model and constructed KG have the potential to provide reference and guidance to sentiment analysis research in the online medical review domain, thus contributing to more informed and personalized healthcare decision-making.",,6,,,
,,"Zhang, Linyi",,,,,,,,,Fine-grained sentiment analysis,,,,,,
,,"Zeng, Chenxi",,,,,,,,,Aspect-level sentiment analysis,,,,,,
,,"Lu, Wenrui",,,,,,,,,Ontology,,,,,,
,,"Chen, Yidan",,,,,,,,,Knowledge graph,,,,,,
,,"Fan, Tao",,,,,,,,,,,,,,,
JOUR,An empirical study of IoT security aspects at sentence-level in developer textual discussions,"Mandal, Nibir",Information and Software Technology,150,106970,2022,2022/10/01/,0950-5849,https://doi.org/10.1016/j.infsof.2022.106970,https://www.sciencedirect.com/science/article/pii/S0950584922001082,IoT,Context:,,,,"found that (1) SecBot is based on the retraining of the deep learning model RoBERTa. SecBot offers the best F1-Score of .935, (2) there are six error categories in misclassified samples by SecBot. SecBot was mostly wrong when the keywords/contexts were ambiguous (e.g., ‘gateway’ can be a security gateway or a simple gateway), (3) there are 9 security topics grouped into three categories: Software, Hardware, and Network, and (4) the highest number of topics belongs to software security, followed by network security and hardware security.","this paper, we aim to understand the challenges IoT developers face while applying security practices and techniques to IoT devices. We have two goals: (1) Develop a model that can automatically find security-related IoT discussions in SO, and (2) Study the model output (i.e., the security discussions) to learn about IoT developer security-related challenges."
,,"Uddin, Gias",,,,,,,,,Security,"IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, ensuring the security of IoT devices is crucial. IoT devices can differ from traditional computing (e.g., low power, storage, computing), thereby the design and implementation of proper security measures can be challenging in IoT devices. We observed that IoT developers discuss their security-related challenges in developer forums like Stack Overflow (SO). However, we find that IoT security discussions can also be buried inside non-security discussions in SO.",,,,Conclusion:,Methods:
,,,,,,,,,,,Stack overflow,Objective:,,,,"IoT researchers and vendors can use SecBot to collect and analyze security-related discussions from developer discussions in SO. The analysis of nine security-related topics can guide major IoT stakeholders like IoT Security Enthusiasts, Developers, Vendors, Educators, and Researchers in the rapidly emerging IoT ecosystems.","First, we download all 53K posts from StackOverflow (SO) that contain discussions about various IoT devices, tools, and techniques. Second, we manually labeled 5,919 sentences from 53K posts as 1 or 0 (i.e., whether they contain a security aspect or not). Third, we then use this benchmark to investigate a suite of deep learning transformer models. The best performing model is called SecBot. Fourth, we apply SecBot on the entire 53K posts and find around 30K sentences labeled as security. Fifth, we apply topic modeling to the 30K security-related sentences labeled by SecBot. Then we label and categorize the topics. Sixth, we analyze the evolution of the topics in SO."
,,,,,,,,,,,Deep learning,,,,,,Results:
,,,,,,,,,,,Empirical study,,,,,,
JOUR,MetaPro: A computational metaphor processing model for text pre-processing,"Mao, Rui",Information Fusion,86-87,30,2022,2022/10/01/,1566-2535,https://doi.org/10.1016/j.inffus.2022.06.002,https://www.sciencedirect.com/science/article/pii/S1566253522000550,Metaphor identification,"Metaphor is a special linguistic phenomenon, challenging diverse natural language processing tasks. Previous works focused on either metaphor identification or domain-specific metaphor interpretation, e.g., interpreting metaphors with a specific part-of-speech, metaphors in a specific application scenario or metaphors with specific concepts. These methods cannot be used directly in everyday texts. In this paper, we propose a metaphor processing model, termed MetaPro, which integrates metaphor identification and interpretation modules for text pre-processing. To the best of our knowledge, this is the first end-to-end metaphor processing approach in the present field. MetaPro can identify metaphors in a sentence on token-level, paraphrasing the identified metaphors into their literal counterparts, and explaining metaphoric multi-word expressions. It achieves state-of-the-art performance in the evaluation of sub-tasks. Besides, the model can be used as a text pre-processing method to support downstream tasks. We examine the utility of MetaPro text pre-processing on a news headline sentiment analysis task. The experimental results show that the performance of sentiment analysis classifiers can be improved with the pre-processed texts.",43,,,,
,,"Li, Xiao",,,,,,,,,Metaphor interpretation,,,,,,
,,"Ge, Mengshi",,,,,,,,,,,,,,,
,,"Cambria, Erik",,,,,,,,,,,,,,,
JOUR,Evading text based emotion detection mechanism via adversarial attacks,"Bajaj, Ashish",Neurocomputing,558,126787,2023,2023/11/14/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126787,https://www.sciencedirect.com/science/article/pii/S0925231223009104,Adversarial Attack,"Textual Emotion Analysis (TEA) seeks to extract and assess the emotional states of users from the text. Various Deep Learning (DL) algorithms have emerged rapidly and demonstrated success in numerous disciplines, including audio, image, and natural language processing. Thetrend has shifted a growing number of researchers from standard machine learning to DL for scientific study. Using DL approaches, we offer an overview of TEA in this paper. After introducing the background for emotion analysis, including the definition of emotion, emotion classification methods, and application domains of emotion analysis, we demonstrated that, despite the immense success of deep learning models in NLP-related tasks, they are susceptible to adversarial attacks, which can lead to incorrect emotion classification. An adversarial text is constructed by altering a few words or characters so as to keep the overall semantic similarity of emotion for a human reader while tricking the machine into making erroneous predictions. This study demonstrates the vulnerability of emotion categorization by generating adversarial text using a variety of cutting-edge attack techniques. Comprehensive experiments are performed to assess the effectiveness of the attack methods against several widely-used models, such as Word-CNN, Bi-LSTM, and four powerful transformer models, namely BERT, DistilBERT, ALBERT, and RoBERTa. These models were trained on an emotion dataset utilized for the purpose of emotion classification. We evaluated and analyzed the behavior of different models under a variety of attack conditions to determine which is the most and least vulnerable. Also, we determine which perturbation technique affects transformer models the most. Using Attack Success Rates (ASR) as our evaluation metric, we have assessed the potential outcomes. The findings reveal that methodologies for classifying emotion predictioncan be circumvented, which has implications for existing policy measures.",,,,,
,,"Kumar Vishwakarma, Dinesh",,,,,,,,,Textual Emotion Analysis (TEA),,,,,,
,,,,,,,,,,,Natural Language Processing (NLP),,,,,,
,,,,,,,,,,,Deep Learning (DL),,,,,,
,,,,,,,,,,,Vulnerability,,,,,,
,,,,,,,,,,,Transformers Semantic,,,,,,
,,,,,,,,,,,Similarity,,,,,,
JOUR,What do Twitter comments tell about news article bias? Assessing the impact of news article bias on its perception on Twitter,"Spinde, Timo",Online Social Networks and Media,37-38,100264,2023,2023/09/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100264,https://www.sciencedirect.com/science/article/pii/S246869642300023X,Media bias,"News stories circulating online, especially on social media platforms, are nowadays a primary source of information. Given the nature of social media, news no longer are just news, but they are embedded in the conversations of users interacting with them. This is particularly relevant for inaccurate information or even outright misinformation because user interaction has a crucial impact on whether information is uncritically disseminated or not. Biased coverage has been shown to affect personal decision-making. Still, it remains an open question whether users are aware of the biased reporting they encounter and how they react to it. The latter is particularly relevant given that user reactions help contextualize reporting for other users and can thus help mitigate but may also exacerbate the impact of biased media coverage. This paper approaches the question from a measurement point of view, examining whether reactions to news articles on Twitter can serve as bias indicators, i.e., whether how users comment on a given article relates to its actual level of bias. We first give an overview of research on media bias before discussing key concepts related to how individuals engage with online content, focusing on the sentiment (or valance) of comments and on outright hate speech. We then present the first dataset connecting reliable human-made media bias classifications of news articles with the reactions these articles received on Twitter. We call our dataset BAT - Bias And Twitter. BAT covers 2,800 (bias-rated) news articles from 255 English-speaking news outlets. Additionally, BAT includes 175,807 comments and retweets referring to the articles. Based on BAT, we conduct a multi-feature analysis to identify comment characteristics and analyze whether Twitter reactions correlate with an article’s bias. First, we fine-tune and apply two XLNet-based classifiers for hate speech detection and sentiment analysis. Second, we relate the results of the classifiers to the article bias annotations within a multi-level regression. The results show that Twitter reactions to an article indicate its bias, and vice-versa. With a regression coefficient of 0.703 (p<0.01), we specifically present evidence that Twitter reactions to biased articles are significantly more hateful. Our analysis shows that the news outlet’s individual stance reinforces the hate-bias relationship. In future work, we will extend the dataset and analysis, including additional concepts related to media bias.",,,,,
,,"Richter, Elisabeth",,,,,,,,,Sentiment analysis,,,,,,
,,"Wessel, Martin",,,,,,,,,Hate speech detection,,,,,,
,,"Kulshrestha, Juhi",,,,,,,,,Transfer learning,,,,,,
,,"Donnay, Karsten",,,,,,,,,,,,,,,
JOUR,TextCNN-based ensemble learning model for Japanese Text Multi-classification,"Chen, Hua",Computers and Electrical Engineering,109,108751,2023,2023/08/01/,0045-7906,https://doi.org/10.1016/j.compeleceng.2023.108751,https://www.sciencedirect.com/science/article/pii/S0045790623001751,ALBERT,"In this paper, we aim at improving Japanese text classification using TextCNN-based ensemble learning model. Specifically, we first construct three different sub-classifiers, combining ALBERT, RoBERTa, DistilBERT with TextCNN, respectively; and then explore the effectiveness of ensemble learning model to leverage complementary information from different sub-classifiers for better text classification. We also conduct a series of experiments with the dataset collected from Japanese Wikipedia pages, which was divided into 31 categories. The experimental results show that the proposed approach achieves a good performance. The accuracy, precision, recall and F1 scores reach 0.881, 0.884, 0.880 and 0.881, respectively, which shows that the TextCNN-based ensemble learning model can be used for Japanese Text Multi-Classification effectively.",,,,,
,,"Zhang, Zepeng",,,,,,,,,RoBERTa,,,,,,
,,"Huang, Shiting",,,,,,,,,DistilBERT,,,,,,
,,"Hu, Jiayu",,,,,,,,,TextCNN,,,,,,
,,"Ni, Wenlong",,,,,,,,,Ensemble learning,,,,,,
,,"Liu, Jianming",,,,,,,,,Japanese text classification,,,,,,
TY,T1,AU,JO,VL,SP,PY,DA,SN,DO,UR,KW,AB,IS,EP,T2,,
JOUR,Refined SBERT: Representing sentence BERT in manifold space,"Chu, Yonghe",Neurocomputing,555,126453,2023,2023/10/28/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126453,https://www.sciencedirect.com/science/article/pii/S0925231223005763,Natural language processing,"Distributed sentence representations have shown great power in a wide range of natural language processing (NLP) tasks. Meanwhile, a contextualized sentence representation, called Sentence BERT (SBERT), achieves excellent performance in quite a few NLP tasks. However, Sentence BERT is typically learned in the Euclidean space, the geometric structure of sentence representations and their relations to the representations of sentence’s contexts has not been carefully studied yet. In this paper, we propose a new sentence representation method, named Refined SBERT, which utilizes manifold learning to refine sentence BERT by re-embedding sentence vectors from the original embedding space to a new refined semantic space. In order to map sentences to manifold space, we utilize neighborhood preserving embedding to construct the local manifold structure of the sentences. Our method can discover the local geometric structure and obtain a compact sentence BERT subspace, which can best detect the essential semantic structure. We conduct comprehensive experiments on various sentence embedding tasks including semantic textual similarity tasks, text classification and document clustering, and the experimental results show that the proposed model achieved promising results, comparing to several popular sentence representations.",,,,,
,,"Cao, Heling",,,,,,,,,Sentence BERT,,,,,,
,,"Diao, Yufeng",,,,,,,,,Manifold learning,,,,,,
,,"Lin, Hongfei",,,,,,,,,Local geometric structure,,,,,,
JOUR,Towards COVID-19 fake news detection using transformer-based models,"Alghamdi, Jawaher",Knowledge-Based Systems,274,110642,2023,2023/08/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110642,https://www.sciencedirect.com/science/article/pii/S0950705123003921,COVID-19,"The COVID-19 pandemic has resulted in a surge of fake news, creating public health risks. However, developing an effective way to detect such news is challenging, especially when published news involves mixing true and false information. Detecting COVID-19 fake news has become a critical task in the field of natural language processing (NLP). This paper explores the effectiveness of several machine learning algorithms and fine-tuning pre-trained transformer-based models, including Bidirectional Encoder Representations from Transformers (BERT) and COVID-Twitter-BERT (CT-BERT), for COVID-19 fake news detection. We evaluate the performance of different downstream neural network structures, such as CNN and BiGRU layers, added on top of BERT and CT-BERT with frozen or unfrozen parameters. Our experiments on a real-world COVID-19 fake news dataset demonstrate that incorporating BiGRU on top of the CT-BERT model achieves outstanding performance, with a state-of-the-art F1 score of 98%. These results have significant implications for mitigating the spread of COVID-19 misinformation and highlight the potential of advanced machine learning models for fake news detection.",,,,,
,,"Lin, Yuqing",,,,,,,,,Fake news,,,,,,
,,"Luo, Suhuai",,,,,,,,,Misinformation,,,,,,
,,,,,,,,,,,Pre-trained transformer models,,,,,,
,,,,,,,,,,,Social media,,,,,,
JOUR,Selecting and combining complementary feature representations and classifiers for hate speech detection,"Cruz, Rafael M.O.",Online Social Networks and Media,28,100194,2022,2022/03/01/,2468-6964,https://doi.org/10.1016/j.osnem.2021.100194,https://www.sciencedirect.com/science/article/pii/S2468696421000719,Hate speech,"Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS.",,,,,
,,"de Sousa, Woshington V.",,,,,,,,,Text classification,,,,,,
,,"Cavalcanti, George D.C.",,,,,,,,,Multiple classifiers system,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
JOUR,"BERT models for Brazilian Portuguese: Pretraining, evaluation and tokenization analysis","Souza, F.C.",Applied Soft Computing,,110901,2023,2023/10/07/,1568-4946,https://doi.org/10.1016/j.asoc.2023.110901,https://www.sciencedirect.com/science/article/pii/S1568494623009195,Language model,"Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of large pretrained language models (LMs) to downstream natural language processing (NLP) tasks. This transfer learning approach improves the overall performance on many tasks and is highly beneficial when labeled data is scarce, making pretrained LMs valuable resources specially for languages with few annotated training examples. In this work, we train BERT (Bidirectional Encoder Representations from Transformers) models for Brazilian Portuguese, which we nickname BERTimbau. We evaluate our models on three downstream NLP tasks: sentence textual similarity, recognizing textual entailment, and named entity recognition. Our models improve the state-of-the-art in all of these tasks, outperforming Multilingual BERT and confirming the effectiveness of large pretrained LMs for Portuguese. We release our models to the community hoping to provide strong baselines for future NLP research: https://github.com/neuralmind-ai/portuguese-bert.",,,,,
,,"Nogueira, R.F.",,,,,,,,,BERT,,,,,,
,,"Lotufo, R.A.",,,,,,,,,Sentence textual similarity,,,,,,
,,,,,,,,,,,Recognizing textual entailment,,,,,,
,,,,,,,,,,,Named entity recognition,,,,,,
JOUR,Multi-view informed attention-based model for Irony and Satire detection in Spanish variants,"Ortega-Bueno, Reynier",Knowledge-Based Systems,235,107597,2022,2022/01/10/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107597,https://www.sciencedirect.com/science/article/pii/S0950705121008595,Irony and satire,"Making machines understand language and reasoning on it has been one of the most challenging problems addressed by Artificial Intelligent researchers. This challenge increases when figurative language is used for communicating complex meanings, intentions, emotions and attitudes in creative and funny ways. In fact, sentiment analysis approaches struggle when facing irony, satire and other figurative languages, particularly those where the explanation of a prediction might arguably be as necessary as the prediction itself. This paper describes a new model MvAttLSTM based on deep learning for irony and satire detection in tweets written in distinct Spanish variants. The proposed model is based on an attentive-LSTM informed with three additional views learned from distinct perspectives. We investigate two strategies to pass these views into MvAttLSTM. We perform an extensive evaluation on three corpora, one for irony detection and two for satire detection. Moreover, in order to study the robustness of our proposed model, we investigate its performance on humor recognition. Experiments confirm that the proposed views help our model to improve its performance. Moreover, they show that affective information benefits our model to detect irony and satire. In particular, a first analysis of the results highlights the discriminating power of emotional features obtained from SenticNet and SEL lexicon. Overall, our system achieves the state-of-the-art performance in irony and satire detection in Spanish variants and competitive results in humor recognition.",,,,,
,,"Rosso, Paolo",,,,,,,,,Attention mechanism,,,,,,
,,"Medina Pagola, José E.",,,,,,,,,Linguistic features,,,,,,
,,,,,,,,,,,Contextualized pre-trained embedding,,,,,,
,,,,,,,,,,,Fusing representation,,,,,,
,,,,,,,,,,,Spanish variants,,,,,,
,,,,,,,,,,,Figurative language,,,,,,
JOUR,Analytics-driven complaint prioritisation via deep learning and multicriteria decision-making,"Vairetti, Carla",European Journal of Operational Research,312,1108,2024,2024/02/01/,0377-2217,https://doi.org/10.1016/j.ejor.2023.08.027,https://www.sciencedirect.com/science/article/pii/S0377221723006562,Analytics,"Complaint analysis is an essential business analytics application because complaints have a strong influence on customer satisfaction (CSAT). However, the process of categorising and prioritising complaints manually can be extremely time consuming for large companies. In this paper, we propose a framework for automatic complaint labelling and prioritisation using text analytics and operational research techniques. The labelling step of the training set is performed using a simple weighting approach from the multiple-criteria decision-making (MCDM) literature, while transformer-based deep learning (DL) techniques are used for text classification. We define two priority classes, namely, urgent complaints and other claims, and develop a system for automatic complaint categorisation. Our experimental results show that excellent predictive performance can be achieved with state-of-the-art text classification models. In particular, BETO, a bidirectional encoder representations from transformers (BERT) model trained on a large Spanish corpus, reaches an accuracy (ACCU) and area under the curve (AUC) of 92.1% and 0.9785, respectively. This positive result translates into a successful complaint prioritisation scheme, which improves CSAT and reduces the churn rate.",3,1118,,,
,,"Aránguiz, Ignacio",,,,,,,,,Complaint management,,,,,,
,,"Maldonado, Sebastián",,,,,,,,,Text analytics,,,,,,
,,"Karmy, Juan Pablo",,,,,,,,,Deep learning,,,,,,
,,"Leal, Alonso",,,,,,,,,BERT,,,,,,
JOUR,Enhancing text representations separately with entity descriptions,"Zhao, Qinghua",Neurocomputing,552,126511,2023,2023/10/01/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126511,https://www.sciencedirect.com/science/article/pii/S0925231223006343,Knowledge enhancement,"Several studies have focused on incorporating language models with entity descriptions to facilitate the model with a better understanding of knowledge. Existing methods usually either integrate descriptions in the pre-training stage by designing description-related tasks, or in the fine-tuning stage by directly appending description strings to the original input, this paper falls into the latter group. We separate entity descriptions from the original text and process them by another lighter module. Specifically, we use the original large model to encode the original input, while the lighter module processes the entity descriptions. We also propose a layer-wise fusion strategy to deeply couple the representations of the input and descriptions. To further improve the fusion of the two representations, we explore two auxiliary tasks: the entity-description enhancement task and the entity contrastive task. Experiments on (Open Entity, FIGER, FewRel, TACRED, SST) datasets yield respective improvements of (0.9, 1.4, 0.6, 0.5, 0.3). Utilizing ChatGPT as the description embedding method holds the potential for even more promising results.",,,,,
,,"Lei, Yuxuan",,,,,,,,,Entity,,,,,,
,,"Wang, Qiang",,,,,,,,,Entity description,,,,,,
,,"Kang, Zhongfeng",,,,,,,,,,,,,,,
,,"Liu, Junfeng",,,,,,,,,,,,,,,
JOUR,Fake news detection in Dravidian languages using transfer learning with adaptive finetuning,"Raja, Eduri",Engineering Applications of Artificial Intelligence,126,106877,2023,2023/11/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106877,https://www.sciencedirect.com/science/article/pii/S0952197623010618,Dravidian languages,"Fake news has become a major challenge for online platforms and society as a whole, with potentially harmful consequences for individuals and organizations. While there has been a lot of research on detecting fake news in high-resource languages, very little attention has been paid to low-resource languages. Due to a lack of corpora and annotated data, the classification of fake news in low-resource languages remains in its infancy. In this research, we present a novel transfer learning strategy for detecting fake news in Dravidian languages. We introduced a Dravidian_Fake a new dataset for fake news classification in Dravidian languages, and we created multilingual datasets by combining the English ISOT with the Dravidian_Fake datasets. We fine-tuned the mBERT and XLM-R pretrained transformer models with adaptive learning using English and Dravidian language fake news datasets. The classification model is evaluated using transfer learning, and the suggested model outperforms current approaches and provides a viable solution for sentence-level fake news classification in a resource-constrained environment. Experimental results on a Dravidian fake news dataset of low-resource languages demonstrate the efficacy of our approach in detecting fake news with an average accuracy of 93.31 percent in multilingual transfer learning.",,,,,
,,"Soni, Badal",,,,,,,,,Fake news,,,,,,
,,"Borgohain, Samir Kumar",,,,,,,,,Low-resource languages,,,,,,
,,,,,,,,,,,Transfer learning,,,,,,
,,,,,,,,,,,Transformers,,,,,,
JOUR,Depression and Suicide Risk Detection on Social Media using fastText Embedding and XGBoost Classifier,"Ghosal, Sayani",Procedia Computer Science,218,1631,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2023.01.141,https://www.sciencedirect.com/science/article/pii/S1877050923001412,Depression/suicide detection,"Nowadays, people with depression reveal their mental conditions on social webs for emotional relief. Detection of suicide risk and depression content can save society and the young generation. A person at risk needs immediate medical attention, so early detection of depressive content with NLP is an important research area. We propose a novel framework to differentiate between depression and suicidal risk content with the fastText embedding for contextual analysis, TF-IDF vector for the relevance of terms, and machine learning classifier XGBoost for accurate classification. This novel approach achieves 0.78 AUC and 0.71 weighted F1-score for this Reddit dataset and also increases accuracy, weighted F1- score against one baseline model. Our experiments and analysis exhibit strong performance against various embedding models and classifiers for this challenging problem.",,1639,International Conference on Machine Learning and Data Engineering,,
,,"Jain, Amita",,,,,,,,,Natural Language Processing,,,,,,
,,,,,,,,,,,Social Media Webs,,,,,,
,,,,,,,,,,,Machine Learning,,,,,,
,,,,,,,,,,,fastText,,,,,,
,,,,,,,,,,,Text Classification,,,,,,
JOUR,Traffic event detection as a slot filling problem,"Yang, Xiangyu",Engineering Applications of Artificial Intelligence,123,106202,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106202,https://www.sciencedirect.com/science/article/pii/S095219762300386X,Traffic event detection,"Social media platforms, such as Twitter, can be used to extract information related to traffic events. Previous works focused mainly on classifying tweets into predefined categories (i.e., traffic or non-traffic) without many details of traffic events. However, extracting traffic-related fine-grained information from tweets is essential to build an intelligent transportation system. In this work, we address for the first time the problem of detecting traffic events using Twitter as two subtasks: (i) identifying whether a tweet is traffic-related or not as a text classification subtask, and (ii) extracting more fine-grained information (i.e., “what”, “when”, “where”, and the “consequence” of the traffic event) as a slot filling subtask. We also publish two Dutch Traffic Twitter datasets from Belgium and the Brussels capital region. We propose using deep learning based methods that process the two subtasks separately or jointly. Experimental results indicate that the proposed architectures achieve high performance scores (i.e., more than 95% F1 score) on the constructed datasets for both subtasks, even in a transfer learning scenario. In addition, incorporating tweet-level information in each of the tokens comprising the tweet (for the BERT-based model) can lead to a performance improvement for the joint setting. Our datasets and code are available on GitHub.11https://github.com/Glovesme/TrafficEventDetectionBE.",,,,,
,,"Bekoulis, Giannis",,,,,,,,,Slot filling,,,,,,
,,"Deligiannis, Nikos",,,,,,,,,Text classification,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
JOUR,Voice artificial intelligence service failure and customer complaint behavior: The mediation effect of customer emotion,"Li, Bin",Electronic Commerce Research and Applications,59,101261,2023,2023/05/01/,1567-4223,https://doi.org/10.1016/j.elerap.2023.101261,https://www.sciencedirect.com/science/article/pii/S1567422323000261,Voice AI service failure,"According to the literature, customer service failure and negative customer reactions are closely associated. However, there has been little exploration of the relationship between service failure and negative customer reactions in the context of artificial intelligence (AI) services. Based on a conceptual model, this study explores the impact of voice AI service failure on customer complaint behavior. A proprietary dataset from a telecommunications firm was used to evaluate the utility of the conceptual model. Customer emotion was tested to mediate between voice AI service failure and customer complaint behavior. We found that the failure of voice AI services substantially influences customer complaint behavior. AI service failure increases the likelihood of complaints to a customer in the call center. Furthermore, customer emotion plays an essential mediating role. Our findings have crucial management implications for customer relationship management and recovery service.",,,,,
,,"Liu, Luning",,,,,,,,,Customer emotion,,,,,,
,,"Mao, Weicheng",,,,,,,,,Customer complaint behavior,,,,,,
,,"Qu, Yongcuomu",,,,,,,,,Mediation effect,,,,,,
,,"Chen, Yanhong",,,,,,,,,,,,,,,
JOUR,Testing SOAR tools in use,"Bridges, Robert A.",Computers & Security,129,103201,2023,2023/06/01/,0167-4048,https://doi.org/10.1016/j.cose.2023.103201,https://www.sciencedirect.com/science/article/pii/S0167404823001116,Security orchestration automation and response (SOAR),"Investigations within Security Operation Centers (SOCs) are tedious as they rely on manual efforts to query diverse data sources, overlay related logs, correlate the data into information, and then document results in a ticketing system. Security Orchestration, Automation, and Response (SOAR) tools are a relatively new technology that promise, with appropriate configuration, to collect, filter, and display needed diverse information; automate many of the common tasks that unnecessarily require SOC analysts’ time; facilitate SOC collaboration; and, in doing so, improve both efficiency and consistency of SOCs. There has been no prior research to test SOAR tools in practice; hence, understanding and evaluation of their effect is nascent and needed. In this paper, we design and administer the first hands-on user study of SOAR tools, involving 24 participants and six commercial SOAR tools. Our contributions include the experimental design, itemizing six defining characteristics of SOAR tools, and a methodology for testing them. We describe configuration of a cyber range test environment, including network, user, and threat emulation; a full SOC tool suite; and creation of artifacts allowing multiple representative investigation scenarios to permit testing. We present the first research results on SOAR tools. Concisely, our findings are that: per-SOC SOAR configuration is extremely important; SOAR tools increase efficiency and reduce context switching, although with potentially decreased ticketing accuracy/completeness; user preference is slightly negatively correlated with their performance with the tool; internet dependence varies widely among SOAR tools; and balance of automation with assisting decision making is preferred by senior participants. We deliver a public user- and tool-anonymized and -obfuscated version of the data.",,,,,
,,"Rice, Ashley E.",,,,,,,,,Test and evaluation,,,,,,
,,"Oesch, Sean",,,,,,,,,User study,,,,,,
,,"Nichols, Jeffrey． A.",,,,,,,,,Security operation center (SOC),,,,,,
,,"Watson, Cory",,,,,,,,,Cybersecurity technology,,,,,,
,,"Spakes, Kevin",,,,,,,,,,,,,,,
,,"Norem, Savannah",,,,,,,,,,,,,,,
,,"Huettel, Mike",,,,,,,,,,,,,,,
,,"Jewell, Brian",,,,,,,,,,,,,,,
,,"Weber, Brian",,,,,,,,,,,,,,,
,,"Gannon, Connor",,,,,,,,,,,,,,,
,,"Bizovi, Olivia",,,,,,,,,,,,,,,
,,"Hollifield, Samuel C.",,,,,,,,,,,,,,,
,,"Erwin, Samantha",,,,,,,,,,,,,,,
JOUR,Transferable adversarial distribution learning: Query-efficient adversarial attack against large language models,"Dong, Huoyuan",Computers & Security,135,103482,2023,2023/12/01/,0167-4048,https://doi.org/10.1016/j.cose.2023.103482,https://www.sciencedirect.com/science/article/pii/S0167404823003929,Adversarial attack,"It is a challenging task to fool a text classifier based on deep neural networks under the black-box setting where the target model can only be queried. Among the existing black-box attacks, decision-based methods have a large query cost due to exponential perturbation space and greedy search strategy. Transfer-based methods, on the other hand, tend to overfit the surrogate model and thus fail when applied to unknown target models. In this paper, we propose a straightforward yet highly effective adversarial attack framework for black-box transformer-based models, thereby exposing vulnerabilities within large language models. Specifically, we leverage a fine-tuned large language model as a white-box surrogate model and optimize a distribution of adversarial text. This distribution is parameterized by a continuous-valued matrix based upon the surrogate model. To avoid overfitting of the distribution and improve its adversarial transferability, we incorporate an additional causal language model into our framework as a constraint model. Based on this constraint model, we add language model perplexity and semantic consistency as regularization terms during the distribution training process. To further reduce the number of queries to the target model, i.e., improve the threat level of examples drawn from our distribution, we employ a geometric loss strategy to ensure that the distribution training process learns the optimal perturbation. Extensive experimental studies have been carried out on benchmark datasets and the results demonstrate significant improvement on the performance and query efficiency under black-box setting in comparison with well-established approaches. Our approach achieves an 80.98% reduction in BERT model accuracy while consuming only 21.86% of the query times required by prior attacks.",,,,,
,,"Dong, Jialiang",,,,,,,,,Natural language processing,,,,,,
,,"Wan, Shaohua",,,,,,,,,Black-box attack,,,,,,
,,"Yuan, Shuai",,,,,,,,,Adversarial distribution,,,,,,
,,"Guan, Zhitao",,,,,,,,,Gradient-based optimization,,,,,,
JOUR,Investigating Hostile Post Detection in Hindi,"Bhatnagar, Varad",Neurocomputing,474,60,2022,2022/02/14/,0925-2312,https://doi.org/10.1016/j.neucom.2021.11.096,https://www.sciencedirect.com/science/article/pii/S0925231221018099,Hostile Post,"Hostile content on Social Media platforms is becoming a problem for governments and organizations. There is a need for AI based intervention which can filter hostile content at scale. The challenge lies in ambiguity of language, absence of training data and local context. In this paper, we investigate Hostile Post Detection for the Hindi Language, which is the topmost language in the Indian Subcontinent in terms of speaker population and third in the world. We extend our prior work in this area along the dimensions of (i) Representations (ii) Data and (iii) Architecture, exploring approaches like Transformers and Multi Task Learning among others, along the way. In this highly experimental study, comparisons are drawn, trends are discovered and insights are presented. We manage to improve on the baseline by 16.5% and 29.77% on the two evaluation metrics viz. Coarse Grained F1 Score and Fine Grained F1 Score. We are also able to beat our prior work results by 0.93% and 9.18% on these two evaluation metrics respectively. Experiments performed by us number 60 which is larger than the number reported in any other work for Hostility Detection in Hindi, to the best of our knowledge.",,81,,,
,,"Kumar, Prince",,,,,,,,,Label Powerset,,,,,,
,,"Bhattacharyya, Pushpak",,,,,,,,,Binary Relevance,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,MTDNN,,,,,,
JOUR,"Human-centered neural reasoning for subjective content processing: Hate speech, emotions, and humor","Kazienko, Przemysław",Information Fusion,94,43,2023,2023/06/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.01.010,https://www.sciencedirect.com/science/article/pii/S1566253523000167,Content perception,"Some tasks in content processing, e.g., natural language processing (NLP), like hate or offensive speech and emotional or funny text detection, are subjective by nature. Each human may perceive some content individually. The existing reasoning methods commonly rely on agreed output values, the same for all recipients. We propose fundamentally different — personalized solutions applicable to any subjective NLP task. Our five new deep learning models take into account not only the textual content but also the opinions and beliefs of a given person. They differ in their approaches to learning Human Bias (HuBi) and fusion with content (text) representation. The experiments were carried out on 14 tasks related to offensive, emotional, and humorous texts. Our personalized HuBi methods radically outperformed the generalized ones for all NLP problems. Personalization also has a greater impact on reasoning quality than commonly explored pre-trained and fine-tuned language models. We discovered a high correlation between human bias calculated using our dedicated formula and that learned by the model. Multi-task solutions achieved better outcomes than single-task architectures. Human and word embeddings also provided additional insights.",,65,,,
,,"Bielaniewicz, Julita",,,,,,,,,NLP,,,,,,
,,"Gruza, Marcin",,,,,,,,,Subjective NLP tasks,,,,,,
,,"Kanclerz, Kamil",,,,,,,,,Personalized NLP,,,,,,
,,"Karanowski, Konrad",,,,,,,,,Offensive content,,,,,,
,,"Miłkowski, Piotr",,,,,,,,,Hate speech,,,,,,
,,"Kocoń, Jan",,,,,,,,,Emotion recognition,,,,,,
,,,,,,,,,,,Humor detection,,,,,,
,,,,,,,,,,,Learning human representations,,,,,,
,,,,,,,,,,,Human bias,,,,,,
,,,,,,,,,,,Text classification,,,,,,
,,,,,,,,,,,Information fusion,,,,,,
JOUR,Better to hear all parties: Understanding the impact of homophily in online financial discussion,"Shi, Yong",Electronic Commerce Research and Applications,54,101159,2022,2022/07/01/,1567-4223,https://doi.org/10.1016/j.elerap.2022.101159,https://www.sciencedirect.com/science/article/pii/S1567422322000436,Homophily,"As social media platforms have become popular venues for investors to share and exchange investment opinions to inform trading decisions, offline social phenomena such as homophily—defined as people’s inclination to seek interactions and associations with similar others—have also surfaced in online interactions. Yet relatively little is known about the economic consequences associated with investor homophily behavior in online social media. This study investigates how homophily in online financial discussion affects financial outcomes, particularly, investors’ judgments about assets’ future prospects and investment decisions, and overall market price efficiency. We theorize that, by reducing the diversity of information that investors access, homophily may lead to undesirable consequences for individual investors as well as the market as a whole. Analyzing approximately 33.5 million online postings by nearly 2.5 million investors from one of the largest social media platforms for investors in China, we find that, on average, homophily (1) reduces the accuracy of investor opinions about stocks’ future prospects, particularly, future stock returns, and (2) is associated with weaker immediate price reaction to earnings news and stronger post-earnings announcement drifts. Taken together, these findings reveal a dark side of homophily in online financial discussion, suggesting that falling prey to homophily could potentially lead to inferior investment decisions and welfare concerns, and aggregately, impair price efficiency and impede the efficient functioning of the capital market.",,,,,
,,"An, Yuan",,,,,,,,,Social media,,,,,,
,,"Zhu, Xiumei",,,,,,,,,Financial market,,,,,,
,,"Jiang, Fuwei",,,,,,,,,Investment decision,,,,,,
,,,,,,,,,,,Textual analysis,,,,,,
JOUR,Bipol: A novel multi-axes bias evaluation metric with explainability for NLP,"Alkhaled, Lama",Natural Language Processing Journal,4,100030,2023,2023/09/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100030,https://www.sciencedirect.com/science/article/pii/S2949719123000274,Bipol,"We introduce bipol, a new metric with explainability, for estimating social bias in text data. Harmful bias is prevalent in many online sources of data that are used for training machine learning (ML) models. In a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (TF). After creating new models to classify bias using SotA architectures, we evaluate two popular NLP datasets (COPA and SQuADv2) and the WinoBias dataset. As additional contribution, we created a large English dataset (with almost 2 million labeled samples) for training models in bias classification and make it publicly available. We also make public our codes.",,,,,
,,"Adewumi, Tosin",,,,,,,,,MAB dataset,,,,,,
,,"Sabry, Sana Sabah",,,,,,,,,NLP,,,,,,
,,,,,,,,,,,Bias,,,,,,
JOUR,TextGuise: Adaptive adversarial example attacks on text classification model,"Chang, Guoqin",Neurocomputing,529,190,2023,2023/04/07/,0925-2312,https://doi.org/10.1016/j.neucom.2023.01.071,https://www.sciencedirect.com/science/article/pii/S0925231223001042,Adversarial examples,"Adversarial examples greatly compromise the security of deep learning models. The key to improving the robustness of a natural language processing (NLP) model is to study attacks and defenses involving adversarial text. However, the current adversarial attack methods still face problems, such as the low success rates of attacks on some datasets, and the existing defense methods can already successfully defend against some attack methods. As a result, such attacks are unable to dig deeper into the flaws of NLP models to inform further defense improvements. Hence, it is necessary to design an adversarial attack method with a wider attack range and stronger performance. Aiming at the advantages and disadvantages of existing methods, this paper proposes a new adaptive black-box text adversarial example generation scheme, TextGuise. First, we design a keyword selection method in which word scores are calculated by combining context semantics to select the appropriate keywords to modify. Second, to maintain semantics, new keyword substitution rules are designed in combination with the characteristics of text and popular text expressions. Finally, the best modification strategy is adaptively selected through a querying model to reduce the magnitudes of disturbances. TextGuise can automatically select replacement keywords and replacement strategies that efficiently generate adversarial examples with good readability for various text classification tasks. Attack experiments conducted with TextGuise on 5 datasets yield high attack success rates that can surpass 80% when the perturbation ratio does not exceed 0.2. In addition, we present and discuss experiments focusing on defense, text similarity, query times, time consumption, etc., to test the attack performance of TextGuise. The results show that our attack method can achieve a good balance among various metrics.",,203,,,
,,"Gao, Haichang",,,,,,,,,Adaptive,,,,,,
,,"Yao, Zhou",,,,,,,,,Text classification,,,,,,
,,"Xiong, Haoquan",,,,,,,,,Keyword substitution,,,,,,
JOUR,Semantic matching in machine reading comprehension: An empirical study,"Liu, Qian",Information Processing & Management,60,103145,2023,2023/03/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103145,https://www.sciencedirect.com/science/article/pii/S0306457322002461,Natural language processing,"Machine reading comprehension (MRC) is a challenging task in the field of artificial intelligence. Most existing MRC works contain a semantic matching module, either explicitly or intrinsically, to determine whether a piece of context answers a question. However, there is scant work which systematically evaluates different paradigms using semantic matching in MRC. In this paper, we conduct a systematic empirical study on semantic matching. We formulate a two-stage framework which consists of a semantic matching model and a reading model, based on pre-trained language models. We compare and analyze the effectiveness and efficiency of using semantic matching modules with different setups on four types of MRC datasets. We verify that using semantic matching before a reading model improves both the effectiveness and efficiency of MRC. Compared with answering questions by extracting information from concise context, we observe that semantic matching yields more improvements for answering questions with noisy and adversarial context. Matching coarse-grained context to questions, e.g., paragraphs, is more effective than matching fine-grained context, e.g., sentences and spans. We also find that semantic matching is helpful for answering who/where/when/what/how/which questions, whereas it decreases the MRC performance on why questions. This may imply that semantic matching helps to answer a question whose necessary information can be retrieved from a single sentence. The above observations demonstrate the advantages and disadvantages of using semantic matching in different scenarios.",2,,,,
,,"Mao, Rui",,,,,,,,,Machine reading comprehension,,,,,,
,,"Geng, Xiubo",,,,,,,,,Question answering,,,,,,
,,"Cambria, Erik",,,,,,,,,Semantic matching,,,,,,
JOUR,An emotion and cognitive based analysis of mental health disorders from social media data,"Uban, Ana-Sabina",Future Generation Computer Systems,124,480,2021,2021/11/01/,0167-739X,https://doi.org/10.1016/j.future.2021.05.032,https://www.sciencedirect.com/science/article/pii/S0167739X21001825,Mental health disorders,"Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.",,494,,,
,,"Chulvi, Berta",,,,,,,,,Early risk prediction,,,,,,
,,"Rosso, Paolo",,,,,,,,,Emotions,,,,,,
,,,,,,,,,,,Cognitive styles,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Social media,,,,,,
JOUR,A survey on text generation using generative adversarial networks,"de Rosa, Gustavo H.",Pattern Recognition,119,108098,2021,2021/11/01/,0031-3203,https://doi.org/10.1016/j.patcog.2021.108098,https://www.sciencedirect.com/science/article/pii/S0031320321002855,Text generation,"This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks. The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called “natural” language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning, and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.",,,,,
,,"Papa, João P.",,,,,,,,,Generative adversarial Networks,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Language modeling,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
JOUR,VictimFinder: Harvesting rescue requests in disaster response from social media with BERT,"Zhou, Bing","Computers, Environment and Urban Systems",95,101824,2022,2022/07/01/,0198-9715,https://doi.org/10.1016/j.compenvurbsys.2022.101824,https://www.sciencedirect.com/science/article/pii/S0198971522000680,Social media,"Social media platforms are playing increasingly critical roles in disaster response and rescue operations. During emergencies, users can post rescue requests along with their addresses on social media, while volunteers can search for those messages and send help. However, efficiently leveraging social media in rescue operations remains challenging because of the lack of tools to identify rescue request messages on social media automatically and rapidly. Analyzing social media data, such as Twitter data, relies heavily on Natural Language Processing (NLP) algorithms to extract information from texts. The introduction of bidirectional transformers models, such as the Bidirectional Encoder Representations from Transformers (BERT) model, has significantly outperformed previous NLP models in numerous text analysis tasks, providing new opportunities to precisely understand and classify social media data for diverse applications. This study developed and compared ten VictimFinder models for identifying rescue request tweets, three based on milestone NLP algorithms and seven BERT-based. A total of 3191 manually labeled disaster-related tweets posted during 2017 Hurricane Harvey were used as the training and testing datasets. We evaluated the performance of each model by classification accuracy, computation cost, and model stability. Experiment results show that all BERT-based models have significantly increased the accuracy of categorizing rescue-related tweets. The best model for identifying rescue request tweets is a customized BERT-based model with a Convolutional Neural Network (CNN) classifier. Its F1-score is 0.919, which outperforms the baseline model by 10.6%. The developed models can promote social media use for rescue operations in future disaster events.",,,,,
,,"Zou, Lei",,,,,,,,,Natural language processing,,,,,,
,,"Mostafavi, Ali",,,,,,,,,BERT,,,,,,
,,"Lin, Binbin",,,,,,,,,Disaster response,,,,,,
,,"Yang, Mingzheng",,,,,,,,,Emergency rescue,,,,,,
,,"Gharaibeh, Nasir",,,,,,,,,,,,,,,
,,"Cai, Heng",,,,,,,,,,,,,,,
,,"Abedin, Joynal",,,,,,,,,,,,,,,
,,"Mandal, Debayan",,,,,,,,,,,,,,,
JOUR,Refining fine-tuned transformers with hand-crafted features for gender screening on question-answering communities,"Figueroa, Alejandro",Information Fusion,92,256,2023,2023/04/01/,1566-2535,https://doi.org/10.1016/j.inffus.2022.12.003,https://www.sciencedirect.com/science/article/pii/S1566253522002512,Gender recognition,"Machine learning and demographic analysis are a cornerstone for making community Question Answering (cQA) platforms more egalitarian and vibrant, safer as well. For instance, the two cooperate on successfully detecting suspicious/malicious activity and on stirring up the interest of community fellows to learn by exploring new topics. In this sense, both research fields play a vital role in reducing gender disparity across categories, when promoting unresolved questions to potential answerers. Current state-of-the-art artificial intelligence architectures, such as pre-trained transformers, train complex goals and million of parameters as a means of inferring and encoding knowledge from massive corpora. Fine-tuning is the process that allows later to transfer this encrypted information to a downstream task (e.g., gender classification). Needless to say, these pre-trained encoders also suffer from multiple disadvantages. To give an example, they are sensitive to irrelevant and misleading words, bringing about overfitting, usually on small datasets. This work offers a fresh look at this kind of technique by introducing PTM-SFFS, a novel approach that effectively pairs frontier transformers with linguistic properties via the use of traditional classifiers. Based on a feature wrapper (SFFS), PTM-SFFS refines the scores produced by a fine-tuned model via seeking for an array of mostly linguistic features to build a conventional statistical classifier (e.g., Bayes and MaxEnt). And as a result, this new discriminant function enhances the overall prediction rate by optimizing the synergy between both sorts of strategies. When applied to automatic gender recognition on cQA sites, PTM-SFFS increased the accuracy of seven fine-tuned state-of-the-art encoders up to 10% (XLNet). Thanks to its interpretability, we discover that it capitalizes on dependency parsing and metadata for improving the transference of lexicalized information to the target domain.",,267,,,
,,,,,,,,,,,User analysis,,,,,,
,,,,,,,,,,,Community question answering,,,,,,
,,,,,,,,,,,Pre-trained models,,,,,,
,,,,,,,,,,,Statistical classifiers,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
JOUR,A hybrid lexicon-based and neural approach for explainable polarity detection,"Polignano, Marco",Information Processing & Management,59,103058,2022,2022/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103058,https://www.sciencedirect.com/science/article/pii/S0306457322001613,Sentiment analysis,"In this work, we propose BERT-WMAL, a hybrid model that brings together information coming from data through the recent transformer deep learning model and those obtained from a polarized lexicon. The result is a model for sentence polarity that manages to have performances comparable with those at the state-of-the-art, but with the advantage of being able to provide the end-user with an explanation regarding the most important terms involved with the provided prediction. The model has been evaluated on three polarity detection Italian dataset, i.e., SENTIPOLC, AGRITREND and ABSITA. While the first contains 7,410 tweets released for training and 2,000 for testing, the second and the third respectively include 1,000 tweets without splitting , and 2,365 reviews for training, 1,171 for testing. The use of lexicon-based information proves to be effective in terms of the F1 measure since it shows an improvement of F1 score on all the observed dataset: from 0.664 to 0.669 (i.e, 0.772%) on AGRITREND, from 0.728 to 0.734 (i.e., 0.854%) on SENTIPOLC and from 0.904 to 0.921 (i.e, 1.873%) on ABSITA. The usefulness of this model not only depends on its effectiveness in terms of the F1 measure, but also on its ability to generate predictions that are more explainable and especially convincing for the end-users. We evaluated this aspect through a user study involving four native Italian speakers, each evaluating 64 sentences with associated explanations. The results demonstrate the validity of this approach based on a combination of weights of attention extracted from the deep learning model and the linguistic knowledge stored in the WMAL lexicon. These considerations allow us to regard the approach provided in this paper as a promising starting point for further works in this research area.",5,,,,
,,"Basile, Valerio",,,,,,,,,Polarity detection,,,,,,
,,"Basile, Pierpaolo",,,,,,,,,Lexicon,,,,,,
,,"Gabrieli, Giuliano",,,,,,,,,WMAL,,,,,,
,,"Vassallo, Marco",,,,,,,,,BERT,,,,,,
,,"Bosco, Cristina",,,,,,,,,Explanation,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
JOUR,Attention uncovers task-relevant semantics in emotional narrative understanding,"Nguyen, Thanh-Son",Knowledge-Based Systems,226,107162,2021,2021/08/17/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107162,https://www.sciencedirect.com/science/article/pii/S0950705121004251,Explainable AI,"Attention mechanisms in deep neural network models have helped them to achieve exceptional performance at complex natural language processing tasks. Previous attempts to investigate what these models have been “paying attention to” suggest that these attention representations capture syntactic information, but there is less evidence for semantics. In this paper, we investigate the capability of an attention mechanism to “attend to” semantically meaningful words. Using a dataset of naturalistic emotional narratives, we first build a Window-Based Attention (WBA) consisting of a hierarchical, two-level long short-term memory (LSTM) with softmax attention. Our model outperforms state-of-the-art models at predicting emotional valence, and even surpassing average human performance. Next, we show in detailed analyses, including word deletion experiments and visualizations, that words that receive higher attention weights in our model also tend to have greater emotional semantic meaning. Experimental results using six different pre-trained word embeddings suggest that deep neural network models which achieve human-level performance may learn to place greater attention weights on words that humans find semantically meaningful to the task at hand.",,,,,
,,"Wu, Zhengxuan",,,,,,,,,Emotion understanding,,,,,,
,,"Ong, Desmond C.",,,,,,,,,Neural network attention,,,,,,
JOUR,A BERT-based deontic logic learner,"Sun, Jingyun",Information Processing & Management,60,103374,2023,2023/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103374,https://www.sciencedirect.com/science/article/pii/S0306457323001115,Legal AI,"In recent years, large-scale Pre-trained Language Models (PLMs) like BERT have achieved state-of-the-art results on many NLP tasks. We explore whether BERT understands deontic logic which is important for the fields of legal AI and digital government. We measure BERT's understanding of deontic logic through the Deontic Modality Classification (DMC) task. Experiments show that without fine-tuning or fine-tuning with only a small amount of data, BERT cannot achieve good performance on the DMC task. Therefore, we propose a new method for BERT fine-tuning and prediction, called DeonticBERT. The method incorporates heuristic knowledge from deontic logic theory as an inductive bias into BERT through a template function and a mapping between category labels and predicted words, to steer BERT understand the DMC task. This can also stimulate BERT to recall the deontic logic knowledge learned in pre-training. We use an English dataset widely used as well as a Chinese dataset we constructed to conduct experiments. Experimental results show that on the DMC task, DeonticBERT can achieve 66.9% and 91% accuracy under zero-shot and few-shot conditions, respectively, far exceeding other baselines. This demonstrates that DeonticBERT does enable BERT to understand deontic logic and can handle related tasks without using much fine-tuning data. Our research helps facilitate applying large-scale PLMs like BERT into legal AI and digital government.",4,,,,
,,"Huang, Shaobin",,,,,,,,,Digital government,,,,,,
,,"Wei, Chi",,,,,,,,,"Normative text, Deontic logic",,,,,,
,,,,,,,,,,,Pre-trained language model,,,,,,
JOUR,Efficient parameter tuning of neural foundation models for drug perspective prediction from unstructured socio-medical data,"Unnikrishnan, Reshma",Engineering Applications of Artificial Intelligence,123,106214,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106214,https://www.sciencedirect.com/science/article/pii/S0952197623003986,Social media analysis,"The phenomenal popularity of social media platforms over the past decade has accelerated the development of intelligent applications that leverage social media data for informed decision-making in diverse domains like finance, education, public policy and healthcare management practices. While understanding the colloquial language of users on social media remains a challenging problem, access to users’ medical perspectives that conversationally divulge healthcare-related experiences and insights can help reshape healthcare ecosystems like chronic disease management, pandemics, public health, pharmacovigilance and more. Most existing models are constrained to a particular dataset while neglecting model adaptability across data sources and domains. Model generalization across variable data sizes also has received very little research attention. Conventional foundation models can be fine-tuned by adding additional model heads or by appending contributing network layers, however, there has been very little focus on effective parameter calibration for adapting neural foundation models to a specific task. In this study, an Adaptive Learning mechanism for Socio-Medical data (AL4SM) built on generic foundation neural models with efficient parameter learning is proposed, to categorize users’ perspectives on prescription drug-related experiences and adapt to diverse socio-medical data sources of variable sizes. AL4SM aims to lighten the over-parameterized mechanisms adopted by existing foundational techniques by efficiently learning latent medical information based on optimized parameter calibration and weight reinitialization techniques. Comprehensive cross-domain and cross-data analyses are undertaken to explore specific user perspectives related to prescription effectiveness and side effects. Validation experiments conducted on standard datasets obtained from Drugs.com and Druglib.com revealed that the proposed AL4SM outperformed state-of-the-art models, achieving an improvement of 6.06% in accuracy and 7.62% in F1-score for 3-class and 2% in F1-score for 10-class drug perspective categorization. The cross-data experiments further emphasized the superiority of the proposed model, with improved accuracy of 17% on Drugs.com and 9% on Druglib.com datasets, respectively.",,,,,
,,"S., Sowmya Kamath",,,,,,,,,Neural language modelling,,,,,,
,,"V.S., Ananthanarayana",,,,,,,,,Efficient parameter learning,,,,,,
,,,,,,,,,,,Data-centric approach,,,,,,
,,,,,,,,,,,Perspective prediction,,,,,,
JOUR,Twitter conversations predict the daily confirmed COVID-19 cases,"Lamsal, Rabindra",Applied Soft Computing,129,109603,2022,2022/11/01/,1568-4946,https://doi.org/10.1016/j.asoc.2022.109603,https://www.sciencedirect.com/science/article/pii/S1568494622006524,Pandemic forecast,"As of writing this paper, COVID-19 (Coronavirus disease 2019) has spread to more than 220 countries and territories. Following the outbreak, the pandemic’s seriousness has made people more active on social media, especially on the microblogging platforms such as Twitter and Weibo. The pandemic-specific discourse has remained on-trend on these platforms for months now. Previous studies have confirmed the contributions of such socially generated conversations towards situational awareness of crisis events. The early forecasts of cases are essential to authorities to estimate the requirements of resources needed to cope with the outgrowths of the virus. Therefore, this study attempts to incorporate the public discourse in the design of forecasting models particularly targeted for the steep-hill region of an ongoing wave. We propose a sentiment-involved topic-based latent variables search methodology for designing forecasting models from publicly available Twitter conversations. As a use case, we implement the proposed methodology on Australian COVID-19 daily cases and Twitter conversations generated within the country. Experimental results: (i) show the presence of latent social media variables that Granger-cause the daily COVID-19 confirmed cases, and (ii) confirm that those variables offer additional prediction capability to forecasting models. Further, the results show that the inclusion of social media variables introduces 48.83%–51.38% improvements on RMSE over the baseline models. We also release the large-scale COVID-19 specific geotagged global tweets dataset, MegaGeoCOV, to the public anticipating that the geotagged data of this scale would aid in understanding the conversational dynamics of the pandemic through other spatial and temporal contexts.",,,,,
,,"Harwood, Aaron",,,,,,,,,Time series analysis,,,,,,
,,"Read, Maria Rodriguez",,,,,,,,,Social media analytics,,,,,,
,,,,,,,,,,,Twitter analytics,,,,,,
,,,,,,,,,,,Granger causality,,,,,,
,,,,,,,,,,,ARIMAX models,,,,,,
,,,,,,,,,,,VAR models,,,,,,
JOUR,Constructing and analyzing domain-specific language model for financial text mining,"Suzuki, Masahiro",Information Processing & Management,60,103194,2023,2023/03/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103194,https://www.sciencedirect.com/science/article/pii/S0306457322002953,Language models,"The application of natural language processing (NLP) to financial fields is advancing with an increase in the number of available financial documents. Transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT) have been successful in NLP in recent years. These cutting-edge models have been adapted to the financial domain by applying financial corpora to existing pre-trained models and by pre-training with the financial corpora from scratch. In Japanese, by contrast, financial terminology cannot be applied from a general vocabulary without further processing. In this study, we construct language models suitable for the financial domain. Furthermore, we compare methods for adapting language models to the financial domain, such as pre-training methods and vocabulary adaptation. We confirm that the adaptation of a pre-training corpus and tokenizer vocabulary based on a corpus of financial text is effective in several downstream financial tasks. No significant difference is observed between pre-training with the financial corpus and continuous pre-training from the general language model with the financial corpus. We have released our source code and pre-trained models.",2,,,,
,,"Sakaji, Hiroki",,,,,,,,,Domain-specific pre-training,,,,,,
,,"Hirano, Masanori",,,,,,,,,Financial market,,,,,,
,,"Izumi, Kiyoshi",,,,,,,,,Natural language processing,,,,,,
JOUR,Multi-label legal document classification: A deep learning-based approach with label-attention and domain-specific pre-training,"Song, Dezhao",Information Systems,106,101718,2022,2022/05/01/,0306-4379,https://doi.org/10.1016/j.is.2021.101718,https://www.sciencedirect.com/science/article/pii/S0306437921000016,Multi-label classification,"Multi-label document classification has a broad range of applicability to various practical problems, such as news article topic tagging, sentiment analysis, medical code classification, etc. A variety of approaches (e.g., tree-based methods, neural networks and deep learning systems that are specifically based on pre-trained language models) have been developed for multi-label document classification problems and have achieved satisfying performance on different datasets. In the legal domain, however, one is often faced with several key challenges when working with multi-label classification tasks. One critical challenge is the lack of high-quality human labeled datasets, which prevents researchers and practitioners from achieving decent performance on respective tasks. Also, existing methods on multi-label classification typically focus on the majority classes, which results in an unsatisfying performance for other important classes that do not have sufficient training samples. In order to tackle the above challenges, in this paper, we first present POSTURE50K, a novel legal extreme multi-label classification dataset, which we will release to the research community. The dataset contains 50,000 legal opinions and their manually labeled legal procedural postures. Labels in this dataset follow a Zipfian distribution, leaving many of the classes with only a few samples. Furthermore, we propose a deep learning architecture that adopts domain-specific pre-training and a label-attention mechanism for multi-label document classification. We evaluate our proposed architecture on POSTURE50K and another legal multi-label dataset EUROLEX57K, and show that our approach achieves better performances than two baseline systems and another four recent state-of-the-art methods on both datasets.",,,,,
,,"Vold, Andrew",,,,,,,,,Procedural posture,,,,,,
,,"Madan, Kanika",,,,,,,,,Label-attended document classification,,,,,,
,,"Schilder, Frank",,,,,,,,,Domain-specific pre-training,,,,,,
JOUR,Sentimental Contrastive Learning for event representation,"Zhou, Yan",Natural Language Processing Journal,4,100031,2023,2023/09/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100031,https://www.sciencedirect.com/science/article/pii/S2949719123000286,Event representation learning,"Event representation learning is crucial for numerous event-driven tasks, as the quality of event representations greatly influences the performance of these tasks. However, many existing event representation methods exhibit a heavy reliance on semantic features, often neglecting the wealth of information available in other dimensions of events. Consequently, these methods struggle to capture subtle distinctions between events. Incorporating sentimental information can be particularly useful when modeling event data, as leveraging such information can yield superior event representations. To effectively integrate sentimental information, we propose a novel event representation learning framework, namely Sentimental Contrastive Learning (SCL). Specifically, we firstly utilize BERT as the backbone network for pre-training and obtain the initial event representations. Subsequently, we employ instance-level and cluster-level contrastive learning to fine-tune the original event representations. We introduce two distinct contrastive losses respectively for instance-level and cluster-level contrastive learning, each aiming to incorporate sentimental information from different perspectives. To evaluate the effectiveness of our proposed model, we select the event similarity evaluation task and conduct experiments on three representative datasets. Extensive experimental results demonstrate obvious performance improvement achieved by our approach over many other models.",,,,,
,,"Li, Xiaodong",,,,,,,,,Contrastive learning,,,,,,
,,,,,,,,,,,Sentiment analysis,,,,,,
,,,,,,,,,,,Pre-trained model,,,,,,
JOUR,KnowleNet: Knowledge fusion network for multimodal sarcasm detection,"Yue, Tan",Information Fusion,100,101921,2023,2023/12/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.101921,https://www.sciencedirect.com/science/article/pii/S1566253523002373,Sarcasm detection,"Sarcasm is a form of communication often used to express contempt or ridicule, where the speaker conveys a message opposite to their true meaning, typically intending to mock or belittle a specific target. Sarcasm detection has gained great attention in the field of natural language processing due to the fact that sarcasm is widespread on social media and difficult to detect for machines. While early efforts in sarcasm detection solely relied on textual data, the abundance of multimodal data on social media is also non-negligible. Recent research has focused on multimodal sarcasm detection, where attention mechanisms and graph neural networks were commonly used to identify relevant information in both image and text data. However, these methods may overlook the importance of prior knowledge and cross-modal semantic contrast, which are crucial factors for human sarcasm detection. In this paper, we propose a novel model named KnowleNet that leverages the ConceptNet knowledge base to incorporate prior knowledge and determine image–text relatedness through sample-level and word-level cross-modal semantic similarity detection. Contrastive learning is also introduced to improve the spatial distribution of sarcastic (positive) and non-sarcastic (negative) samples. The proposed model achieves state-of-the-art performance on publicly available benchmark datasets.",,,,,
,,"Mao, Rui",,,,,,,,,Multimodal learning,,,,,,
,,"Wang, Heng",,,,,,,,,Information fusion,,,,,,
,,"Hu, Zonghai",,,,,,,,,,,,,,,
,,"Cambria, Erik",,,,,,,,,,,,,,,
JOUR,Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech,"Ilias, Loukas",Knowledge-Based Systems,277,110834,2023,2023/10/09/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110834,https://www.sciencedirect.com/science/article/pii/S0950705123005841,Dementia,"Alzheimer’s disease (AD) constitutes a complex neurocognitive disease and is the main cause of dementia. Although many studies have been proposed targeting at diagnosing dementia through spontaneous speech, there are still limitations. Existing state-of-the-art approaches, which propose multimodal methods, train separately language and acoustic models, employ majority-vote approaches, and concatenate the representations of the different modalities either at the input level, i.e., early fusion, or during training. Also, some of them employ self-attention layers, which calculate the dependencies between representations without considering the contextual information. In addition, no prior work has taken into consideration the model calibration. To address these limitations, we propose some new methods for detecting AD patients, which capture the intra- and cross-modal interactions. First, we convert the audio files into log-Mel spectrograms, their delta, and delta-delta and create in this way an image per audio file consisting of three channels. Next, we pass each transcript and image through BERT and DeiT models respectively. After that, context-based self-attention layers, self-attention layers with a gate model, and optimal transport domain adaptation methods are employed for capturing the intra- and inter-modal interactions. Finally, we exploit two methods for fusing the self and cross-attention features. For taking into account the model calibration, we apply label smoothing. We use both performance and calibration metrics. Experiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the efficacy of our introduced approaches over existing research initiatives with our best performing model reaching Accuracy and F1-score up to 91.25% and 91.06% respectively.",,,,,
,,"Askounis, Dimitris",,,,,,,,,Log-Mel spectrogram,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,DeiT,,,,,,
,,,,,,,,,,,Optimal transport,,,,,,
,,,,,,,,,,,Context-based self-attention,,,,,,
,,,,,,,,,,,Model calibration,,,,,,
,,,,,,,,,,,Label smoothing,,,,,,
JOUR,Generating news image captions with semantic discourse extraction and contrastive style-coherent learning,"Zhang, Zhengkun",Computers and Electrical Engineering,104,108429,2022,2022/12/01/,0045-7906,https://doi.org/10.1016/j.compeleceng.2022.108429,https://www.sciencedirect.com/science/article/pii/S0045790622006462,News image captioning,"News image captioning is a popular and advanced task requiring the generation of image captions that reflect the relevant information contained in the news articles. The existing approaches usually align objects in images with corresponding news content and focus on entity-based approaches. However, their limitations are threefold: (a) semantic discourses of news contexts are largely unexplored, (b) the caption style has not yet been used, and (c) key entities are not ensured to be generated during caption generation. In this paper, we propose a novel framework for generating news image captions which are semantically informative, well-styled and entity-aware controllable. Specifically, the semantics of news articles are preserved by separating them into elementary discourse units (EDUs), which are fine-grained basic semantic units. Moreover, we introduce a Contrastive Style Reward to ensure that the style of captions and corresponding news content are coherent. Furthermore, we introduce a controllable mechanism to ensure that key entities are generated during the generation process. Extensive experiments on two large-scale news datasets demonstrate the effectiveness of the proposed framework.",,,,,
,,"Zhang, Huibin",,,,,,,,,Element discourse unit,,,,,,
,,"Wang, Jun",,,,,,,,,Text style,,,,,,
,,"Sun, Zhe",,,,,,,,,Entity-aware controllable generation,,,,,,
,,"Yang, Zhenglu",,,,,,,,,,,,,,,
JOUR,Persuasive dialogue understanding: The baselines and negative results,"Chen, Hui",Neurocomputing,431,47,2021,2021/03/28/,0925-2312,https://doi.org/10.1016/j.neucom.2020.11.040,https://www.sciencedirect.com/science/article/pii/S0925231220318336,Persuasive dialogue systems,"Persuasion aims at forming one’s opinion and action via a series of persuasive messages containing persuader’s strategies. Due to its potential application in persuasive dialogue systems, the task of persuasive strategy recognition has gained much attention lately. Previous methods on user intent recognition in dialogue systems adopt recurrent neural network (RNN) or convolutional neural network (CNN) to model context in conversational history, neglecting the tactic history and intra-speaker relation. In this paper, we demonstrate the limitations of a Transformer-based approach coupled with Conditional Random Field (CRF) for the task of persuasive strategy recognition. In this model, we leverage inter- and intra-speaker contextual semantic features, as well as label dependencies to improve the recognition. Despite extensive hyper-parameter optimizations, this architecture fails to outperform the baseline methods. We observe two negative results. Firstly, CRF cannot capture persuasive label dependencies, possibly as strategies in persuasive dialogues do not follow any strict grammar or rules as the cases in Named Entity Recognition (NER) or part-of-speech (POS) tagging. Secondly, the Transformer encoder trained from scratch is less capable of capturing sequential information in persuasive dialogues than Long Short-Term Memory (LSTM). We attribute this to the reason that the vanilla Transformer encoder does not efficiently consider relative position information of sequence elements.",,56,,,
,,"Ghosal, Deepanway",,,,,,,,,Transformer-based neural networks,,,,,,
,,"Majumder, Navonil",,,,,,,,,Conditional random Field,,,,,,
,,"Hussain, Amir",,,,,,,,,Persuasive strategy recognition,,,,,,
,,"Poria, Soujanya",,,,,,,,,,,,,,,
JOUR,FacTeR-Check: Semi-automated fact-checking through semantic similarity and natural language inference,"Martín, Alejandro",Knowledge-Based Systems,251,109265,2022,2022/09/05/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109265,https://www.sciencedirect.com/science/article/pii/S0950705122006323,Misinformation,"Our society produces and shares overwhelming amounts of information through Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern in most countries. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is a crucial step to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi-automated fact-checking and hoaxes propagation analysis that can be used to implement applications designed for both the general public and for fact-checking organisations. FacTeR-Check implements three different modules relying on the XLM-RoBERTa Transformer architecture to evaluate semantic similarity, to calculate natural language inference and to build search queries through automatic keywords extraction and Named-Entity Recognition. The three modules have been validated using state-of-the-art benchmark datasets, exhibiting good performance in all of them. Besides, FacTeR-Check is employed to collect and label a dataset, called NLI19-SP, composed of more than 40,000 tweets supporting or denying 60 hoaxes related to COVID-19, released publicly. Finally, an analysis of the data collected in this dataset is provided, which allows to obtain a deep insight of how disinformation operated during the COVID-19 pandemic in Spanish-speaking countries.",,,,,
,,"Huertas-Tato, Javier",,,,,,,,,Transformers,,,,,,
,,"Huertas-García, Álvaro",,,,,,,,,COVID-19,,,,,,
,,"Villar-Rodríguez, Guillermo",,,,,,,,,Hoax,,,,,,
,,"Camacho, David",,,,,,,,,Natural language inference,,,,,,
,,,,,,,,,,,Semantic similarity,,,,,,
JOUR,Natural language understanding for argumentative dialogue systems in the opinion building domain,"Abro, Waheed Ahmed",Knowledge-Based Systems,242,108318,2022,2022/04/22/,0950-7051,https://doi.org/10.1016/j.knosys.2022.108318,https://www.sciencedirect.com/science/article/pii/S0950705122001149,Natural language understanding,"This paper introduces a natural language understanding (NLU) framework for argumentative dialogue systems in the information-seeking and opinion building domain. The proposed framework consists of two sub-models, namely intent classifier and argument similarity. Intent classifier model stack BiLSTM with attention mechanism on top of pre-trained BERT model and fine-tune the model for recognizing the user intent, whereas argument similarity model employs BERT+BiLSTM for identifying system arguments the user refers to in his or her natural language utterances. Our model is evaluated in an argumentative dialogue system that engages the user to inform him-/herself about a controversial topic by exploring pro and con arguments and build his/her opinion towards the topic. In order to evaluate the proposed approach, we collect user utterances for the interaction with the respective system labelling intent and referenced argument in an extensive online study. The data collection includes multiple topics and two different user types (native English speakers from the UK and non-native English speakers from China). Additionally, we evaluate the proposed intent classifier and argument similarity models separately on the publicly available Banking77 and STS benchmark datasets. The evaluation indicates a clear advantage of the utilized techniques over baseline approaches on several datasets, as well as the robustness of the proposed approach against new topics and different language proficiency as well as the cultural background of the user. Furthermore, results show that our intent classifier model outperforms DIET, DistillBERT, and BERT fine-tuned models in few-shot setups (i.e., with 10, 20, or 30 labelled examples per intent) and full data setup.",,,,,
,,"Aicher, Annalena",,,,,,,,,Intent classification,,,,,,
,,"Rach, Niklas",,,,,,,,,Sentence similarity,,,,,,
,,"Ultes, Stefan",,,,,,,,,Argumentative dialogue system,,,,,,
,,"Minker, Wolfgang",,,,,,,,,Human–computer interaction,,,,,,
,,"Qi, Guilin",,,,,,,,,,,,,,,
JOUR,How can we detect Homophobia and Transphobia? Experiments in a multilingual code-mixed setting for social media governance,"Chakravarthi, Bharathi Raja",International Journal of Information Management Data Insights,2,100119,2022,2022/11/01/,2667-0968,https://doi.org/10.1016/j.jjimei.2022.100119,https://www.sciencedirect.com/science/article/pii/S2667096822000623,Homophobia detection,"Homophobia or Transphobia can be defined as the hatred, discomfort, or dislike of lesbian, gay, transgender or bisexual people. Studies have shown that these individuals were more likely to develop mental health issues, likely due to being subjected to more forms of abuse on social media. Hence there is an ardent need to develop automated abusive speech detection systems to tackle the abusive content on social media. There has been an elevation in hate speech or abuse and this paper focuses on the LGBTQIA+ community. Due to the shortage of resources in the said study area, we hypothesize that data augmentation via Pseudolabeling by transliterating the code-mixed text to the parent language will improve the models’ performances on the newly constructed dataset. We put our hypothesis into testing, and studied the performances of several multilingual language models for our cause.",2,,,,
,,"Hande, Adeep",,,,,,,,,Transphobia detection,,,,,,
,,"Ponnusamy, Rahul",,,,,,,,,Hate speech detection,,,,,,
,,"Kumaresan, Prasanna Kumar",,,,,,,,,,,,,,,
,,"Priyadharshini, Ruba",,,,,,,,,,,,,,,
JOUR,DictPrompt: Comprehensive dictionary-integrated prompt tuning for pre-trained language model,"Cao, Rui",Knowledge-Based Systems,273,110605,2023,2023/08/03/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110605,https://www.sciencedirect.com/science/article/pii/S0950705123003556,Pre-trained language model,"The textual semantics contained in the PLM (Pre-trained Language Model) is constrained by the text distribution in the original training corpus. Due to the lack of sufficient contextual training corpus, the low-frequency word representations in the PLM often have difficulty capturing their actual semantics. Previous research has shown that using semantic information from dictionaries can alleviate this problem. Unfortunately, these works neglected the infinite potential of example sentences from different target words with various meanings. To re-explore methods for enhancing PLM using the dictionary, we propose a novel Comprehensive Dictionary-based tuning approach integrating the latest Prompt learning (DictPrompt). We first collect a dataset based on the Oxford Advanced Learner’s English Dictionary. Then, we designed a set of comprehensive prompt templates with the corpus combining the word, the definition, and its example sentence. Finally, we insert a word game training task between pre-training and fine-tuning using these templates, allowing the model to inject more semantic information into PLM. We test our Dictprompt tuning method on three commonly used PLMs. The testing results on five fine-grained semantic tasks show that our dictionary-based secondary tuning can bring additional gains to the model’s performance. The best accuracy improves 3.09% on average with our tuning on the WiC task and 7.93% on the WSC task. We also plot the sentence embedding scatters of polysemy words. Our method can smooth the decision boundary and help the model output more distinguishable embedding. The code is available at https://github.com/xbdxwyh/Dictprompt.",,,,,
,,"Wang, Yihao",,,,,,,,,Prompt learning,,,,,,
,,"Gao, Ling",,,,,,,,,Dictionary information,,,,,,
,,"Yang, Meng",,,,,,,,,Masked language model,,,,,,
,,,,,,,,,,,Word game task,,,,,,
JOUR,Creating deep neural networks for text classification tasks using grammar genetic programming,"Magalhães, Dimmy",Applied Soft Computing,135,110009,2023,2023/03/01/,1568-4946,https://doi.org/10.1016/j.asoc.2023.110009,https://www.sciencedirect.com/science/article/pii/S1568494623000273,Text classification,"Text classification is one of the Natural Language Processing (NLP) tasks. Its objective is to label textual elements, such as phrases, queries, paragraphs, and documents. In NLP, several approaches have achieved promising results regarding this task. Deep Learning-based approaches have been widely used in this context, with deep neural networks (DNNs) adding the ability to generate a representation for the data and a learning model. The increasing scale and complexity of DNN architectures was expected, creating new challenges to design and configure the models. In this paper, we present a study on the application of a grammar-based evolutionary approach to the design of DNNs, using models based on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), and Graph Neural Networks (GNNs). We propose different grammars, which were defined to capture the features of each type of network, also proposing some combinations, verifying their impact on the produced designs and performance of the generated models. We create a grammar that is able to generate different networks specialized on text classification, by modification of Grammatical Evolution (GE), and it is composed of three main components: the grammar, mapping, and search engine. Our results offer promising future research directions as they show that the projected architectures have a performance comparable to that of their counterparts but can still be further improved. We were able to improve the results of a manually structured neural network in 8,18% in the best case.",,,,,
,,"Lima, Ricardo H.R.",,,,,,,,,Evolutionary algorithms,,,,,,
,,"Pozo, Aurora",,,,,,,,,Genetic programming,,,,,,
,,,,,,,,,,,Automatic design,,,,,,
,,,,,,,,,,,Grammatical evolution,,,,,,
,,,,,,,,,,,Deep neural networks,,,,,,
JOUR,Multitask learning for recognizing stress and depression in social media,"Ilias, Loukas",Online Social Networks and Media,37-38,100270,2023,2023/09/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100270,https://www.sciencedirect.com/science/article/pii/S2468696423000290,Stress,"Stress and depression are prevalent nowadays across people of all ages due to the quick paces of life. People use social media to express their feelings. Thus, social media constitute a valuable form of information for the early recognition of stress and depression. Although many research works have been introduced targeting the early recognition of stress and depression, there are still limitations. There have been proposed multi-task learning settings, which use depression and emotion (or figurative language) as the primary and auxiliary tasks respectively. However, although stress is inextricably linked with depression, researchers face these two tasks as two separate tasks. To address these limitations, we present the first study, which exploits two different datasets collected under different conditions, and introduce two multitask learning frameworks, which use depression and stress as the main and auxiliary tasks respectively. Specifically, we use a depression dataset and a stressful dataset including stressful posts from ten subreddits of five domains. In terms of the first approach, each post passes through a shared BERT layer, which is updated by both tasks. Next, two separate BERT encoder layers are exploited, which are updated by each task separately. Regarding the second approach, it consists of shared and task-specific layers weighted by attention fusion networks. We conduct a series of experiments and compare our approaches with existing research initiatives, single-task learning, and transfer learning. Experiments show multiple advantages of our approaches over state-of-the-art ones.",,,,,
,,"Askounis, Dimitris",,,,,,,,,Depression,,,,,,
,,,,,,,,,,,Multi-task learning,,,,,,
,,,,,,,,,,,Single-task learning,,,,,,
,,,,,,,,,,,Transfer learning,,,,,,
JOUR,Boosting source code suggestion with self-supervised Transformer Gated Highway,"Hussain, Yasir",Journal of Systems and Software,196,111553,2023,2023/02/01/,0164-1212,https://doi.org/10.1016/j.jss.2022.111553,https://www.sciencedirect.com/science/article/pii/S0164121222002291,Deep learning,"Attention-based transformer language models have shown significant performance gains in various natural language tasks. In this work, we explore the impact of transformer language models on the task of source code suggestion. The core intention of this work is to boost the modeling performance for the source code suggestion task and to explore how the training procedures and model architectures impact modeling performance. Additionally, we propose a transformer-based self-supervised learning technique called Transformer Gated Highway that outperforms recurrent and transformer language models of comparable size. The proposed approach combines the Transformer language model with Gated Highway introducing a notion of recurrence. We compare the performance of the proposed approach with transformer-based BERT (CodeTran), RoBERTa (RoBERTaCode), GPT2 (TravTrans), CodeGen and recurrent neural language-based LSTM (CodeLSTM) models. Moreover, we have experimented with various architectural settings for the transformer models to evaluate their impact on modeling performance. The extensive evaluation of the presented approach exhibits better performance on two programming language datasets; Java and C#. Additionally, we have adopted the presented approach for the syntax error correction task to predict the correct syntax token to render its possible implications for other source code modeling tasks.",,,,,
,,"Huang, Zhiqiu",,,,,,,,,Transformer models,,,,,,
,,"Zhou, Yu",,,,,,,,,Source code modeling,,,,,,
,,"Wang, Senzhang",,,,,,,,,Source code suggestion,,,,,,
JOUR,AdaSL: An Unsupervised Domain Adaptation framework for Arabic multi-dialectal Sequence Labeling,"El Mekki, Abdellah",Information Processing & Management,59,102964,2022,2022/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.102964,https://www.sciencedirect.com/science/article/pii/S0306457322000814,Dialectal Arabic,"Dialectal Arabic (DA) refers to varieties of everyday spoken languages in the Arab world. These dialects differ according to the country and region of the speaker, and their textual content is constantly growing with the rise of social media networks and web blogs. Although research on Natural Language Processing (NLP) on standard Arabic, namely Modern Standard Arabic (MSA), has witnessed remarkable progress, research efforts on DA are rather limited. This is due to numerous challenges, such as the scarcity of labeled data as well as the nature and structure of DA. While some recent works have reached decent results on several DA sentence classification tasks, other complex tasks, such as sequence labeling, still suffer from weak performances when it comes to DA varieties with either a limited amount of labeled data or unlabeled data only. Besides, it has been shown that zero-shot transfer learning from models trained on MSA does not perform well on DA. In this paper, we introduce AdaSL, a new unsupervised domain adaptation framework for Arabic multi-dialectal sequence labeling, leveraging unlabeled DA data, labeled MSA data, and existing multilingual and Arabic Pre-trained Language Models (PLMs). The proposed framework relies on four key components: (1) domain adaptive fine-tuning of multilingual/MSA language models on unlabeled DA data, (2) sub-word embedding pooling, (3) iterative self-training on unlabeled DA data, and (4) iterative DA and MSA distribution alignment. We evaluate our framework on multi-dialectal Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The overall results show that the zero-shot transfer learning, using our proposed framework, boosts the performance of the multilingual PLMs by 40.87% in macro-F1 score for the NER task, while it boosts the accuracy by 6.95% for the POS tagging task. For the Arabic PLMs, our proposed framework increases performance by 16.18% macro-F1 for the NER task and 2.22% accuracy for the POS tagging task, and thus, achieving new state-of-the-art zero-shot transfer learning performance for Arabic multi-dialectal sequence labeling.",4,,,,
,,"El Mahdaouy, Abdelkader",,,,,,,,,Arabic natural language processing,,,,,,
,,"Berrada, Ismail",,,,,,,,,Domain adaptation,,,,,,
,,"Khoumsi, Ahmed",,,,,,,,,Multi-dialectal sequence labeling,,,,,,
,,,,,,,,,,,Named entity recognition,,,,,,
,,,,,,,,,,,Part-of-speech tagging,,,,,,
,,,,,,,,,,,Zero-shot transfer learning,,,,,,
JOUR,Building for tomorrow: Assessing the temporal persistence of text classifiers,"Alkhalifa, Rabab",Information Processing & Management,60,103200,2023,2023/03/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103200,https://www.sciencedirect.com/science/article/pii/S0306457322003016,Text classification,"Performance of text classification models tends to drop over time due to changes in data, which limits the lifetime of a pretrained model. Therefore an ability to predict a model’s ability to persist over time can help design models that can be effectively used over a longer period of time. In this paper, we provide a thorough discussion into the problem, establish an evaluation setup for the task. We look at this problem from a practical perspective by assessing the ability of a wide range of language models and classification algorithms to persist over time, as well as how dataset characteristics can help predict the temporal stability of different models. We perform longitudinal classification experiments on three datasets spanning between 6 and 19 years, and involving diverse tasks and types of data. By splitting the longitudinal datasets into years, we perform a comprehensive set of experiments by training and testing across data that are different numbers of years apart from each other, both in the past and in the future. This enables a gradual investigation into the impact of the temporal gap between training and test sets on the classification performance, as well as measuring the extent of the persistence over time. Through experimenting with a range of language models and algorithms, we observe a consistent trend of performance drop over time, which however differs significantly across datasets; indeed, datasets whose domain is more closed and language is more stable, such as with book reviews, exhibit a less pronounced performance drop than open-domain social media datasets where language varies significantly more. We find that one can estimate how a model will retain its performance over time based on (i) how well the model performs over a restricted time period and its extrapolation to a longer time period, and (ii) the linguistic characteristics of the dataset, such as the familiarity score between subsets from different years. Findings from these experiments have important implications for the design of text classification models with the aim of preserving performance over time.",2,,,,
,,"Kochkina, Elena",,,,,,,,,Temporal embedding,,,,,,
,,"Zubiaga, Arkaitz",,,,,,,,,Temporal generalisability,,,,,,
,,,,,,,,,,,Temporal persistence,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Pretrained language models,,,,,,
JOUR,Cross-SEAN: A cross-stitch semi-supervised neural attention model for COVID-19 fake news detection,"Paka, William Scott",Applied Soft Computing,107,107393,2021,2021/08/01/,1568-4946,https://doi.org/10.1016/j.asoc.2021.107393,https://www.sciencedirect.com/science/article/pii/S1568494621003161,Fake news detection,"As the COVID-19 pandemic sweeps across the world, it has been accompanied by a tsunami of fake news and misinformation on social media. At the time when reliable information is vital for public health and safety, COVID-19 related fake news has been spreading even faster than the facts. During times such as the COVID-19 pandemic, fake news can not only cause intellectual confusion but can also place people’s lives at risk. This calls for an immediate need to contain the spread of such misinformation on social media. We introduce CTF, a large-scale COVID-19 Twitter dataset with labelled genuine and fake tweets. Additionally, we propose Cross-SEAN, a cross-stitch based semi-supervised end-to-end neural attention model which leverages the large amount of unlabelled data. Cross-SEAN partially generalises to emerging fake news as it learns from relevant external knowledge. We compare Cross-SEAN with seven state-of-the-art fake news detection methods. We observe that it achieves 0.95 F1 Score on CTF, outperforming the best baseline by 9%. We also develop Chrome-SEAN, a Cross-SEAN based chrome extension for real-time detection of fake tweets.",,,,,
,,"Bansal, Rachit",,,,,,,,,Social media,,,,,,
,,"Kaushik, Abhay",,,,,,,,,COVID-19,,,,,,
,,"Sengupta, Shubhashis",,,,,,,,,,,,,,,
,,"Chakraborty, Tanmoy",,,,,,,,,,,,,,,
JOUR,Multimodal hate speech detection via multi-scale visual kernels and knowledge distillation architecture,"Chhabra, Anusha",Engineering Applications of Artificial Intelligence,126,106991,2023,2023/11/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106991,https://www.sciencedirect.com/science/article/pii/S0952197623011752,Hate content,"People increasingly use social media platforms to express themselves by posting visuals and texts. As a result, hate content is on the rise, necessitating practical visual caption analysis. Thus, the relationship between image and caption modalities is crucial in visual caption analysis. Contrarily, most methods combine features from the image and caption modalities using deep learning architectures with millions of parameters already trained without integrating a specialized attention module, resulting in less desirable outcomes. This paper suggests a novel multi-modal architecture for identifying hateful memetic information in response to the above observation. The proposed architecture contains a novel ""multi-scale kernel attentive visual"" (MSKAV) module that uses an efficient multi-branch structure to extract discriminative visual features. Additionally, MSKAV utilizes an adaptive receptive field using multi-scale kernels. MSKAV also incorporates a multi-directional visual attention module to highlight spatial regions of importance. The proposed model also contains a novel ""knowledge distillation-based attentional caption"" (KDAC) module. It uses a transformer-based self-attentive block to extract discriminative features from meme captions. Thorough experimentation on multi-modal hate speech benchmarks MultiOff, Hateful Memes, and MMHS150K datasets achieved accuracy scores of 0.6250, 0.8750, and 0.8078, respectively. It also reaches impressive AUC scores of 0.6557, 0.8363, and 0.7665 on the three datasets, respectively, beating SOTA multi-modal hate speech identification models.",,,,,
,,"Vishwakarma, Dinesh Kumar",,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Multimodal,,,,,,
,,,,,,,,,,,Adaptive receptive field,,,,,,
JOUR,Improving classifier training efficiency for automatic cyberbullying detection with Feature Density,"Eronen, Juuso",Information Processing & Management,58,102616,2021,2021/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2021.102616,https://www.sciencedirect.com/science/article/pii/S0306457321001126,Feature density,"We study the effectiveness of Feature Density (FD) using different linguistically-backed feature preprocessing methods in order to estimate dataset complexity, which in turn is used to comparatively estimate the potential performance of machine learning (ML) classifiers prior to any training. We hypothesize that estimating dataset complexity allows for the reduction of the number of required experiments iterations. This way we can optimize the resource-intensive training of ML models which is becoming a serious issue due to the increases in available dataset sizes and the ever rising popularity of models based on Deep Neural Networks (DNN). The problem of constantly increasing needs for more powerful computational resources is also affecting the environment due to alarmingly-growing amount of CO2 emissions caused by training of large-scale ML models. The research was conducted on multiple datasets, including popular datasets, such as Yelp business review dataset used for training typical sentiment analysis models, as well as more recent datasets trying to tackle the problem of cyberbullying, which, being a serious social problem, is also a much more sophisticated problem form the point of view of linguistic representation. We use cyberbullying datasets collected for multiple languages, namely English, Japanese and Polish. The difference in linguistic complexity of datasets allows us to additionally discuss the efficacy of linguistically-backed word preprocessing.",5,,,,
,,"Ptaszynski, Michal",,,,,,,,,Dataset complexity,,,,,,
,,"Masui, Fumito",,,,,,,,,Linguistics,,,,,,
,,"Smywiński-Pohl, Aleksander",,,,,,,,,Cyberbullying,,,,,,
,,"Leliwa, Gniewosz",,,,,,,,,Document classification,,,,,,
,,"Wroczynski, Michal",,,,,,,,,Preprocessing,,,,,,
JOUR,End-to-end multimodal affect recognition in real-world environments,"Tzirakis, Panagiotis",Information Fusion,68,46,2021,2021/04/01/,1566-2535,https://doi.org/10.1016/j.inffus.2020.10.011,https://www.sciencedirect.com/science/article/pii/S1566253520303808,Deep learning,"Automatic affect recognition in real-world environments is an important task towards a natural interaction between humans and machines. The recent years, several advancements have been accomplished in determining the emotional states with the use of Deep Neural Networks (DNNs). In this paper, we propose an emotion recognition system that utilizes the raw text, audio and visual information in an end-to-end manner. To capture the emotional states of a person, robust features need to be extracted from the various modalities. To this end, we utilize Convolutional Neural Networks (CNNs) and propose a novel transformer-based architecture for the text modality that can robustly capture the semantics of sentences. We develop an audio model to process the audio channel, and adopt a variation of a high resolution network (HRNet) to process the visual modality. To fuse the modality-specific features, we propose novel attention-based methods. To capture the temporal dynamics in the signal, we utilize Long Short-Term Memory (LSTM) networks. Our model is trained on the SEWA dataset of the AVEC 2017 research sub-challenge on emotion recognition, and produces state-of-the-art results in the text, visual and multimodal domains, and comparable performance in the audio case when compared with the winning papers of the challenge that use several hand-crafted and DNN features. Code is available at: https://github.com/glam-imperial/multimodal-affect-recognition.",,53,,,
,,"Chen, Jiaxin",,,,,,,,,Sentiment analysis,,,,,,
,,"Zafeiriou, Stefanos",,,,,,,,,Emotion recognition,,,,,,
,,"Schuller, Björn",,,,,,,,,Multimodal machine learning,,,,,,
JOUR,A contrastive learning framework for Event Detection via semantic type prototype representation modelling,"Hao, Anran",Neurocomputing,556,126613,2023,2023/11/01/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126613,https://www.sciencedirect.com/science/article/pii/S0925231223007361,Event Detection,"The diversity of natural language expressions for describing events poses a challenge for the task of Event Detection (ED) with machine learning methods. To detect and classify event mentions, ED models essentially need to construct a semantic linkage between representations of the mentions and a set of target types. Unfortunately, most existing models use meaningless homogeneous one-hot vectors to represent the event type classes in ED, ignoring the fact that the event type labels also consist of meaningful words and can provide important clues for type representation learning. In this paper, we propose a Contrastive Semantic Prototype Representation Learning Framework for Event Detection (SemPRE), which exploits the pre-defined event type label words to inject the semantic information of the types and guide event detection. Specifically, we utilize pre-trained BERT to fuse text and event type into a joint representation space, and employ a contrastive-regularized module to enhance cross-type interaction. We conduct extensive experiments on the ACE 2005 and MAVEN benchmark datasets. The performance results show that our proposed SemPRE model achieves state-of-the-art performance on the datasets and outperforms existing baselines on limited annotated data and without using any external resources. Further analysis shows that our model is also effective in detecting multiple events and ambiguous trigger words.",,,,,
,,"Luu, Anh Tuan",,,,,,,,,Information extraction,,,,,,
,,"Hui, Siu Cheung",,,,,,,,,Representation learning,,,,,,
,,"Su, Jian",,,,,,,,,,,,,,,
JOUR,What goes on inside rumour and non-rumour tweets and their reactions: A psycholinguistic analyses,"Butt, Sabur",Computers in Human Behavior,135,107345,2022,2022/10/01/,0747-5632,https://doi.org/10.1016/j.chb.2022.107345,https://www.sciencedirect.com/science/article/pii/S0747563222001674,Rumour detection,"In recent years, the problem of rumours on online social media (OSM) has attracted lots of attention. Researchers have started investigating from two main directions. First is the descriptive analysis of rumours and secondly, proposing techniques to detect (or classify) rumours. In the descriptive line of works, where researchers have tried to analyse rumours using NLP approaches, there isn't much emphasis on psycho-linguistics analyses of social media text. These kinds of analyses on rumour case studies are vital for drawing meaningful conclusions to mitigate misinformation. For our analysis, we explored the PHEME-9 rumour dataset (consisting of 9 events), including source tweets (both rumour and non-rumour categories) and response tweets. We compared the rumour and non-rumour source tweets and then their corresponding reply (response) tweets to understand how they differ linguistically for every incident. Furthermore, we also evaluated if these features can be used for classifying rumour vs. non-rumour tweets through machine learning models. To this end, we employed various classical and ensemble-based approaches. To filter out the highly discriminative psycholinguistic features, we explored the SHAP AI Explainability tool. To summarise, this research contributes by performing an in-depth psycholinguistic analysis of rumours related to various kinds of events.",,,,,
,,"Sharma, Shakshi",,,,,,,,,Psycho-linguistic analyses,,,,,,
,,"Sharma, Rajesh",,,,,,,,,Explainable AI,,,,,,
,,"Sidorov, Grigori",,,,,,,,,,,,,,,
,,"Gelbukh, Alexander",,,,,,,,,,,,,,,
JOUR,Data augmentation via context similarity: An application to biomedical Named Entity Recognition,"Bartolini, Ilaria",Information Systems,119,102291,2023,2023/10/01/,0306-4379,https://doi.org/10.1016/j.is.2023.102291,https://www.sciencedirect.com/science/article/pii/S0306437923001278,Named Entity Recognition,"In this paper, we present COntext SImilarity-based data augmentation for NER (COSINER), a new method for improving Named Entity Recognition (NER) tasks using data augmentation. Unlike current techniques, which may generate noisy and mislabeled samples through text manipulation, COSINER uses context similarity to replace entity mentions with more plausible ones on the basis of available training data and considering the context in which entities typically appear. Through experiments on popular benchmark datasets, we show that COSINER outperforms existing baselines in various few-shot scenarios where training data is limited. Additionally, our method’s computing times are comparable to the simplest augmentation methods and are better than approaches that rely on pre-trained models in their architecture.",,,,,
,,"Moscato, Vincenzo",,,,,,,,,Data augmentation,,,,,,
,,"Postiglione, Marco",,,,,,,,,Similarity learning,,,,,,
,,"Sperlì, Giancarlo",,,,,,,,,Few shot learning,,,,,,
,,"Vignali, Andrea",,,,,,,,,,,,,,,
JOUR,Efficient text-based evolution algorithm to hard-label adversarial attacks on text,"Peng, Hao",Journal of King Saud University - Computer and Information Sciences,35,101539,2023,2023/05/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.03.017,https://www.sciencedirect.com/science/article/pii/S131915782300085X,Natural language processing,"Deep neural networks that play a pivotal role in fields such as images, text, and audio are vulnerable to adversarial attacks. In current textual adversarial attacks, the vast majority are configured with a black-box soft-label which is achieved by the gradient information or confidence of the model. Therefore, it becomes challenging and realistic to implement adversarial attacks using only the predicted top labels of the hard-label model. Existing methods to implement hard-label adversarial attacks use population-based genetic optimization algorithms. However, this approach requires significant query consumption, which is a considerable shortcoming. To solve this problem, we propose a new textual black-box hard-label adversarial attack algorithm based on the idea of differential evolution of populations, called the text-based differential evolution (TDE) algorithm. First, the method will judge the importance of the words of the initial rough adversarial examples, according to which only the keywords in the text sentence will be operated, and the rest of the words will be gradually replaced with the original words so as to reduce the words in the sentence in which the replacement occurs. Our method judges the quality of semantic similarity of the adversarial examples in the replacement process and deposits high-quality adversarial example individuals into the population. Secondly, the optimization process of adversarial examples is combined and optimized according to the word importance. Compared with existing methods based on genetic algorithm guidance, our method avoids a large number of meaningless repetitive queries and significantly improves the overall attack efficiency of the algorithm and the semantic quality of the generated adversarial examples. We experimented with multiple datasets on three text tasks of sentiment classification, natural language inference, and toxic comment, and also perform experimental comparisons on models and APIs in realistic scenarios. For example, in the Google Cloud commercial API adversarial attack experiment, compared to the existing hard-label method, our method reduces the average number of queries required for the attack from 6986 to 176, and increases semantic similarity from 0.844 to 0.876. It is shown through extensive experimental data that our approach not only significantly reduces the number of queries, but also significantly outperforms existing methods in terms of the quality of adversarial examples.",5,,,,
,,"Wang, Zhe",,,,,,,,,Machine learning,,,,,,
,,"Zhao, Dandan",,,,,,,,,Language model,,,,,,
,,"Wu, Yiming",,,,,,,,,Adversarial attack,,,,,,
,,"Han, Jianming",,,,,,,,,Black-box attack,,,,,,
,,"Guo, Shixin",,,,,,,,,Hard-label,,,,,,
,,"Ji, Shouling",,,,,,,,,,,,,,,
,,"Zhong, Ming",,,,,,,,,,,,,,,
JOUR,Transformer-based language models for mental health issues: A survey,"Greco, Candida M.",Pattern Recognition Letters,167,204,2023,2023/03/01/,0167-8655,https://doi.org/10.1016/j.patrec.2023.02.016,https://www.sciencedirect.com/science/article/pii/S0167865523000430,Transformers,"Early identification and prevention of mental health stresses and their outcomes has become of urgent importance worldwide. To this purpose, artificial intelligence provides a body of advanced computational tools that can effectively support decision-making clinical processes by modeling and analyzing the presence of a variety of mental health issues, particularly when these can be detected in text data. In this regard, Transformer-based language models (TLMs) have demonstrated exceptional efficacy in a number of NLP tasks also in the health domain. To the best of our knowledge, the use of TLMs for specifically addressing mental health issues has not been deeply investigated so far. In this paper, we aim to fill this gap in the literature by providing the first survey of methods using TLMs for text-based identification of mental health issues.",,211,,,
,,"Simeri, Andrea",,,,,,,,,Language models,,,,,,
,,"Tagarelli, Andrea",,,,,,,,,Mental health,,,,,,
,,"Zumpano, Ester",,,,,,,,,NLP,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Benchmarks,,,,,,
JOUR,Hierarchical BERT with an adaptive fine-tuning strategy for document classification,"Kong, Jun",Knowledge-Based Systems,238,107872,2022,2022/02/28/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107872,https://www.sciencedirect.com/science/article/pii/S0950705121010479,Document classification,"Pretrained language models (PLMs) have achieved impressive results and have become vital tools for various natural language processing (NLP) tasks. However, there is a limitation that applying these PLMs to document classification when the document length exceeds the maximum acceptable length of the PLM since the excess portion is truncated in these models. If the keywords are in the truncated part, then the performance of the model declines. To address this problem, this paper proposes a hierarchical BERT with an adaptive fine-tuning strategy (HAdaBERT). It consists of a BERT-based model as the local encoder and an attention-based gated memory network as the global encoder. In contrast to existing PLMs that directly truncate documents, the proposed model uses a part of the document as a region, dividing input document into several containers. This allows the useful information in each container to be extracted by a local encoder and composed by a global encoder according to its contribution to the classification. To further improve the performance of the model, this paper proposes an adaptive fine-tuning strategy, which dynamically decides the layers of BERT to be fine-tuned instead of fine-tuning all layers for each input text. Experimental results on different corpora indicated that this method outperformed existing neural networks for document classification.",,,,,
,,"Wang, Jin",,,,,,,,,Hierarchical BERT,,,,,,
,,"Zhang, Xuejie",,,,,,,,,Adaptive fine-tuning strategy,,,,,,
,,,,,,,,,,,Pretrained language model,,,,,,
JOUR,Syntax-based dynamic latent graph for event relation extraction,"Zhuang, Ling",Information Processing & Management,60,103469,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103469,https://www.sciencedirect.com/science/article/pii/S0306457323002066,Information extraction,"This paper focuses on extracting temporal and parent–child relationships between news events in social news. Previous methods have proved that syntactic features are valid. However, most previous methods directly use the static outcomes parsed by syntactic parsing tools, but task-irrelevant or erroneous parses will inevitably degrade the performance of the model. In addition, many implicit higher-order connections that are directly related and critical to tasks are not explicitly exploited. In this paper, we propose a novel syntax-based dynamic latent graph model (SDLG) for this task. Specifically, we first apply a syntactic type-enhanced attention mechanism to assign different weights to different connections in the parsing results, which helps to filter out noisy connections and better fuse the information in the syntactic structures. Next, we introduce a dynamic event pair-aware induction graph to mine the task-related latent connections. It constructs a potential attention matrix to complement and correct the supervised syntactic features, using the semantics of the event pairs as a guide. Finally, the latent graph, together with the syntactic information, is fed into the graph convolutional network to obtain an improved representation of the event to complete relational reasoning. We have conducted extensive experiments on four public benchmarks, MATRES, TCR, HiEve and TB-Dense. The results show that our model outperforms the state-of-the-art model by 0.4%, 1.5%, 3.0% and 1.3% in F1 scores on the four datasets, respectively. Finally, we provide detailed analyses to show the effectiveness of each proposed component.",5,,,,
,,"Fei, Hao",,,,,,,,,Event relation extraction,,,,,,
,,"Hu, Po",,,,,,,,,Syntactic dependence,,,,,,
,,,,,,,,,,,Graph modeling,,,,,,
,,,,,,,,,,,Latent structure,,,,,,
JOUR,Speciesist language and nonhuman animal bias in English Masked Language Models,"Takeshita, Masashi",Information Processing & Management,59,103050,2022,2022/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103050,https://www.sciencedirect.com/science/article/pii/S0306457322001558,Social bias,"Warning: This paper contains examples of offensive language, including insulting or objectifying expressions. Various existing studies have analyzed what social biases are inherited by NLP models. These biases may directly or indirectly harm people, therefore previous studies have focused only on human attributes. However, until recently no research on social biases in NLP regarding nonhumans existed. In this paper,11Anonymous previous version of this paper is accessible at https://openreview.net/forum?id=dfqMpjZOgv4, we have also published a paper on this topic in Japanese (Takeshita et al., 2021). we analyze biases to nonhuman animals, i.e. speciesist bias, inherent in English Masked Language Models such as BERT. We analyzed speciesist bias against 46 animal names using template-based and corpus-extracted sentences containing speciesist (or non-speciesist) language. We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names. Our code for reproducing the experiments will be made available on GitHub.22https://github.com/Language-Media-Lab/speciesist-language.",5,,,,
,,"Rzepka, Rafal",,,,,,,,,Bias evaluation,,,,,,
,,"Araki, Kenji",,,,,,,,,Masked Language Model,,,,,,
,,,,,,,,,,,Animal ethics,,,,,,
,,,,,,,,,,,Speciesism,,,,,,
JOUR,SMFNM: Semi-supervised multimodal fusion network with main-modal for real-time emotion recognition in conversations,"Yang, Juan",Journal of King Saud University - Computer and Information Sciences,35,101791,2023,2023/10/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101791,https://www.sciencedirect.com/science/article/pii/S1319157823003452,Real-time Emotion recognition in conversations,"Real-time emotion recognition in conversations (ERC), which relies on only the historical utterances to achieve ERC, has recently gained increasing attention due to its significance in providing real-time empathetic services. Although utilizing multimodal information can mitigate the issues of unimodal approaches, few real-time ERC studies consider the differences in representation ability of different modalities and explore comprehensive conversational context from different perspectives based on different structures. Furthermore, the heavy annotation cost makes it difficult to collect sufficient labeled data, which also limits the performance of current supervised ERC approaches. To address these issues, we propose a novel framework SMFNM for real-time ERC, which integrates semi-supervised learning with multimodal fusion under the guidance of main-modal. Specifically, SMFNM utilizes additional unlabeled data to extract high-quality intra-modal representations, and implements cross-modal interaction to capture complementary information to enhance the audio representations. Then SMFNM employs the directed acyclic graph and the Gated Recurrent Units for exploring more accurate conversational context from both the multimodal and main-modal perspectives, respectively. Finally, these two types of contextual features are fused for emotion identification. Extensive experiments on benchmark datasets (i.e., IEMOCAP (4-way), IEMOCAP (6-way) and MELD) demonstrate the effectiveness, superiority and rationality of our SMFNM.",9,,,,
,,"Dong, Xuanxiong",,,,,,,,,Semi-supervised learning,,,,,,
,,"Du, Xu",,,,,,,,,Main modal,,,,,,
,,,,,,,,,,,Multimodal interaction,,,,,,
,,,,,,,,,,,Multimodal fusion network,,,,,,
JOUR,On exploiting transformers for detecting explicit song lyrics,"Rospocher, Marco",Entertainment Computing,43,100508,2022,2022/08/01/,1875-9521,https://doi.org/10.1016/j.entcom.2022.100508,https://www.sciencedirect.com/science/article/pii/S1875952122000325,Transformer-based language models,"Determining if the lyrics of a given song could be hurtful or inappropriate for children is of utmost importance to prevent the reproduction of songs whose textual content is unsuitable for them. This problem can be computationally tackled as a binary classification task, and in the last couple of years various machine learning approaches have been applied to perform this task automatically. In this work, we investigate the automatic detection of explicit song lyrics by leveraging transformer-based language models, i.e., large language representations, unsupervisely built from huge textual corpora, that can be fine-tuned on various natural language processing tasks, such as text classification. We assess the performance of various transformer-based language model classifiers on a dataset consisting of more than 800K lyrics, marked with explicit information. The evaluation shows that while the classifiers built with these powerful tools achieve state-of-the-art performance, they do not outperform lighter and computationally less demanding approaches. We complement this empirical evaluation with further analyses, including an assessment of the performance of these classifiers in a few-shot learning scenario, where they are trained with just few thousands of samples.",,,,,
,,,,,,,,,,,Convolutional neural networks,,,,,,
,,,,,,,,,,,Text classification,,,,,,
,,,,,,,,,,,Explicit content detection,,,,,,
JOUR,Question classification using limited labelled data,"Mallikarjuna, Chindukuri",Information Processing & Management,59,103094,2022,2022/11/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103094,https://www.sciencedirect.com/science/article/pii/S0306457322001959,Question classification,"Question classification (QC) involves classifying given question based on the expected answer type and is an important task in the Question Answering(QA) system. Existing approaches for question classification use full training dataset to fine-tune the models. It is expensive and requires more time to develop labelled datasets in huge size. Hence, there is a need to develop approaches that can achieve comparable or state of the art performance using limited training instances. In this paper, we propose an approach that uses data augmentation as a tool to generate additional training instances. We evaluate our proposed approach on two question classification datasets namely TREC and ICHI datasets. Experimental results show that our proposed approach reduces the requirement of labelled instances (a) up to 81.7% and achieves new state of the art accuracy of 98.11 on TREC dataset and (b) up to 75% and achieves 67.9 on ICHI dataset.",6,,,,
,,"Sivanesan, Sangeetha",,,,,,,,,Pre-trained language models,,,,,,
,,,,,,,,,,,Transfer learning,,,,,,
,,,,,,,,,,,Data augmentation,,,,,,
,,,,,,,,,,,Minimal labelled data,,,,,,
JOUR,Fake news detection via knowledgeable prompt learning,"Jiang, Gongyao",Information Processing & Management,59,103029,2022,2022/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103029,https://www.sciencedirect.com/science/article/pii/S030645732200139X,Fake news detection,"The spread of fake news has become a significant social problem, drawing great concern for fake news detection (FND). Pretrained language models (PLMs), such as BERT and RoBERTa can benefit this task much, leading to state-of-the-art performance. The common paradigm of utilizing these PLMs is fine-tuning, in which a linear classification layer is built upon the well-initialized PLM network, resulting in an FND mode, and then the full model is tuned on a training corpus. Although great successes have been achieved, this paradigm still involves a significant gap between the language model pretraining and target task fine-tuning processes. Fortunately, prompt learning, a new alternative to PLM exploration, can handle the issue naturally, showing the potential for further performance improvements. To this end, we propose knowledgeable prompt learning (KPL) for this task. First, we apply prompt learning to FND, through designing one sophisticated prompt template and the corresponding verbal words carefully for the task. Second, we incorporate external knowledge into the prompt representation, making the representation more expressive to predict the verbal words. Experimental results on two benchmark datasets demonstrate that prompt learning is better than the baseline fine-tuning PLM utilization for FND and can outperform all previous representative methods. Our final knowledgeable model (i.e, KPL) can provide further improvements. In particular, it achieves an average increase of 3.28% in F1 score under low-resource conditions compared with fine-tuning.",5,,,,
,,"Liu, Shuang",,,,,,,,,Prompt learning,,,,,,
,,"Zhao, Yu",,,,,,,,,Pretrained language model,,,,,,
,,"Sun, Yueheng",,,,,,,,,Knowledge utilization,,,,,,
,,"Zhang, Meishan",,,,,,,,,,,,,,,
JOUR,Extensive evaluation of transformer-based architectures for adverse drug events extraction,"Scaboro, Simone",Knowledge-Based Systems,275,110675,2023,2023/09/05/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110675,https://www.sciencedirect.com/science/article/pii/S0950705123004252,Adverse drug events,"Adverse Drug Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that describe them: model category (AutoEncoding, AutoRegressive, Text-to-Text), pre-training domain, training from scratch, and model size in number of parameters. At the end of our analyses, we identify a list of take-home messages that can be derived from the experimental data.",,,,,
,,"Portelli, Beatrice",,,,,,,,,Transformers,,,,,,
,,"Chersoni, Emmanuele",,,,,,,,,Side effects,,,,,,
,,"Santus, Enrico",,,,,,,,,Extraction,,,,,,
,,"Serra, Giuseppe",,,,,,,,,,,,,,,
JOUR,Hybrid weakly supervised learning with deep learning technique for detection of fake news from cyber propaganda,"Syed, Liyakathunisa",Array,19,100309,2023,2023/09/01/,2590-0056,https://doi.org/10.1016/j.array.2023.100309,https://www.sciencedirect.com/science/article/pii/S2590005623000346,Fake news detection,"Due to the emergence of social networking sites and social media platforms, there is faster information dissemination to the public. Unverified information is widely disseminated across social media platforms without any apprehension about the accuracy of the information. The propagation of false news has imposed significant challenges on governments and society and has several adverse effects on many aspects of human life. Fake News is inaccurate information deliberately created and spread to the public. Accurate detection of fake news from cyber propagation is thus a significant and challenging issue that can be addressed through deep learning techniques. It is impossible to manually annotate large volumes of social media-generated data. In this research, a hybrid approach is proposed to detect fake news, novel weakly supervised learning is applied to provide labels to the unlabeled data, and detection of fake news is performed using Bi- GRU and Bi-LSTM deep learning techniques. Feature extraction was performed by utilizing TF-IDF and Count Vectorizers techniques. Bi-LSTM and Bi-GRU deep learning techniques with Weakly supervised SVM techniques provided an accuracy of 90% in detecting fake news. This approach of labeling large amounts of unlabeled data with weakly supervised learning and deep learning techniques for the detection of fake and real news is highly effective and efficient when there exist no labels to the data.",,,,,
,,"Alsaeedi, Abdullah",,,,,,,,,Feature extraction,,,,,,
,,"Alhuri, Lina A.",,,,,,,,,Deep learning,,,,,,
,,"Aljohani, Hutaf R.",,,,,,,,,Weakly supervised learning,,,,,,
JOUR,Multi-loop graph convolutional network for multimodal conversational emotion recognition,"Ren, Minjie",Journal of Visual Communication and Image Representation,94,103846,2023,2023/06/01/,1047-3203,https://doi.org/10.1016/j.jvcir.2023.103846,https://www.sciencedirect.com/science/article/pii/S1047320323000962,Conversational emotion recognition,"Emotion recognition in conversations (ERC) has gained increasing research attention in recent years due to its wide applications in a surge of emerging tasks, such as social media analysis, dialog generation, and recommender systems. Since constituent utterances in a conversation are closely semantic-related, the constituent utterances’ emotional states are also closely related. In our consideration, this correlation could serve as a guide for the emotion recognition of constituent utterances. Accordingly, we propose a novel approach named Semantic-correlation Graph Convolutional Network (SC-GCN) to take advantage of this correlation for the ERC task in multimodal scenario. Specifically, we first introduce a hierarchical fusion module to model the dynamics among the textual, acoustic and visual features and fuse the multimodal information. Afterward, we construct a graph structure based on the speaker and temporal dependency of the dialog. We put forward a novel multi-loop architecture to explore the semantic correlations by the self-attention mechanism and enhance the correlation information via multiple loops. Through the graph convolution process, the proposed SC-GCN finally obtains a refined representation of each utterance, which is used for the final prediction. Extensive experiments are conducted on two benchmark datasets and the experimental results demonstrate the superiority of our SC-GCN.",,,,,
,,"Huang, Xiangdong",,,,,,,,,Multi-modal sentiment analysis,,,,,,
,,"Li, Wenhui",,,,,,,,,Graph convolutional network,,,,,,
,,"Liu, Jing",,,,,,,,,,,,,,,
JOUR,A short survey for Vietnamese micro-text augmentation techniques,"Duong, Huu-Thanh",Procedia Computer Science,218,301,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2023.01.012,https://www.sciencedirect.com/science/article/pii/S1877050923000121,text augmentation,"The requisites of a powered-AI system is to have a big enough annotated data. Lack of the datasets is a big challenge to obtain the robustness of the predictive models so that it can broaden the AI ideas to various domains. The predictive models are less generalized and prone to overfit. Although the resources for Vietnamese have been investigated more and more, it has still been a low-resources language which is the biggest barrier in order to leverage the robustness of the AI applications. Building the datasets consumes so much time and money. This paper presents the text augmentation to generate the new annotated training data without user's intervention. This paper has summarized the potential methods, especially for the cross-languages methods to enhance the data, analyzed and evaluated the advantages and disadvantages of each method to apply to Vietnamese language processing. The synthetic presentation shows text augmentation has gained competitive performances and helped to save the time and money to build the data.",,309,International Conference on Machine Learning and Data Engineering,,
,,"Hoang, Vinh Truong",,,,,,,,,limited training data,,,,,,
,,,,,,,,,,,embedding mixup,,,,,,
,,,,,,,,,,,imbalance data,,,,,,
JOUR,A multistage retrieval system for health-related misinformation detection,"Fernández-Pichel, Marcos",Engineering Applications of Artificial Intelligence,115,105211,2022,2022/10/01/,0952-1976,https://doi.org/10.1016/j.engappai.2022.105211,https://www.sciencedirect.com/science/article/pii/S0952197622002950,Engineering applications,"Web search is widely used to find online medical advice. As such, health-related information access requires retrieval algorithms capable of promoting reliable documents and filtering out unreliable ones. To this end, different types of components, such as query-document matching features, passage relevance estimation and AI-based reliability estimators, need to be combined. In this paper, we propose an entire pipeline for misinformation detection, based on the fusion of multiple content-based features. We present experiments which study the influence of each pipeline stage for the target task. Our technological solution incorporates signals from technologies derived from diverse research fields, including search, deep learning for natural language processing, as well as advanced supervised and unsupervised learning. To combine evidence, different score fusion strategies are compared, including unsupervised rank fusion techniques and learning-to-rank methods. The reference framework for empirically validating our solution is the TREC Health Misinformation Track, which provides several challenging subtasks that foster research on the identification of reliable and correct information for health-related decision making tasks. More specifically, we address a total recall task, the goal of which is to identify all the documents conveying incorrect information for a specific set of topics, and an ad-hoc retrieval task, aiming to rank credible and correct information over incorrect information. All variants are evaluated with an assorted set of effectiveness metrics, which includes standard search measures, such as R-Precision, Average Precision or Normalised Discounted Cumulative Gain, and innovative metrics based on the compatibility between the ranked output and two reference rankings composed of helpful and harmful documents, respectively. Our experiments demonstrate the effectiveness of the proposed pipeline stages and indicate that sophisticated supervised fusion methods do not fare better than simpler fusion alternatives. Additionally, for reliability estimation, unsupervised textual similarity performs better than textual classification based on supervised learning. The results also show that the presented approach is highly competitive when compared with state-of-the-art solutions for the same problem.",,,,,
,,"Losada, David E.",,,,,,,,,Web search,,,,,,
,,"Pichel, Juan C.",,,,,,,,,Health misinformation,,,,,,
,,,,,,,,,,,Information retrieval,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,Artificial intelligence,,,,,,
,,,,,,,,,,,Deep learning for natural language processing,,,,,,
JOUR,Enhancing emotion inference in conversations with commonsense knowledge,"Li, Dayu",Knowledge-Based Systems,232,107449,2021,2021/11/28/,0950-7051,https://doi.org/10.1016/j.knosys.2021.107449,https://www.sciencedirect.com/science/article/pii/S0950705121007115,Emotion analysis,"Existing studies on emotion analysis in conversations have mainly focused on recognizing the emotion of a given utterance. This paper investigates the task of emotion inference in conversations, which explores how the utterances affect the addressee’s emotion, without knowing the addressee’s response yet. While it is straightforward for humans to perceive and reason about the feelings of others in conversations, it is a severe challenge for machines, mainly due to the lack of commonsense knowledge. In this work, we propose to leverage external inferential knowledge to enhance the emotion inference in conversations. Specifically, a conversation modeling module is designed to accumulate information from the conversation history based on the emotional interaction between the addressee and writers. In addition, a knowledge integration strategy is also proposed to integrate the conversation-related commonsense knowledge generated from the event-based knowledge graph. The experiments on three different benchmark conversational datasets demonstrate the effectiveness of the proposed models, and prove the benefits of commonsense knowledge for emotion inference in conversations.",,,,,
,,"Zhu, Xiaodan",,,,,,,,,Emotion inference in conversations,,,,,,
,,"Li, Yang",,,,,,,,,Conversation modeling,,,,,,
,,"Wang, Suge",,,,,,,,,Commonsense knowledge integration,,,,,,
,,"Li, Deyu",,,,,,,,,,,,,,,
,,"Liao, Jian",,,,,,,,,,,,,,,
,,"Zheng, Jianxing",,,,,,,,,,,,,,,
JOUR,A novel medical text classification model with Kalman filter for clinical decision making,"Li, Jie",Biomedical Signal Processing and Control,82,104503,2023,2023/04/01/,1746-8094,https://doi.org/10.1016/j.bspc.2022.104503,https://www.sciencedirect.com/science/article/pii/S1746809422009570,Medical text classification,"Medical text classification has long been a challenging issue in the construction of clinical decision support systems because medical texts contain medical terminologies and various medical abbreviations. This paper proposes an ALBERT-based fusion Kalman-filter model, named AFKF, to address word-level and sentence-level noises in electronic medical records. Specifically, a sliding window scheme is explored to deal with the coupling relationship among large sequences. Furthermore, we design a fusion block on the basis of Kalman filter to integrate representations of multiple segment sequences. Experiments indicate that our approach significantly outperforms the baseline methods with real-world medical texts. Our fusion strategy also improves the performance of other feature classification models in the medical text classification task.",,,,,
,,"Huang, Qilin",,,,,,,,,Clinical decision support systems,,,,,,
,,"Ren, Siyu",,,,,,,,,Sling window,,,,,,
,,"Jiang, Li",,,,,,,,,Kalman filter,,,,,,
,,"Deng, Bo",,,,,,,,,,,,,,,
,,"Qin, Yi",,,,,,,,,,,,,,,
JOUR,FNReq-Net: A hybrid computational framework for functional and non-functional requirements classification,"Saleem, Summra",Journal of King Saud University - Computer and Information Sciences,35,101665,2023,2023/09/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101665,https://www.sciencedirect.com/science/article/pii/S1319157823002197,Software development,"Requirements classification is a key component of software development life cycle. It enhances our understanding about project requirements, which in turn enables us to effectively identify and mitigate risks that could lead to project failure. Existing requirements classification predictors do not utilize feature selection methods competence in their predictive pipelines and lack in performance. To empower the process of automatic requirements classification, contributions of this paper are manifold. Firstly, it explores the potential of 7 filter-based feature selection techniques and 11 traditional machine learning classifiers. Secondly, for the first time it investigates combined potential of traditional feature selection and 9 diverse types of deep learning predictors. Thirdly, it presents a hybrid computational predictor namely FNReq-Net that reaps combine benefits of traditional feature selection and a novel deep learning predictor based on attention mechanism. Over two public benchmark datasets, large-scale experimental results reveal feature selection not only improves predictive performance of traditional machine learning predictors, but it also improves performance of deep learning predictors. The proposed FNReq-Net predictor outperforms state-of-the-art functional and non-functional requirements classification predictors by 4% and 1% in terms of F1-score over Promise and Promise-exp datasets, respectively.",8,,,,
,,"Asim, Muhammad Nabeel",,,,,,,,,Functional & non-functional requirements,,,,,,
,,"Elst, Ludger Van",,,,,,,,,Feature selection,,,,,,
,,"Dengel, Andreas",,,,,,,,,Feature pruning,,,,,,
,,,,,,,,,,,Attention mechanism,,,,,,
,,,,,,,,,,,Hybrid predictor,,,,,,
JOUR,gDART: Improving rumor verification in social media with Discrete Attention Representations,"Roy, Saswata",Information Processing & Management,59,102927,2022,2022/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.102927,https://www.sciencedirect.com/science/article/pii/S0306457322000516,Rumor verification,"Due to the harmful impact of fabricated information on social media, many rumor verification techniques have been introduced in recent years. Advanced techniques like multi-task learning (MTL), shared-private models suffer from many strategic limitations that restrict their capability of veracity identification on social media. These models are often reliant on multiple tasks for the primary targeted objective. Even the most recent deep neural network (DNN) models like VRoC, Hierarchical-PSV, StA-HiTPLAN etc. based on VAE, GCN, Transformer respectively with improved modification are able to perform good on veracity identification task but with the help of additional auxiliary information, mostly. However, their rise is still not substantial with respect to the proposed model even though the proposed model is not using any additional information. To come up with an improved DNN model architecture, we introduce globally Discrete Attention Representations from Transformers (gDART). Discrete-Attention mechanism in gDART is capable of capturing multifarious correlations veiled among the sequence of words which existing DNN models including Transformer often overlook. Our proposed framework uses a Branch-CoRR Attention Network to extract highly informative features in branches, and employs Feature Fusion Network Component to identify deep embedded features and use them to make enhanced identification of veracity of an unverified claim. Moreover, to achieve its goal, gDART is not dependent on any costly auxiliary resource but on an unsupervised learning process. Extensive experiments reveal that gDART marks a considerable performance gain in veracity identification task over state-of-the-art models on two real world rumor datasets. gDART reports a gain of 36.76%, 40.85% on standard benchmark metrics.",3,,,,
,,"Bhanu, Manish",,,,,,,,,Transformer,,,,,,
,,"Saxena, Shruti",,,,,,,,,Branch-CoRR network,,,,,,
,,"Dandapat, Sourav",,,,,,,,,Discrete-Attention,,,,,,
,,"Chandra, Joydeep",,,,,,,,,Unsupervised loss,,,,,,
,,,,,,,,,,,Correlations,,,,,,
JOUR,Extractive social media text summarization based on MFMMR-BertSum,"Fan, Junqing",Array,20,100322,2023,2023/12/01/,2590-0056,https://doi.org/10.1016/j.array.2023.100322,https://www.sciencedirect.com/science/article/pii/S2590005623000474,Natural language processing,"The advancement of computer technology has led to an overwhelming amount of textual information, hindering the efficiency of knowledge intake. To address this issue, various text summarization techniques have been developed, including statistics, graph sorting, machine learning, and deep learning. However, the rich semantic features of text often interfere with the abstract effects and lack effective processing of redundant information. In this paper, we propose the Multi-Features Maximal Marginal Relevance BERT (MFMMR-BertSum) model for Extractive Summarization, which utilizes the pre-trained model BERT to tackle the text summarization task. The model incorporates a classification layer for extractive summarization. Additionally, the Maximal Marginal Relevance (MMR) component is utilized to remove information redundancy and optimize the summary results. The proposed method outperforms other sentence-level extractive summarization baseline methods on the CNN/DailyMail dataset, thus verifying its effectiveness.",,,,,
,,"Tian, Xiaorong",,,,,,,,,Abstractive text summarization,,,,,,
,,"Lv, Chengyao",,,,,,,,,BERT,,,,,,
,,"Zhang, Simin",,,,,,,,,Machine learning,,,,,,
,,"Wang, Yuewei",,,,,,,,,,,,,,,
,,"Zhang, Junfeng",,,,,,,,,,,,,,,
JOUR,Systematic review of data-centric approaches in artificial intelligence and machine learning,"Singh, Prerna",Data Science and Management,6,144,2023,2023/09/01/,2666-7649,https://doi.org/10.1016/j.dsm.2023.06.001,https://www.sciencedirect.com/science/article/pii/S2666764923000279,Data-centric,"Artificial intelligence (AI) relies on data and algorithms. State-of-the-art (SOTA) AI smart algorithms have been developed to improve the performance of AI-oriented structures. However, model-centric approaches are limited by the absence of high-quality data. Data-centric AI is an emerging approach for solving machine learning (ML) problems. It is a collection of various data manipulation techniques that allow ML practitioners to systematically improve the quality of the data used in an ML pipeline. However, data-centric AI approaches are not well documented. Researchers have conducted various experiments without a clear set of guidelines. This survey highlights six major data-centric AI aspects that researchers are already using to intentionally or unintentionally improve the quality of AI systems. These include big data quality assessment, data preprocessing, transfer learning, semi-supervised learning, machine ​learning ​operations (MLOps), and the effect of adding more data. In addition, it highlights recent data-centric techniques adopted by ML practitioners. We addressed how adding data might harm datasets and how HoloClean can be used to restore and clean them. Finally, we discuss the causes of technical debt in AI. Technical debt builds up when software design and implementation decisions run into “or outright collide with” business goals and timelines. This survey lays the groundwork for future data-centric AI discussions by summarizing various data-centric approaches.",3,157,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Semi-supervised learning,,,,,,
,,,,,,,,,,,Data preprocessing,,,,,,
,,,,,,,,,,,MLOps,,,,,,
,,,,,,,,,,,Data management,,,,,,
,,,,,,,,,,,Technical debt,,,,,,
JOUR,ReCoMIF: Reading comprehension based multi-source information fusion network for Chinese spoken language understanding,"Xie, Bo",Information Fusion,96,192,2023,2023/08/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.03.016,https://www.sciencedirect.com/science/article/pii/S1566253523001057,Chinese spoken language understanding,"Spoken language understanding (SLU) plays a crucial role in the performance of dialogue systems. It usually includes slot filling and intent detection (SFID) tasks aiming at semantic parsing of utterances. At present, researchers focus mainly on English SLU tasks, while such investigations on Chinese utterances are not sufficient. In this paper, we first propose a reading comprehension based multi-source information fusion network, called ReCoMIF for Chinese SFID tasks by transforming the SLU task into a multi-turn question answering procedure comprising multiple choice for intent detection and span extraction for slot filling. Moreover, we present a TCM_CLS module with a concise architecture composed of TextCNN, MaxPooling, and feed forward network plus the [CLS] representation. Such three TCM_CLS modules are stacked in the proposed ReCoMIF network that can serve as sufficient integration of multi-source information originating from contexts, reading comprehension based queries, and hidden representations concerning intent and slot semantics. Finally, experimental results and ablation studies on three Chinese SLU datasets show that our proposed model can effectively fuse intent and slot information achieved by\ state-of-the-art performance compared with other baseline methods.",,201,,,
,,"Jia, Xiaohui",,,,,,,,,Intent detection,,,,,,
,,"Song, Xiawen",,,,,,,,,Slot filling,,,,,,
,,"Zhang, Hua",,,,,,,,,Machine reading comprehension,,,,,,
,,"Chen, Bi",,,,,,,,,Multi-source information fusion,,,,,,
,,"Jiang, Bo",,,,,,,,,,,,,,,
,,"Wang, Ye",,,,,,,,,,,,,,,
,,"Pan, Yun",,,,,,,,,,,,,,,
JOUR,TCAMixer: A lightweight Mixer based on a novel triple concepts attention mechanism for NLP,"Liu, Xiaoyan",Engineering Applications of Artificial Intelligence,123,106471,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106471,https://www.sciencedirect.com/science/article/pii/S0952197623006553,Textual feature representation,"Large-scale model sizes and expensive computing costs cause the challenge of deploying and applying large pre-trained models. Hence, this paper presents a novel Triple Concepts Attention Mechanism and a lightweight TCAMixer model for edge devices to classify texts. Furthermore, the TCAMixer abstracts textual concepts in a human way, which is unmatched by other counterparts such as pNLP-Mixer (a projection-based MLP-Mixer model for Nature Language Processing) and HyperMixer (a hyper network using dynamic token-mixing layers). Experimental results on several public datasets demonstrate that the TCAMixer outperforms the counterparts by a significant margin, for example, achieving 3% higher accuracy with a smaller model size of 0.177M. Additionally, the TCAMixer achieves a performance of 85% to 98.7% compared to that of large pre-trained models but only occupies 1/3000 to 1/2000 of their size on most test datasets.",,,,,
,,"Tang, Huanling",,,,,,,,,Attention mechanism,,,,,,
,,"Zhao, Jie",,,,,,,,,Deep learning,,,,,,
,,"Dou, Quansheng",,,,,,,,,Modeling lighter,,,,,,
,,"Lu, Mingyu",,,,,,,,,,,,,,,
JOUR,KPT++: Refined knowledgeable prompt tuning for few-shot text classification,"Ni, Shiwen",Knowledge-Based Systems,274,110647,2023,2023/08/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110647,https://www.sciencedirect.com/science/article/pii/S0950705123003970,Natural language processing,"Recently, the new paradigm “pre-train, prompt, and predict” has achieved remarkable few-shot learning achievements compared with the “pre-train, fine-tune” paradigm. Prompt-tuning inserts the prompt text into the input and converts the classification task into a masked language modeling task. One of the key steps is to build a projection between the labels and the label words, i.e., the verbalizer. Knowledgeable prompt-tuning (KPT), which integrates external knowledge into the verbalizer to improve and stabilize prompt-tuning. KPT uses word embeddings and various knowledge graphs to expand the label words space to hundreds of words per class. However, some unreasonable label words in the verbalizer may damage the accuracy. In this paper, a new method called KPT++ is proposed to improve the few-shot text classification. KPT++ is refined knowledgeable prompt-tuning, which can also be regarded as an upgraded version of KPT. Specifically, KPT++ uses two newly proposed prompt grammar refinement (PGR) and probability distribution refinement (PDR) to refine the knowledgeable verbalizer. Extensive experiments on few-shot text classification tasks demonstrate that our KPT++ outperforms state-of-the-art method KPT and other baseline methods. Furthermore, ablation experiments and case studies demonstrate the effectiveness of both PGR and PDR refining methods.",,,,,
,,"Kao, Hung-Yu",,,,,,,,,Prompt tuning,,,,,,
,,,,,,,,,,,Few-shot learning,,,,,,
,,,,,,,,,,,Text classification,,,,,,
JOUR,Cyberbullying detection for low-resource languages and dialects: Review of the state of the art,"Mahmud, Tanjim",Information Processing & Management,60,103454,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103454,https://www.sciencedirect.com/science/article/pii/S0306457323001917,Automatic cyberbullying detection,"The struggle of social media platforms to moderate content in a timely manner, encourages users to abuse such platforms to spread vulgar or abusive language, which, when performed repeatedly becomes cyberbullying — a social problem taking place in virtual environments, yet with real-world consequences, such as depression, withdrawal, or even suicide attempts of its victims. Systems for the automatic detection and mitigation of cyberbullying have been developed but, unfortunately, the vast majority of them are for the English language, with only a handful available for low-resource languages. To estimate the present state of research and recognize the needs for further development, in this paper we present a comprehensive systematic survey of studies done so far for automatic cyberbullying detection in low-resource languages. We analyzed all studies on this topic that were available.We investigated more than seventy published studies on automatic detection of cyberbullying or related language in low-resource languages and dialects that were published between around 2017 and January 2023. There are 23 low-resource languages and dialects covered by this paper, including Bangla, Hindi, Dravidian languages and others. In the survey, we identify some of the research gaps of previous studies, which include the lack of reliable definitions of cyberbullying and its relevant subcategories, biases in the acquisition, and annotation of data. Based on recognizing those research gaps, we provide some suggestions for improving the general research conduct in cyberbullying detection, with a primary focus on low-resource languages. Based on those proposed suggestions, we collect and release a cyberbullying dataset in the Chittagonian dialect of Bangla and propose a number of initial ML solutions trained on that dataset. In addition, pre-trained transformer-based the BanglaBERT model was also attempted. We conclude with additional discussions on ethical issues regarding such studies, highlight how our survey improves on similar surveys done in the past, and discuss the usefulness of recently popular AI-enhanced tools for streamlining such scientific surveys.",5,,,,
,,"Ptaszynski, Michal",,,,,,,,,Low-resource language,,,,,,
,,"Eronen, Juuso",,,,,,,,,Machine learning,,,,,,
,,"Masui, Fumito",,,,,,,,,Social media,,,,,,
JOUR,FooDis: A food-disease relation mining pipeline,"Cenikj, Gjorgjina",Artificial Intelligence in Medicine,142,102586,2023,2023/08/01/,0933-3657,https://doi.org/10.1016/j.artmed.2023.102586,https://www.sciencedirect.com/science/article/pii/S0933365723001008,Text mining,"Nowadays, it is really important and crucial to follow the new biomedical knowledge that is presented in scientific literature. To this end, Information Extraction pipelines can help to automatically extract meaningful relations from textual data that further require additional checks by domain experts. In the last two decades, a lot of work has been performed for extracting relations between phenotype and health concepts, however, the relations with food entities which are one of the most important environmental concepts have never been explored. In this study, we propose FooDis, a novel Information Extraction pipeline that employs state-of-the-art approaches in Natural Language Processing to mine abstracts of biomedical scientific papers and automatically suggests potential cause or treat relations between food and disease entities in different existing semantic resources. A comparison with already known relations indicates that the relations predicted by our pipeline match for 90% of the food-disease pairs that are common in our results and the NutriChem database, and 93% of the common pairs in the DietRx platform. The comparison also shows that the FooDis pipeline can suggest relations with high precision. The FooDis pipeline can be further used to dynamically discover new relations between food and diseases that should be checked by domain experts and further used to populate some of the existing resources used by NutriChem and DietRx.",,,,,
,,"Eftimov, Tome",,,,,,,,,Relation extraction,,,,,,
,,"Koroušić Seljak, Barbara",,,,,,,,,Named entity recognition,,,,,,
,,,,,,,,,,,Named entity linking,,,,,,
,,,,,,,,,,,Food-disease relations,,,,,,
JOUR,Beyond fear and anger: A global analysis of emotional response to Covid-19 news on Twitter,"Oliveira, Francisco Bráulio",Online Social Networks and Media,36,100253,2023,2023/07/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100253,https://www.sciencedirect.com/science/article/pii/S2468696423000125,Covid-19,"The media has been used to disseminate public information amid the Covid-19 pandemic. However, Covid-19 news has triggered emotional responses in people that have impacted their mental well-being and led to news avoidance. To understand the emotional response to Covid-19 news, we studied user comments on news published on Twitter by 37 media outlets in 11 countries from January 2020 to December 2022. We employed a deep-learning-based model to identify the basic human emotions defined by Ekman in comments related to Covid-19 news. Additionally, we implemented Latent Dirichlet Allocation (LDA) to identify the news topics. Our analysis found that while nearly half of the user comments showed no significant emotions, negative emotions were more common. Anger was the most prevalent emotion, particularly in the media and comments regarding political responses and governmental actions in the United States. On the other hand, joy was mainly linked to media outlets from the Philippines and news about vaccination. Over time, anger consistently remained the most prevalent emotion, with fear being most prevalent at the start of the pandemic but decreasing over time, occasionally spiking with news on Covid-19 variants, cases, and deaths. Emotions also varied across media outlets, with Fox News being associated with the highest level of disgust, the second-highest level of anger, and the lowest level of fear. Sadness was highest at Citizen TV, SABC, and Nation Africa, all three African media outlets. Additionally, fear was most evident in the comments on news from The Times of India.",,,,,
,,"Mougouei, Davoud",,,,,,,,,News,,,,,,
,,"Haque, Amanul",,,,,,,,,Emotion,,,,,,
,,"Sichman, Jaime Simão",,,,,,,,,Twitter,,,,,,
,,"Dam, Hoa Khanh",,,,,,,,,Media,,,,,,
,,"Evans, Simon",,,,,,,,,Topic modeling,,,,,,
,,"Ghose, Aditya",,,,,,,,,Deep learning,,,,,,
,,"Singh, Munindar P.",,,,,,,,,,,,,,,
JOUR,A deep learning approach for detecting fake reviewers: Exploiting reviewing behavior and textual information,"Zhang, Dong",Decision Support Systems,166,113911,2023,2023/03/01/,0167-9236,https://doi.org/10.1016/j.dss.2022.113911,https://www.sciencedirect.com/science/article/pii/S0167923622001828,Fake reviewer detection,"Ensuring the credibility of online consumer reviews (OCRs) is a growing societal concern. However, the problem of fake reviewers on online platforms significantly influences e-commerce authenticity and consumer trust. Existing studies for fake reviewer detection mainly focus on deriving novel behavioral and linguistic features. These features require extensive human labor and expertise, placing a heavy burden on platforms. Therefore, we propose a novel end-to-end framework to detect fake reviewers based on behavior and textual information. It has two key components: (1) a behavior-sensitive feature extractor that learns the underlying patterns of reviewing behavior; (2) a context-aware attention mechanism that extracts valuable features from online reviews. We rigorously evaluate each proposed module and the entire framework against state-of-the-art benchmarks on two real-world datasets from http://Yelp.com. Experimental results demonstrate that our method achieves state-of-the-art results on fake reviewer detection. Our method can be considered a tentative step toward lowering human labor costs in realizing automated fake reviewer detection on e-commerce platforms.",,,,,
,,"Li, Wenwen",,,,,,,,,Deep learning,,,,,,
,,"Niu, Baozhuang",,,,,,,,,Behavioral feature,,,,,,
,,"Wu, Chong",,,,,,,,,Textual feature,,,,,,
,,,,,,,,,,,Contextualized text representation,,,,,,
JOUR,Transformer-based identification of stochastic information cascades in social networks using text and image similarity,"Kasnesis, Panagiotis",Applied Soft Computing,108,107413,2021,2021/09/01/,1568-4946,https://doi.org/10.1016/j.asoc.2021.107413,https://www.sciencedirect.com/science/article/pii/S1568494621003367,Information cascade,"Identifying the origin of information posted on social media and how this may have changed over time can be very helpful to users in determining whether they trust it or not. This currently requires disproportionate effort for the average social media user, who instead has to rely on fact-checkers or other intermediaries to identify information provenance for them. We show that it is possible to disintermediate this process by providing an automated mechanism for determining the information cascade where a post belongs. We employ a transformer-based language model as well as pretrained ResNet50 model for image similarity, to decide whether two posts are sufficiently similar to belong to the same cascade. By using semantic similarity, as well as image in addition to text, we increase accuracy where there is no explicit diffusion of reshares. In a new dataset of 1,200 news items on Twitter, our approach is able to increase clustering performance above 7% and 4.5% for the validation and test sets respectively over the previous state of the art. Moreover, we employ a probabilistic subsampling mechanism, reducing significantly cascade creation time without affecting the performance of large-scale semantic text analysis and the quality of information cascade generation. We have implemented a prototype that offers this new functionality to the user and have deployed it in our own instance of social media platform Mastodon.",,,,,
,,"Heartfield, Ryan",,,,,,,,,Semantic textual similarity,,,,,,
,,"Liang, Xing",,,,,,,,,Image similarity,,,,,,
,,"Toumanidis, Lazaros",,,,,,,,,Deep learning,,,,,,
,,"Sakellari, Georgia",,,,,,,,,,,,,,,
,,"Patrikakis, Charalampos",,,,,,,,,,,,,,,
,,"Loukas, George",,,,,,,,,,,,,,,
JOUR,"An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools","Lauriola, Ivano",Neurocomputing,470,443,2022,2022/01/22/,0925-2312,https://doi.org/10.1016/j.neucom.2021.05.103,https://www.sciencedirect.com/science/article/pii/S0925231221010997,Deep Learning,"Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions.",,456,,,
,,"Lavelli, Alberto",,,,,,,,,Natural Language Processing,,,,,,
,,"Aiolli, Fabio",,,,,,,,,Transformer,,,,,,
,,,,,,,,,,,Language Models,,,,,,
,,,,,,,,,,,Software,,,,,,
JOUR,icsBERTs: Optimizing Pre-trained Language Models in Intelligent Customer Service,"Liu, Shixuan",Procedia Computer Science,222,127,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2023.08.150,https://www.sciencedirect.com/science/article/pii/S1877050923009158,pre-trained language models,"Automatic processing of textual information is a growing application area in intelligent customer service platforms due to the large number of customer requests constantly provided in the form of text. Many pre-trained language models have shown their high performance on text processing tasks. However, these pre-training strategies do not leverage domain-specific information. In this paper, we propose icsBERTs optimized for intelligent customer service on both word and sentence levels. Specifically, the automatic business words extraction and multi-task training methods are designed to enhance language representations for our models. Furthermore, we also propose the use of adaptors based on these icsBERTs for more complicated tasks. Our models outperform mainstream pre-trained language models on several tasks in the field of intelligent customer service and these experimental results demonstrate that using targeted strategies can further improve the performance of pre-trained language models in this field.",,136,International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023),,
,,"Peng, Chen",,,,,,,,,intent classification,,,,,,
,,"Wang, Chao",,,,,,,,,query matching,,,,,,
,,"Chen, Xiangyan",,,,,,,,,,,,,,,
,,"Song, Shuangyong",,,,,,,,,,,,,,,
JOUR,Combining FastText and Glove Word Embedding for Offensive and Hate speech Text Detection,"Badri, Nabil",Procedia Computer Science,207,769,2022,2022/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2022.09.132,https://www.sciencedirect.com/science/article/pii/S1877050922010134,Inappropriate content classification,"Over the past decade, increased use of social media has led to an increase in hate content. To address this issue new solutions must be implemented to filter out this kind of inappropriate content. Because manual filtering is difficult, several studies have been conducted in order to automate the process. This paper introduces a method based on a combination of Glove and FastText word embedding as input features and a BiGRU model to identify hate speech from social media websites. The obtained results show that our proposed model (BiGRU Glove FT) is effective in detecting inappropriate content. This model detect hate speech on OLID dataset, using an effective learning process that classifies the text into offensive and not offensive language. The performance of the system attained 84%, 87%, 93%, 90% accuracy, precision, recall, and f1-score respectively.",,778,Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022,,
,,"Kboubi, Ferihane",,,,,,,,,Machine,,,,,,
,,"Chaibi, Anja Habacha",,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Neural networks,,,,,,
,,,,,,,,,,,NLP,,,,,,
,,,,,,,,,,,Glove,,,,,,
,,,,,,,,,,,FastText word embeddings,,,,,,
JOUR,AED: An black-box NLP classifier model attacker,"Liu, Yueyang",Neurocomputing,550,126489,2023,2023/09/14/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126489,https://www.sciencedirect.com/science/article/pii/S0925231223006124,Black-box NLP attack,"Deep Neural Networks (DNNs) have been successful in solving real-world tasks in domains such as connected and automated vehicles, disease, and job hiring. However, their implications are far-reaching in critical application areas. Hence, there is a growing concern regarding the potential bias and robustness of these DNN models. A transparency and robust model is always demanded in high-stakes domains where reliability and safety are enforced, such as healthcare and finance. While most studies have focused on adversarial image attack scenarios, fewer studies have investigated the robustness of DNN models in natural language processing (NLP) due to their adversarial samples are difficult to generate. To address this gap, we propose a word-level NLP classifier attack model called ”AED,” which stands for Attention mechanism enabled post-model Explanation with Density peaks clustering algorithm for synonyms search and substitution. AED aims to test the robustness of NLP DNN models by interpretability their weaknesses and exploring alternative ways to optimize them. By identifying vulnerabilities and providing explanations, AED can help improve the reliability and safety of DNN models in critical application areas such as healthcare and automated transportation. Our experiment results demonstrate that compared with other existing models, AED can effectively generate adversarial examples that can fool the victim model while maintaining the original meaning of the input.",,,,,
,,"Huang, Yan",,,,,,,,,Explainable AI,,,,,,
,,"Cai, Zhipeng",,,,,,,,,Peaks clustering synonyms search,,,,,,
JOUR,A competitive intelligence acquisition framework for mining user perception from user generated content,"Lin, Jie",Applied Soft Computing,147,110764,2023,2023/11/01/,1568-4946,https://doi.org/10.1016/j.asoc.2023.110764,https://www.sciencedirect.com/science/article/pii/S1568494623007822,Competitive intelligence,"Competitive intelligence is an important basis for companies in the process of developing competitive strategies. However, competitive intelligence acquisition methods such as surveys and expert ratings are not able to achieve rapid responses to user perceptions. In addition, the huge amount of unstructured user-generated content makes it more difficult to analyze user perceptions. In this paper, we propose a competitive intelligence mining framework for acquiring user perceptions from user-generated content. The framework covers multiple aspects of competitive intelligence acquisition, mining, analysis, and decision-making. A unified text processing model (WS-TCM) can automatically filter the content irrelevant to competitive intelligence and quickly extract fine-grained user perceptions of competitive attributes from the huge amount of user-generated content. In addition, the quantile-based intelligence mapping method (QB-IM) determines the competitive landscape of an enterprise based on fine-grained user perceptions and provides help for managers’ strategic decisions. In the case study, the method proposed in this paper has significantly improved compared with the baseline model in the stages of competitive intelligence identification, competitive attribute identification, and user perception identification. Especially, the accuracy improvement in identifying user dissatisfaction perception is more obvious. Meanwhile, our model shows strong robustness. In the competitive analysis stage, the results of the framework are consistent with those obtained by market analysts through surveys. This study provides a new approach to competitive intelligence research and fine-grained user perception mining, which improves the efficiency of competitive intelligence acquisition for companies while also improving the reliability, accuracy, and usefulness of competitive intelligence.",,,,,
,,"Jiang, Xiaoyan",,,,,,,,,User-generated content,,,,,,
,,"Li, Qing",,,,,,,,,Text mining,,,,,,
,,"Wang, Chao",,,,,,,,,,,,,,,
JOUR,A robust hybrid machine learning model for Bengali cyber bullying detection in social media,"Akhter, Arnisha",Natural Language Processing Journal,4,100027,2023,2023/09/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100027,https://www.sciencedirect.com/science/article/pii/S2949719123000249,Cyberbully,"Social networking platforms give users countless opportunities to share information, collaborate, and communicate positively. The same platform can be extended to a fabricated and poisonous atmosphere that gives an impersonal, harmful platform for online misuse and assault. Cyberstalking is when someone uses an internet system to ridicule, torment, insult, criticize, slander, and discredit a victim while never seeing them. With the growth of social networks, Facebook has become the online arena for bullying. Since the effects could result in a widespread contagion, it is vital to have models and mechanisms in place for the automatic identification and removal of internet cyberbullying data. This paper presents a robust hybrid ML model for cyberbullying detection in the Bengali language on social media. The Bengalibullying proposal involves an effective text preprocessing to make the Bengali text data into a useful text format, feature extraction using the TfidfVectorizer (TFID) to get the beneficial information of text data and resampling by Instance Hardness Threshold (IHT) procedure to balance the dataset to avoid overfitting or underfitting problems. In our experiment, we used the publicly available Bangla text dataset (44,001 comments) and got the highest performance ever published works on it. The model achieved the most elevated accuracy rate of 98.57% and 98.82% in binary and multilabel classification to detect cyberbullying on social media in the Bengali language. Our best performance findings are more effective than any previous effort in identifying and categorizing bullying in the Bengali language. As a result, we might use our model to correctly classify Bengali bullying in online bullying detection systems, protecting people from being the targets of social bullying.",,,,,
,,"Acharjee, Uzzal Kumar",,,,,,,,,Bengali,,,,,,
,,"Talukder, Md. Alamin",,,,,,,,,Bullying,,,,,,
,,"Islam, Md. Manowarul",,,,,,,,,Machine learning,,,,,,
,,"Uddin, Md Ashraf",,,,,,,,,Harassment,,,,,,
,,,,,,,,,,,Language,,,,,,
,,,,,,,,,,,Social media,,,,,,
JOUR,"Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach","Kocoń, Jan",Information Processing & Management,58,102643,2021,2021/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2021.102643,https://www.sciencedirect.com/science/article/pii/S0306457321001333,Hate speech,"Analysis of subjective texts like offensive content or hate speech is a great challenge, especially regarding annotation process. Most of current annotation procedures are aimed at achieving a high level of agreement in order to generate a high quality reference source. However, the annotation guidelines for subjective content may restrict the annotators’ freedom of decision making. Motivated by a moderate annotation agreement in offensive content datasets, we hypothesize that personalized approaches to offensive content identification should be in place. Thus, we propose two novel perspectives of perception: group-based and individual. Using demographics of annotators as well as embeddings of their previous decisions (annotated texts), we are able to train multimodal models (including transformer-based) adjusted to personal or community profiles. Based on the agreement of individuals and groups, we experimentally showed that annotator group agreeability strongly correlates with offensive content recognition quality. The proposed personalized approaches enabled us to create models adaptable to personal user beliefs rather than to agreed offensiveness understanding. Overall, our individualized approaches to offensive content classification outperform classic data-centric methods that generalize offensiveness perception and it refers to all six tested models. Additionally, we developed requirements for annotation procedures, personalization and content processing to make the solutions human-centered.",5,,,,
,,"Figas, Alicja",,,,,,,,,Offensive content,,,,,,
,,"Gruza, Marcin",,,,,,,,,Human-centered NLP,,,,,,
,,"Puchalska, Daria",,,,,,,,,Multimodal deep learning,,,,,,
,,"Kajdanowicz, Tomasz",,,,,,,,,Personalization,,,,,,
,,"Kazienko, Przemysław",,,,,,,,,Subjective content perception,,,,,,
,,,,,,,,,,,Annotator agreement,,,,,,
JOUR,Cross-modal distillation with audio–text fusion for fine-grained emotion classification using BERT and Wav2vec 2.0,"Kim, Donghwa",Neurocomputing,506,168,2022,2022/09/28/,0925-2312,https://doi.org/10.1016/j.neucom.2022.07.035,https://www.sciencedirect.com/science/article/pii/S0925231222008931,Multi-class emotion classification,"Fine-grained emotion classification for mood- and emotion-related physical-characteristics detection and its application to computer technology using biometric sensors has been extensively researched in the field of affective computing. Although text modality has achieved a considerably high performance from the perspective of sentiment analysis, which simply classifies a positive or negative label, fine-grained emotion classification requires additional information besides text. An audio feature can be adopted as the additional information as it is closely associated with text, and the characteristics of the changes in sound pulses can be employed in fine-grained emotion classification. However, the multimodal datasets related to fine-grained emotion are limited, and the scalability and efficiency are insufficient for multimodal training to be applied extensively via the self-supervised learning (Self-SL) approach, which can adequately represent modality. To address these limitations, we propose cross-modal distillation (CMD), which induces the feature spaces of student models with a few parameters while receiving those of the teacher models that can adequately express each modality based on Self-SL. The proposed CMD performs the mapping of a feature space between teacher-student models based on contrastive learning, while two attention mechanisms—cross-attention between audio and text features and self-attention for features in modality—are performed during knowledge distillation. Wav2vec 2.0 and BERT, which are already adequately trained for audio and text via Self-SL, were adopted as teacher models; audio–text transformer models were used as student models. Accordingly, the CMD-based representation learning applies a lightweight model for IEMOCAP, MELD, and CMU–MOSEI datasets with the task of multi-class emotion classification, while exhibiting better fine-grained emotion classification performance than benchmark models with a considerably low uncertainty for prediction.",,183,,,
,,"Kang, Pilsung",,,,,,,,,Knowledge distillation,,,,,,
,,,,,,,,,,,Transformer,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Wav2Vec 2.0,,,,,,
,,,,,,,,,,,Contrastive learning,,,,,,
JOUR,Multi-Aspect co-Attentional Collaborative Filtering for extreme multi-label text classification,"Wang, Jiyao",Knowledge-Based Systems,260,110110,2023,2023/01/25/,0950-7051,https://doi.org/10.1016/j.knosys.2022.110110,https://www.sciencedirect.com/science/article/pii/S0950705122012060,Extreme multi-label text classification,"The extreme multi-label text classification (XMTC) keeps attracting researchers’ attention due to its wide application. Recent studies have been trying to enhance text representation or reduce the number of labels to optimize the lack of information in a text or the sparsity of the possibility vector. In the social recommendation field, a similar problem has already been defined and studied extensively, the methods from which can be adopted in XMTC to identify matching relations in large datasets accurately. Thus, we proposed a general architecture enhanced by a pre-defined global nearest label neighbor group for XMTC, which reformulates the learning task to an interaction function between document and label by a multi-layer perceptron. Further, with co-attention mechanism and neural collaborative filtering, our architecture learns informative label representation enhanced by document-specific label group vector and label-specific text feature vector, which builds an effective interaction function to get a matching score. Through extensive experiments comparing various models and ablation studies, results show that our proposed architecture outperforms most existing methods for XMTC and significantly improves the performance of elemental document encoders.",,,,,
,,"Chen, Zijie",,,,,,,,,Multi-aspect representation,,,,,,
,,"Qin, Yang",,,,,,,,,Co-attention mechanism,,,,,,
,,"He, Dengbo",,,,,,,,,Neural collaborative filtering,,,,,,
,,"Lin, Fangzhen",,,,,,,,,,,,,,,
JOUR,Utilizing subjectivity level to mitigate identity term bias in toxic comments classification,"Zhao, Zhixue",Online Social Networks and Media,29,100205,2022,2022/05/01/,2468-6964,https://doi.org/10.1016/j.osnem.2022.100205,https://www.sciencedirect.com/science/article/pii/S246869642200009X,Language model,"Toxic comment classification models are often found biased towards identity terms, i.e., terms characterizing a specific group of people such as “Muslim” and “black”. Such bias is commonly reflected in false positive predictions, i.e., non-toxic comments with identity terms. In this work, we propose a novel approach to debias the model in toxic comment classification, leveraging the notion of subjectivity level of a comment and the presence of identity terms. We hypothesize that toxic comments containing identity terms are more likely to be expressions of subjective feelings or opinions. Therefore, the subjectivity level of a comment containing identity terms can be helpful for classifying toxic comments and mitigating the identity term bias. To implement this idea, we propose a model based on BERT and study two different methods of measuring the subjectivity level. The first method uses a lexicon-based tool. The second method is based on the idea of calculating the embedding similarity between a comment and a relevant Wikipedia text of the identity term in the comment. We thoroughly evaluate our method on an extensive collection of four datasets collected from different social media platforms. Our results show that: (1) our models that incorporate both features of subjectivity and identity terms consistently outperform strong SOTA baselines, with our best performing model achieving an improvement in F1 of 4.75% over a Twitter dataset; (2) our idea of measuring subjectivity based on the similarity to the relevant Wikipedia text is very effective on toxic comment classification as our model using this has achieved the best performance on 3 out of 4 datasets while obtaining comparative performance on the remaining dataset. We further test our method on RoBERTa to evaluate the generality of our method and the results show the biggest improvement in F1 of up to 1.29% (on a dataset from a white supremacist online forum).",,,,,
,,"Zhang, Ziqi",,,,,,,,,Transfer learning,,,,,,
,,"Hopfgartner, Frank",,,,,,,,,Hate speech,,,,,,
,,,,,,,,,,,Classification,,,,,,
JOUR,TaxonPrompt: Taxonomy-aware curriculum prompt learning for few-shot event classification,"Song, Chengyu",Knowledge-Based Systems,264,110290,2023,2023/03/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110290,https://www.sciencedirect.com/science/article/pii/S0950705123000400,Event classification,"Event classification (EC) aims to assign the event labels to unlabeled sentences and tends to struggle in real-world applications when only a few annotated samples are available. Previous studies have mainly focused on using meta-learning to overcome the low-resource problem where label data from other tasks are still required for model learning and selection. Accordingly, prompt learning-based approaches are proposed to address the low-resource issue. However, such approaches generally ignore task-specific information and adopt demonstration learning for fine-tuning, which fails to leverage the most informative examples for training and hurts performance. Thus, we propose a taxonomy-aware prompt learning framework TaxonPrompt that trains the language model with samples from easy to hard by imitating the human curricula, which effectively alleviates the classification bottleneck caused by insufficient data. We first design an event prompt generation (EPG) for automatically generating task-specific templates using sentences, labels, and trigger words. Then, we propose a Fisher information-based demonstration filtering (FDF) to dynamically select the most informative support examples for each query to train the model. We have conducted extensive experiments on two EC datasets: FewEvent and RAMS. The experimental results demonstrate the superiority of the proposed model over state-of-the-art baselines. In particular, our approach works well in the scenario of an extremely small number of available task resources and therefore constitutes a solution for few-shot event classification.",,,,,
,,"Cai, Fei",,,,,,,,,Few-shot learning,,,,,,
,,"Wang, Mengru",,,,,,,,,Prompt tuning,,,,,,
,,"Zheng, Jianming",,,,,,,,,Pre-trained language model,,,,,,
,,"Shao, Taihua",,,,,,,,,Template generation,,,,,,
,,,,,,,,,,,Information extraction,,,,,,
JOUR,Session-based cyberbullying detection in social media: A survey,"Yi, Peiling",Online Social Networks and Media,36,100250,2023,2023/07/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100250,https://www.sciencedirect.com/science/article/pii/S2468696423000095,Cyberbullying detection,"Cyberbullying is a pervasive problem in online social media, where a bully abuses a victim through a social media session. By investigating cyberbullying perpetrated through social media sessions, recent research has looked into mining patterns and features for modelling and understanding the two defining characteristics of cyberbullying: repetitive behaviour and power imbalance. In this survey paper, we define a framework that encapsulates four different steps session-based cyberbullying detection should go through, and discuss the multiple challenges that differ from single text-based cyberbullying detection. Based on this framework, we provide a comprehensive overview of session-based cyberbullying detection in social media, delving into existing efforts from a data and methodological perspective. Our review leads us to proposing evidence-based criteria for a set of best practices to create session-based cyberbullying datasets. In addition, we perform benchmark experiments comparing the performance of state-of-the-art session-based cyberbullying detection models as well as large pre-trained language models across two different datasets. Through our review, we also put forth a set of open challenges as future research directions.",,,,,
,,"Zubiaga, Arkaitz",,,,,,,,,Cyberbullying,,,,,,
,,,,,,,,,,,Session-based cyberbullying detection,,,,,,
,,,,,,,,,,,Social media,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
JOUR,TWilBert: Pre-trained deep bidirectional transformers for Spanish Twitter,"González, José Ángel",Neurocomputing,426,58,2021,2021/02/22/,0925-2312,https://doi.org/10.1016/j.neucom.2020.09.078,https://www.sciencedirect.com/science/article/pii/S0925231220316180,Contextualized Embeddings,"In recent years, the Natural Language Processing community have been moving from uncontextualized word embeddings towards contextualized word embeddings. Among these contextualized architectures, BERT stands out due to its capacity to compute bidirectional contextualized word representations. However, its competitive performance in English downstream tasks is not obtained by its multilingual version when it is applied to other languages and domains. This is especially true in the case of the Spanish language used in Twitter. In this work, we propose TWiLBERT, a specialization of BERT architecture both for the Spanish language and the Twitter domain. Furthermore, we propose a Reply Order Prediction signal to learn inter-sentence coherence in Twitter conversations, which improves the performance of TWilBERT in text classification tasks that require reasoning on sequences of tweets. We perform an extensive evaluation of TWilBERT models on 14 different text classification tasks, such as irony detection, sentiment analysis, or emotion detection. The results obtained by TWilBERT outperform the state-of-the-art systems and Multilingual BERT. In addition, we carry out a thorough analysis of the TWilBERT models to study the reasons of their competitive behavior. We release the pre-trained TWilBERT models used in this paper, along with a framework for training, evaluating, and fine-tuning TWilBERT models.",,69,,,
,,"Hurtado, Lluís-F.",,,,,,,,,Spanish,,,,,,
,,"Pla, Ferran",,,,,,,,,Twitter,,,,,,
,,,,,,,,,,,TWilBERT,,,,,,
JOUR,Improving the robustness and accuracy of biomedical language models through adversarial training,"Moradi, Milad",Journal of Biomedical Informatics,132,104114,2022,2022/08/01/,1532-0464,https://doi.org/10.1016/j.jbi.2022.104114,https://www.sciencedirect.com/science/article/pii/S1532046422001307,Biomedical natural language processing,"Deep transformer neural network models have improved the predictive accuracy of intelligent text processing systems in the biomedical domain. They have obtained state-of-the-art performance scores on a wide variety of biomedical and clinical Natural Language Processing (NLP) benchmarks. However, the robustness and reliability of these models has been less explored so far. Neural NLP models can be easily fooled by adversarial samples, i.e. minor changes to input that preserve the meaning and understandability of the text but force the NLP system to make erroneous decisions. This raises serious concerns about the security and trust-worthiness of biomedical NLP systems, especially when they are intended to be deployed in real-world use cases. We investigated the robustness of several transformer neural language models, i.e. BioBERT, SciBERT, BioMed-RoBERTa, and Bio-ClinicalBERT, on a wide range of biomedical and clinical text processing tasks. We implemented various adversarial attack methods to test the NLP systems in different attack scenarios. Experimental results showed that the biomedical NLP models are sensitive to adversarial samples; their performance dropped in average by 21 and 18.9 absolute percent on character-level and word-level adversarial noise, respectively, on Micro-F1, Pearson Correlation, and Accuracy measures. Conducting extensive adversarial training experiments, we fine-tuned the NLP models on a mixture of clean samples and adversarial inputs. Results showed that adversarial training is an effective defense mechanism against adversarial noise; the models’ robustness improved in average by 11.3 absolute percent. In addition, the models’ performance on clean data increased in average by 2.4 absolute percent, demonstrating that adversarial training can boost generalization abilities of biomedical NLP systems. This study takes an important step towards revealing vulnerabilities of deep neural language models in biomedical NLP applications. It also provides practical and effective strategies to develop secure, trust-worthy, and accurate intelligent text processing systems in the biomedical domain.",,,,,
,,"Samwald, Matthias",,,,,,,,,Robustness,,,,,,
,,,,,,,,,,,Adversarial attack,,,,,,
,,,,,,,,,,,Adversarial training,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Biomedical text,,,,,,
JOUR,A benchmark for evaluating Arabic contextualized word embedding models,"Elnagar, Ashraf",Information Processing & Management,60,103452,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103452,https://www.sciencedirect.com/science/article/pii/S0306457323001899,Word contextualized embedding,"Word embeddings, which represent words as numerical vectors in a high-dimensional space, are contextualized by generating a unique vector representation for each sense of a word based on the surrounding words and sentence structure. They are typically generated using such deep learning models as BERT and trained on large amounts of text data and using self-supervised learning techniques. Resulting embeddings are highly effective at capturing the nuances of language, and have been shown to significantly improve the performance of numerous NLP tasks. Word embeddings represent textual records of human thinking, with all the mental relations that we utilize to produce the succession of sentences that make up texts and discourses. Consequently, the distributed representation of words within embeddings ought to capture the reasoning relations that hold texts together. This paper makes its contribution to the field by proposing a benchmark for the assessment of contextualized word embeddings that probes into their capability for true contextualization by inspecting how well they capture resemblance, contrariety, comparability, identity, relations in time and space, causation, analogy, and sense disambiguation. The proposed metrics adopt a triangulation approach, so they use (1) Hume’s reasoning relations, (2) standard analogy, and (3) sense disambiguation. The benchmark has been evaluated against 22 Arabic contextualized embeddings and has proven to be capable of quantifying their differential performance in terms of these reasoning relations. Results of evaluation of the target embeddings revealed that they do take context into account and that they do reasonably well in sense disambiguation but have weakness in their identification of converseness, synonymy, complementarity, and analogy. Results also show that size of an embedding has diminishing returns because the highly frequent language patterns swamp low frequency patterns. Furthermore, the suggest that future research endeavors should not be concerned with the quantity of data as much as its quality, and that it should focus more on the representativeness of data, and on model architecture, design, and training.",5,,,,
,,"Yagi, Sane",,,,,,,,,Metrics,,,,,,
,,"Mansour, Youssef",,,,,,,,,Intrinsic and extrinsic evaluation,,,,,,
,,"Lulu, Leena",,,,,,,,,Transformer,,,,,,
,,"Fareh, Shehdeh",,,,,,,,,BERT,,,,,,
JOUR,CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection,"Tang, Wei",Journal of Systems and Software,199,111623,2023,2023/05/01/,0164-1212,https://doi.org/10.1016/j.jss.2023.111623,https://www.sciencedirect.com/science/article/pii/S0164121223000183,Graph neural networks,"In order to secure software, it is critical to detect potential vulnerabilities. The performance of traditional static vulnerability detection methods is limited by predefined rules, which rely heavily on the expertise of developers. Existing deep learning-based vulnerability detection models usually use only a single sequence or graph embedding approach to extract vulnerability features. Sequence embedding-based models ignore the structured information inherent in the code, and graph embedding-based models lack effective node and graph embedding methods. As a result, we propose a novel deep learning-based approach, CSGVD (Combining Sequence and Graph embedding for Vulnerability Detection), which considers function-level vulnerability detection as a graph binary classification task. Firstly, we propose a PE-BL module, which inherits and enhances the knowledge from the pre-trained language model. It extracts the code’s local semantic features as node embedding in the control flow graph by using sequence embedding. Secondly, CSGVD uses graph neural networks to extract the structured information of the graph. Finally, we propose a mean biaffine attention pooling, M-BFA, to better aggregate node information as a graph’s feature representation. The experimental results show that CSGVD outperforms the existing state-of-the-art models and obtains 64.46% accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection.",,,,,
,,"Tang, Mingwei",,,,,,,,,Vulnerability detection,,,,,,
,,"Ban, Minchao",,,,,,,,,Sequence embedding,,,,,,
,,"Zhao, Ziguo",,,,,,,,,Graph embedding,,,,,,
,,"Feng, Mingjun",,,,,,,,,Pre-trained language model,,,,,,
,,,,,,,,,,,Attention pooling,,,,,,
JOUR,SUNET: Speaker-utterance interaction Graph Neural Network for Emotion Recognition in Conversations,"Song, Rui",Engineering Applications of Artificial Intelligence,123,106315,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106315,https://www.sciencedirect.com/science/article/pii/S0952197623004992,Emotion Recognition in Conversations,"Emotion Recognition in Conversations (ERC) can capture the speakers’ emotional changes in multiple rounds of conversation, so it has a wide range of applications. In recent years, Graph Neural Networks have been naturally used in ERC tasks due to their ability to capture complex non-Euclidian spatial features. However, how to model conversations easily and effectively to improve the effect of ERC in the complex interaction mode still needs to be explored. To this end, we propose a new approach to construct a speaker-utterance interactive heterogeneous network that effectively models context while taking into account the global characteristics of speakers. On this basis, we propose a graph neural network based on the speaker and the corresponding utterances interactions, which dynamically updates the representations of utterances and speakers according to the order in which the speakers talk. We formulate different update methods for utterance and speaker nodes to ensure that the particularity of the heterogeneous network is fully explored. We conduct extensive experiments on four ERC benchmark datasets, and our approach achieves an average 0.7% performance improvement over the most advanced methods, which validates the effectiveness of properly modeling speakers in ERC tasks.",,,,,
,,"Giunchiglia, Fausto",,,,,,,,,Heterogeneous network,,,,,,
,,"Shi, Lida",,,,,,,,,Graph Neural Network,,,,,,
,,"Shen, Qiang",,,,,,,,,,,,,,,
,,"Xu, Hao",,,,,,,,,,,,,,,
JOUR,Are the BERT family zero-shot learners? A study on their potential and limitations,"Wang, Yue",Artificial Intelligence,322,103953,2023,2023/09/01/,0004-3702,https://doi.org/10.1016/j.artint.2023.103953,https://www.sciencedirect.com/science/article/pii/S0004370223000991,Pre-trained language model,"Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised fine-tuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as newly presented pre-training techniques, make the PLMs perform well under the few-shot setting, especially in the recent paradigm of prompt-based learning. After witnessing the success of PLMs for few-shot tasks, we propose to further study the potential and limitations of PLMs for the zero-shot setting. We utilize 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. We are surprised to find that some simple strategies (without the need of human efforts or unsupervised data) can yield very promising results on a few widely-used datasets, e.g., 88.34%(±0.60) accuracy on the IMDB dataset, and 84.88%(±2.83) accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06%(±13.04), 75.54%(±11.77) for comparison. However, we also observe some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks (e.g., GLUE, SuperGLUE).2",,,,,
,,"Wu, Lijun",,,,,,,,,Zero-shot text classification,,,,,,
,,"Li, Juntao",,,,,,,,,Prompt-based learning,,,,,,
,,"Liang, Xiaobo",,,,,,,,,,,,,,,
,,"Zhang, Min",,,,,,,,,,,,,,,
JOUR,Knowledge-based BERT word embedding fine-tuning for emotion recognition,"Zhu, Zixiao",Neurocomputing,552,126488,2023,2023/10/01/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126488,https://www.sciencedirect.com/science/article/pii/S0925231223006112,Emotion recognition,"Emotion recognition has received considerable attention in recent years, with the popularity of social media. It is noted, however, that the state-of-the-art language models such as Bidirectional Encoder Representations from Transformers (BERT) may not produce the best performance in emotion recognition. We found the main cause of the problem is that the embedding of emotional words from the pre-trained BERT model may not exhibit high between-class difference and within-class similarity. While BERT model fine-tuning is a common practice when it is applied to specific tasks, this may not be practical in emotion recognition because most datasets are small and some texts are short and noisy, without containing much useful contextual information. In this paper, we propose to use the knowledge of emotion vocabulary to fine-tune embedding of emotional words. As a separate module independent of the embedding learning model, the fine-tuning model aims to produce emotional word embedding with improved within-class similarity and between-class difference. By combining the emotionally discriminative fine-tuned embedding with contextual information-rich embedding from pre-trained BERT model, the emotional features underlying the texts could be more effectively captured in the subsequent feature learning module, which in turn leads to improved emotion recognition performance. The knowledge-based word embedding fine-tuning model is tested on five datasets of emotion recognition, and the results and analysis demonstrate the effectiveness of the proposed method.",,,,,
,,"Mao, Kezhi",,,,,,,,,Word embedding fine-tuning,,,,,,
,,,,,,,,,,,BERT,,,,,,
JOUR,A benchmark study of machine learning models for online fake news detection,"Khan, Junaed Younus",Machine Learning with Applications,4,100032,2021,2021/06/15/,2666-8270,https://doi.org/10.1016/j.mlwa.2021.100032,https://www.sciencedirect.com/science/article/pii/S266682702100013X,Fake news,"The proliferation of fake news and its propagation on social media has become a major concern due to its ability to create devastating impacts. Different machine learning approaches have been suggested to detect fake news. However, most of those focused on a specific type of news (such as political) which leads us to the question of dataset-bias of the models used. In this research, we conducted a benchmark study to assess the performance of different applicable machine learning approaches on three different datasets where we accumulated the largest and most diversified one. We explored a number of advanced pre-trained language models for fake news detection along with the traditional and deep learning ones and compared their performances from different aspects for the first time to the best of our knowledge. We find that BERT and similar pre-trained models perform the best for fake news detection, especially with very small dataset. Hence, these models are significantly better option for languages with limited electronic contents, i.e., training data. We also carried out several analysis based on the models’ performance, article’s topic, article’s length, and discussed different lessons learned from them. We believe that this benchmark study will help the research community to explore further and news sites/blogs to select the most appropriate fake news detection method.",,,,,
,,"Khondaker, Md. Tawkat Islam",,,,,,,,,Fake news detection,,,,,,
,,"Afroz, Sadia",,,,,,,,,Benchmark study,,,,,,
,,"Uddin, Gias",,,,,,,,,Machine learning,,,,,,
,,"Iqbal, Anindya",,,,,,,,,Neural network,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Natural language processing,,,,,,
JOUR,Enhancing evidence-based medicine with natural language argumentative analysis of clinical trials,"Mayer, Tobias",Artificial Intelligence in Medicine,118,102098,2021,2021/08/01/,0933-3657,https://doi.org/10.1016/j.artmed.2021.102098,https://www.sciencedirect.com/science/article/pii/S0933365721000919,Argument mining,"In the latest years, the healthcare domain has seen an increasing interest in the definition of intelligent systems to support clinicians in their everyday tasks and activities. Among others, also the field of Evidence-Based Medicine is impacted by this twist, with the aim to combine the reasoning frameworks proposed thus far in the field with mining algorithms to extract structured information from clinical trials, clinical guidelines, and Electronic Health Records. In this paper, we go beyond the state of the art by proposing a new end-to-end pipeline to address argumentative outcome analysis on clinical trials. More precisely, our pipeline is composed of (i) an Argument Mining module to extract and classify argumentative components (i.e., evidence and claims of the trial) and their relations (i.e., support, attack), and (ii) an outcome analysis module to identify and classify the effects (i.e., improved, increased, decreased, no difference, no occurrence) of an intervention on the outcome of the trial, based on PICO elements. We annotated a dataset composed of more than 500 abstracts of Randomized Controlled Trials (RCT) from the MEDLINE database, leading to a labeled dataset with 4198 argument components, 2601 argument relations, and 3351 outcomes on five different diseases (i.e., neoplasm, glaucoma, hepatitis, diabetes, hypertension). We experiment with deep bidirectional transformers in combination with different neural architectures (i.e., LSTM, GRU and CRF) and obtain a macro F1-score of.87 for component detection and.68 for relation prediction, outperforming current state-of-the-art end-to-end Argument Mining systems, and a macro F1-score of.80 for outcome classification.",,,,,
,,"Marro, Santiago",,,,,,,,,Randomized controlled trials,,,,,,
,,"Cabrio, Elena",,,,,,,,,Biomedical natural language processing,,,,,,
,,"Villata, Serena",,,,,,,,,Information extraction,,,,,,
,,,,,,,,,,,Clinical decision support systems,,,,,,
,,,,,,,,,,,PICO analysis,,,,,,
TY,T1,AU,JO,VL,SP,PY,DA,SN,DO,UR,KW,AB,IS,EP,T2,,
JOUR,Quantifying and alleviating political bias in language models,"Liu, Ruibo",Artificial Intelligence,304,103654,2022,2022/03/01/,0004-3702,https://doi.org/10.1016/j.artint.2021.103654,https://www.sciencedirect.com/science/article/pii/S0004370221002058,Bias in language models,"Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in real-world settings. In this paper, we first describe metrics for measuring political bias in GPT-2 generation, and discuss several interesting takeaways: 1) The generation of vanilla GPT-2 model is mostly liberal-leaning, 2) Such political bias depends on the sensitive attributes mentioned in the context, and 3) Priming the generation with a explicit political identifier, the extent of political bias is imbalanced (between liberal and conservative). We then propose a reinforcement learning (RL) framework for mitigating such political biases in generated text: By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence.",,,,,
,,"Jia, Chenyan",,,,,,,,,Natural language generation,,,,,,
,,"Wei, Jason",,,,,,,,,Political bias,,,,,,
,,"Xu, Guangxuan",,,,,,,,,Measuring bias,,,,,,
,,"Vosoughi, Soroush",,,,,,,,,Mitigating bias,,,,,,
JOUR,ChatAgri: Exploring potentials of ChatGPT on cross-linguistic agricultural text classification,"Zhao, Biao",Neurocomputing,557,126708,2023,2023/11/07/,0925-2312,https://doi.org/10.1016/j.neucom.2023.126708,https://www.sciencedirect.com/science/article/pii/S0925231223008317,Agricultural text classification,"In the era of sustainable smart agriculture, a vast amount of agricultural news text is posted online, accumulating significant agricultural knowledge. To efficiently access this knowledge, effective text classification techniques are urgently needed. Deep learning approaches, such as fine-tuning strategies on pre-trained language models (PLMs), have shown remarkable performance gains. Nonetheless, these methods face several complex challenges, including limited agricultural training data, poor domain transferability (especially across languages), and complex and expensive deployment of large models. Inspired by the success of recent ChatGPT models (e.g., GPT-3.5, GPT-4), this work explores the potential of applying ChatGPT in the field of agricultural informatization. Various crucial factors, such as prompt construction, answer parsing, and different ChatGPT variants, are thoroughly investigated to maximize its capabilities. A preliminary comparative study is conducted, comparing ChatGPT with PLMs-based fine-tuning methods and PLMs-based prompt-tuning methods. Empirical results demonstrate that ChatGPT effectively addresses the mentioned research challenges and bottlenecks, making it an ideal solution for agricultural text classification. Moreover, ChatGPT achieves comparable performance to existing PLM-based fine-tuning methods, even without fine-tuning on agricultural data samples. We hope this preliminary study could inspire the emergence of a general-purpose AI paradigm for agricultural text processing.",,,,,
,,"Jin, Weiqiang",,,,,,,,,Very large pre-trained language model,,,,,,
,,"Del Ser, Javier",,,,,,,,,Generative Pre-trained Transformer (GPT),,,,,,
,,"Yang, Guang",,,,,,,,,ChatGPT,,,,,,
,,,,,,,,,,,GPT-4,,,,,,
JOUR,Towards automatic conceptual metaphor detection for psychological tasks,"Panicheva, Polina V.",Information Processing & Management,60,103191,2023,2023/03/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103191,https://www.sciencedirect.com/science/article/pii/S0306457322002928,Metaphor detection,"Conceptual metaphor detection is a well-researched topic in Natural Language Processing. At the same time, conceptual metaphor use analysis produces unique insight into individual psychological processes and characteristics, as demonstrated by research in cognitive psychology. Despite the fact that state-of-the-art language models allow for highly effective automatic detection of conceptual metaphor in benchmark datasets, the models have never been applied to psychological tasks. The benchmark datasets differ a lot from experimental texts recorded or produced in a psychological setting, in their domain, genre, and the scope of metaphoric expressions covered. We present the first experiment to apply NLP metaphor detection methods to a psychological task, specifically, analyzing individual differences. For that, we annotate MetPersonality, a dataset of Russian texts written in a psychological experiment setting, with conceptual metaphor. With a widely used conceptual metaphor annotation procedure, we obtain low annotation quality, which arises from the dataset characteristics uncommon in typical automatic metaphor detection tasks. We suggest a novel conceptual metaphor annotation procedure to mitigate issues in annotation quality, increasing the inter-annotator agreement to a moderately high level. We leverage the annotated dataset and existing metaphor datasets in Russian to select, train and evaluate state-of-the-art metaphor detection models, obtaining acceptable results in the metaphor detection task. In turn, the most effective model is used to detect conceptual metaphor automatically in RusPersonality, a larger dataset containing meta-information on psychological traits of the participant authors. Finally, we analyze correlations of automatically detected metaphor use with psychological traits encoded in the Freiburg Personality Inventory (FPI). Our pioneering work on automatically-detected metaphor use and individual differences demonstrates the possibility of unprecedented large-scale research on the relation between of metaphor use and personality traits and dispositions, cognitive and emotional processing.",2,,,,
,,"Mamaev, Ivan D.",,,,,,,,,Conceptual metaphor,,,,,,
,,"Litvinova, Tatiana A.",,,,,,,,,Individual differences,,,,,,
JOUR,Combining natural language processing and multidimensional classifiers to predict and correct CMMS metadata,"Deloose, Arne",Computers in Industry,145,103830,2023,2023/02/01/,0166-3615,https://doi.org/10.1016/j.compind.2022.103830,https://www.sciencedirect.com/science/article/pii/S0166361522002263,Natural language processing,"Computerized maintenance management systems (CMMSs) contain valuable data on the maintenance operations in an organization. A large part of these data consists of unstructured, written texts contained in failure notifications which are generated each time an unexpected failure occurs, enriched with structured metadata consisting of a number of labels that allow to categorize the failures, such as the type of failure, its cause or the corrective action that was taken. In this paper, we show that natural language processing techniques can be used to predict the structured metadata based on the unstructured text and even identify mislabeled notifications or ambiguous labels. Specific attention is given to the complexity that arises from the highly technical nature of the texts combined with a telegraphic writing style and heavy use of sentence fragments and abbreviations. Moreover, it is shown that exploiting dependencies between different components of the metadata, and regarding the prediction problem as a multidimensional classification problem, can improve the reliability of the predicted labels. We illustrate and test our label prediction pipeline on the CMMS data of a large pharmaceutical company.",,,,,
,,"Gysels, Glenn",,,,,,,,,CMMS,,,,,,
,,"De Baets, Bernard",,,,,,,,,Maintenance,,,,,,
,,"Verwaeren, Jan",,,,,,,,,Failure logs,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
JOUR,Multi-level out-of-vocabulary words handling approach,"Lochter, Johannes V.",Knowledge-Based Systems,251,108911,2022,2022/09/05/,0950-7051,https://doi.org/10.1016/j.knosys.2022.108911,https://www.sciencedirect.com/science/article/pii/S0950705122004361,Out-of-vocabulary words,"Distributed representation models can generate a vector representation only for words that belong to a finite vocabulary collected from the training data. If out-of-vocabulary (OOV) words are not handled properly, they can impair the performance of machine learning methods in a given natural language processing task. This study offers a new methodology based on the consolidated top-down human reading theory, which may serve as a strong basis for developing new techniques to deal with the OOV problem. For this, we present MLOH, a Multi-Level OOV Handling approach, based on three chained strategies: analogy, decoding, and prediction. The techniques available in the literature, in general, are limited since they often resolve specific types of OOV words, such as those that can be inferred by analyzing their morphological structure or context. Compared to the process used by human readers to infer unknown words, using a single strategy is generally not effective. We evaluated MLOH performance on tasks that can be highly affected by OOV words, such as part-of-speech tagging, named entity recognition, and text categorization of short and noisy texts. The results indicate that the proposed approach is promising since it could handle most of the OOV words presented, is more generalist, and obtained competitive performance in all experiments.",,,,,
,,"Silva, Renato M.",,,,,,,,,Distributed vector representation,,,,,,
,,"Almeida, Tiago A.",,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
JOUR,A machine learning approach for hierarchical classification of software requirements,"Binkhonain, Manal",Machine Learning with Applications,12,100457,2023,2023/06/15/,2666-8270,https://doi.org/10.1016/j.mlwa.2023.100457,https://www.sciencedirect.com/science/article/pii/S2666827023000105,Requirements engineering,Context:,,,,,
,,"Zhao, Liping",,,,,,,,,Requirements classification,Classification of software requirements into different categories is a critically important task in requirements engineering (RE). Developing machine learning (ML) approaches for requirements classification has attracted great interest in the RE community since the 2000s.,,,,,
,,,,,,,,,,,Machine learning,Objective:,,,,,
,,,,,,,,,,,Hierarchical classification,This paper aims to address two related problems that have been challenging real-world applications of ML approaches: the problems of class imbalance and high dimensionality with low sample size data (HDLSS). These problems can greatly degrade the classification performance of ML methods.,,,,,
,,,,,,,,,,,Imbalanced classes,Methods:,,,,,
,,,,,,,,,,,High Dimensional Data with Low Sample Size (HDLSS),"The paper proposes HC4RC, a novel ML approach for multiclass classification of requirements. HC4RC solves the aforementioned problems through semantic-role based feature selection, dataset decomposition and hierarchical classification. We experimentally compare the effectiveness of HC4RC with three closely related approaches — two of which are based on a traditional statistical classification model whereas one using an advanced deep learning model.",,,,,
,,,,,,,,,,,,Results:,,,,,
,,,,,,,,,,,,Our experiment shows: (1) The class imbalance and HDLSS problems present a challenge to both traditional and advanced ML approaches. (2) The HC4RC approach is simple to use and can effectively address the class imbalance and HDLSS problems compared to similar approaches.,,,,,
,,,,,,,,,,,,Conclusion:,,,,,
,,,,,,,,,,,,This paper makes an important practical contribution to addressing the class imbalance and HDLSS problems in multiclass classification of software requirements.,,,,,
JOUR,Variational autoencoder densified graph attention for fusing synonymous entities: Model and protocol,"Li, Qian",Knowledge-Based Systems,259,110061,2023,2023/01/10/,0950-7051,https://doi.org/10.1016/j.knosys.2022.110061,https://www.sciencedirect.com/science/article/pii/S0950705122011546,Open knowledge graph,"The prediction of missing links of open knowledge graphs (OpenKGs) poses unique challenges compared with well-studied curated knowledge graphs (CuratedKGs). Unlike CuratedKGs whose entities are fully disambiguated against a fixed vocabulary, OpenKGs consist of entities represented by non-canonicalized free-form noun phrases and do not require an ontology specification, which drives the synonymity (multiple entities with different surface forms have the same meaning) and sparsity (a large portion of entities with few links). How to capture synonymous features in such sparse situations and how to evaluate the multiple answers pose challenges to existing models and evaluation protocols. In this paper, we propose VGAT, a variational autoencoder densified graph attention model to automatically mine synonymity features, and propose CR, a cluster ranking protocol to evaluate multiple answers in OpenKGs. For the model, VGAT investigates the following key ideas: (1) phrasal synonymity encoder attempts to capture phrasal features, which can automatically make entities with synonymous texts have closer representations; (2) neighbor synonymity encoder mines structural features with a graph attention network, which can recursively make entities with synonymous neighbors closer in representations. (3) densification attempts to densify the OpenKGs by generating similar embeddings and negative samples. For the protocol, CR is designed from the significance and compactness perspectives to comprehensively evaluate multiple answers. Extensive experiments and analysis show the effectiveness of the VGAT model and rationality of the CR protocol.",,,,,
,,"Wang, Daling",,,,,,,,,Knowledge graph representation,,,,,,
,,"Feng, Shi",,,,,,,,,Cluster ranking,,,,,,
,,"Song, Kaisong",,,,,,,,,Link prediction,,,,,,
,,"Zhang, Yifei",,,,,,,,,,,,,,,
,,"Yu, Ge",,,,,,,,,,,,,,,
JOUR,An evaluation and annotation methodology for product category matching in e-commerce,"Kejriwal, Mayank",Computers in Industry,131,103497,2021,2021/10/01/,0166-3615,https://doi.org/10.1016/j.compind.2021.103497,https://www.sciencedirect.com/science/article/pii/S0166361521001044,E-commerce,"Product category matching is an important task in digital marketplaces and e-commerce, helping to power better search and recommendations in an online context. While variants of the problem have received some attention in academia, there is no documented guidance on how to efficiently acquire annotations for evaluating multiple (current and future) models, many of which rely on modern machine learning techniques such as neural representation learning. In this paper, we motivate and formalize the problem of product category matching in e-commerce, and present a rigorously designed set of guidelines and methodology for acquiring annotations in a cost-effective and reliable manner. We also present a methodology for using the annotations to compare solutions of two or more product category matching methods, including comparing models both before and after annotation. Three widely used e-commerce product category taxonomies, and multiple metrics, are used to demonstrate the utility of our proposals.",,,,,
,,"Shen, Ke",,,,,,,,,Product search and recommendation,,,,,,
,,"Ni, Chien-Chun",,,,,,,,,Annotation,,,,,,
,,"Torzec, Nicolas",,,,,,,,,Methodology,,,,,,
,,,,,,,,,,,Evaluation,,,,,,
,,,,,,,,,,,Language representation model,,,,,,
,,,,,,,,,,,Taxonomy alignment,,,,,,
JOUR,Factors influencing users’ post replying behavior in a senior online community: An empirical investigation,"Qian, Yuxing",Telematics and Informatics,77,101926,2023,2023/02/01/,0736-5853,https://doi.org/10.1016/j.tele.2022.101926,https://www.sciencedirect.com/science/article/pii/S0736585322001599,Senior online community,"Senior online communities (SOCs) have become an important venue for older people to seek support and exchange information. While online community engagement has been well studied in the existing literature, few studies have explored how older adults behave in online communities. Therefore, drawing upon signaling theory, this study aims to investigate how different content-related and social-related signals influence users’ post replying behavior (i.e., reply to another user’s post) in SOCs. We collected 7486 health-related posts and 71,859 comments from one of the most popular Chinese SOCs, Keai (https://www.keai99.com). Information signals in the posts were operationalized using different techniques such as text mining and social network analysis. Results from negative binomial regression indicated that content-related signals (posts’ topic and length) and social-related signals (authors’ position and centrality) were related to replying behavior. In addition, we revealed some differences between the effects of these signals on informational replies and emotional replies. More specifically, compared to posts mentioning traditional Chinese medicine, posts mentioning western medicine received more informational replies, but less emotional replies. Original posts triggered more informational replies, whereas shared posts attracted more emotional replies. Average reply length was positively related to informational replies, but negatively related to emotional replies. Considering the important role of SOCs in satisfying older adults’ social and informational needs, future research is needed to promote user social engagement in SOCs, thereby maintaining their sustainability.",,,,,
,,"Ni, Zhenni",,,,,,,,,Online user engagement,,,,,,
,,"Zheng, Han",,,,,,,,,Post replying behavior,,,,,,
,,"Liu, Zhenghao",,,,,,,,,Informational replies,,,,,,
,,"Ma, Feicheng",,,,,,,,,Emotional replies,,,,,,
JOUR,Evaluating the generalisability of neural rumour verification models,"Kochkina, Elena",Information Processing & Management,60,103116,2023,2023/01/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103116,https://www.sciencedirect.com/science/article/pii/S0306457322002175,Rumour verification,"Research on automated social media rumour verification, the task of identifying the veracity of questionable information circulating on social media, has yielded neural models achieving high performance, with accuracy scores that often exceed 90%. However, none of these studies focus on the real-world generalisability of the proposed approaches, that is whether the models perform well on datasets other than those on which they were initially trained and tested. In this work we aim to fill this gap by assessing the generalisability of top performing neural rumour verification models covering a range of different architectures from the perspectives of both topic and temporal robustness. For a more complete evaluation of generalisability, we collect and release COVID-RV, a novel dataset of Twitter conversations revolving around COVID-19 rumours. Unlike other existing COVID-19 datasets, our COVID-RV contains conversations around rumours that follow the format of prominent rumour verification benchmarks, while being different from them in terms of topic and time scale, thus allowing better assessment of the temporal robustness of the models. We evaluate model performance on COVID-RV and three popular rumour verification datasets to understand limitations and advantages of different model architectures, training datasets and evaluation scenarios. We find a dramatic drop in performance when testing models on a different dataset from that used for training. Further, we evaluate the ability of models to generalise in a few-shot learning setup, as well as when word embeddings are updated with the vocabulary of a new, unseen rumour. Drawing upon our experiments we discuss challenges and make recommendations for future research directions in addressing this important problem.",1,,,,
,,"Hossain, Tamanna",,,,,,,,,Generalisability,,,,,,
,,"Logan, Robert L.",,,,,,,,,Rumour dataset,,,,,,
,,"Arana-Catania, Miguel",,,,,,,,,Deep learning,,,,,,
,,"Procter, Rob",,,,,,,,,,,,,,,
,,"Zubiaga, Arkaitz",,,,,,,,,,,,,,,
,,"Singh, Sameer",,,,,,,,,,,,,,,
,,"He, Yulan",,,,,,,,,,,,,,,
,,"Liakata, Maria",,,,,,,,,,,,,,,
JOUR,HeadlineStanceChecker: Exploiting summarization to detect headline disinformation,"Sepúlveda-Torres, Robiert",Journal of Web Semantics,71,100660,2021,2021/11/01/,1570-8268,https://doi.org/10.1016/j.websem.2021.100660,https://www.sciencedirect.com/science/article/pii/S1570826821000354,Natural Language Processing,"The headline of a news article is designed to succinctly summarize its content, providing the reader with a clear understanding of the news item. Unfortunately, in the post-truth era, headlines are more focused on attracting the reader’s attention for ideological or commercial reasons, thus leading to mis- or disinformation through false or distorted headlines. One way of combating this, although a challenging task, is by determining the relation between the headline and the body text to establish the stance. Hence, to contribute to the detection of mis- and disinformation, this paper proposes an approach (HeadlineStanceChecker) that determines the stance of a headline with respect to the body text to which it is associated. The novelty rests on the use of a two-stage classification architecture that uses summarization techniques to shape the input for both classifiers instead of directly passing the full news body text, thereby reducing the amount of information to be processed while keeping important information. Specifically, summarization is done through Positional Language Models leveraging on semantic resources to identify salient information in the body text that is then compared to its corresponding headline. The results obtained show that our approach achieves 94.31% accuracy for the overall classification and the best FNC-1 relative score compared with the state of the art. It is especially remarkable that the system, which uses only the relevant information provided by the automatic summaries instead of the whole text, is able to classify the different stance categories with very competitive results, especially in the discuss stance between the headline and the news body text. It can be concluded that using automatic extractive summaries as input of our approach together with the two-stage architecture is an appropriate solution to the problem.",,,,,
,,"Vicente, Marta",,,,,,,,,Fake news,,,,,,
,,"Saquete, Estela",,,,,,,,,Misleading headlines,,,,,,
,,"Lloret, Elena",,,,,,,,,Stance detection,,,,,,
,,"Palomar, Manuel",,,,,,,,,Applied computing,,,,,,
,,,,,,,,,,,Document management and text processing,,,,,,
,,,,,,,,,,,Semantic summarization,,,,,,
JOUR,Synset2Node: A new synset embedding based upon graph embeddings,"Jafarinejad, Fatemeh",Intelligent Systems with Applications,17,200159,2023,2023/02/01/,2667-3053,https://doi.org/10.1016/j.iswa.2022.200159,https://www.sciencedirect.com/science/article/pii/S2667305322000965,Synset embeddings,"Due to the advances made in recent years, embedding methods caused a significant increase in the accuracy of text or graph processing methods. Embedding methods exhibit a compact vector representation of the basic elements (words, synsets, nodes,..) of the underlying system to encode the semantic information between the elements. Of course, due to the polysemous nature of words, in some NLP tasks, the use of sense/synset embedding is better than word embedding. However, in the literature, the introduction of embedding for synsets has received less attention. Existing synset embedding methods have complex calculations to calculate synset embedding based on word embeddings or base upon a defined pairwise synset similarity. In this paper, considering the graphical structure of the WordNet and the high-level knowledge encoded in it, we will create a synset embedding directly from the WordNet graph and its synset relations. Node2Vec graph embedding is used to map nodes of this graph to a vector space. We evaluate the performance of different graph structures (e.g. weighted/weightless, directed/undirected graphs). Moreover, we propose a weighting strategy to weight different synset relation types in the resulting WordNet graph. Experimental results of evaluation of the proposed synset embedding on the task of measuring lexical semantic similarities shows that mean squared error of similarities for the proposed synset embedding method on MEM and WordSim353 datasets are 0.065 and 0.035, resp., which is better than the mean squared error of Word2Vec on these datasets, (0.073 and 0.045, resp.). Furthermore, we use the Pearson correlation and Spearman correlation to compare the performance of the proposed synset embedding method with the state-of-the-art ones. The obtained results show the efficiency of the proposed method on various datasets. .The spearman correlation of the SimLex999 is improved by 0.02, while it improves WordSim353 Pearson correlation by 0.14.",,,,,
,,,,,,,,,,,Graph embeddings,,,,,,
,,,,,,,,,,,Node2vec,,,,,,
,,,,,,,,,,,WordNet,,,,,,
,,,,,,,,,,,Lexical Semantic Similarity,,,,,,
JOUR,FADO: Feedback-Aware Double COntrolling Network for Emotional Support Conversation,"Peng, Wei",Knowledge-Based Systems,264,110340,2023,2023/03/15/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110340,https://www.sciencedirect.com/science/article/pii/S0950705123000904,Emotional Support Conversation,"Emotional Support Conversation (ESConv) aims to reduce help-seekers’ emotional distress with a supportive strategy and response. It is essential for the supporter to select an appropriate strategy according to the feedback of the help-seeker (e.g., emotion change during dialog turns, etc.) in ESConv. However, previous methods mainly rely on the dialog history to select the strategy and ignore the help-seeker’s feedback, causing wrong and user-irrelevant strategy predictions. Meanwhile, these methods only model the context-to-strategy flow but pay less attention to the strategy-to-context flow involving the strategy-related context for generating strategy-constrained responses. In this paper, a Feedback-Aware Double COntrolling Network (FADO) is proposed to make a strategy schedule and generate supportive responses. The core modules in FADO include a dual-level feedback strategy selector and a double control reader, where the former leverages the turn-level and conversation-level feedback to encourage or penalize strategies, and the latter constructs a novel strategy-to-context flow to generate strategy-constrain responses. Besides, a strategy dictionary is designed to enrich the semantic information of the strategy and improve the quality of the strategy-constrained response. Experimental results on ESConv indicate that the proposed FADO achieves SOTA performance in terms of strategy selection and response generation. Our code is available at https://github.com/Thedatababbler/FADO.",,,,,
,,"Qin, Ziyuan",,,,,,,,,Strategy selection,,,,,,
,,"Hu, Yue",,,,,,,,,Dual-level feedback,,,,,,
,,"Xie, Yuqiang",,,,,,,,,Response generation,,,,,,
,,"Li, Yunpeng",,,,,,,,,,,,,,,
JOUR,"Multimodal Co-learning: Challenges, applications with datasets, recent advances and future directions","Rahate, Anil",Information Fusion,81,203,2022,2022/05/01/,1566-2535,https://doi.org/10.1016/j.inffus.2021.12.003,https://www.sciencedirect.com/science/article/pii/S1566253521002530,Multimodal co-learning,"Multimodal deep learning systems that employ multiple modalities like text, image, audio, video, etc., are showing better performance than individual modalities (i.e., unimodal) systems. Multimodal machine learning involves multiple aspects: representation, translation, alignment, fusion, and co-learning. In the current state of multimodal machine learning, the assumptions are that all modalities are present, aligned, and noiseless during training and testing time. However, in real-world tasks, typically, it is observed that one or more modalities are missing, noisy, lacking annotated data, have unreliable labels, and are scarce in training or testing, and or both. This challenge is addressed by a learning paradigm called multimodal co-learning. The modeling of a (resource-poor) modality is aided by exploiting knowledge from another (resource-rich) modality using the transfer of knowledge between modalities, including their representations and predictive models. Co-learning being an emerging area, there are no dedicated reviews explicitly focusing on all challenges addressed by co-learning. To that end, in this work, we provide a comprehensive survey on the emerging area of multimodal co-learning that has not been explored in its entirety yet. We review implementations that overcome one or more co-learning challenges without explicitly considering them as co-learning challenges. We present the comprehensive taxonomy of multimodal co-learning based on the challenges addressed by co-learning and associated implementations. The various techniques, including the latest ones, are reviewed along with some applications and datasets. Additionally, we review techniques that appear to be similar to multimodal co-learning and are being used primarily in unimodal or multi-view learning. The distinction between them is documented. Our final goal is to discuss challenges and perspectives and the important ideas and directions for future work that we hope will benefit for the entire research community focusing on this exciting domain.",,239,,,
,,"Walambe, Rahee",,,,,,,,,Multimodal deep learning,,,,,,
,,"Ramanna, Sheela",,,,,,,,,Missing modalities,,,,,,
,,"Kotecha, Ketan",,,,,,,,,Multimodal taxonomy,,,,,,
,,,,,,,,,,,Multimodal datasets,,,,,,
JOUR,Discontinuous grammar as a foreign language,"Fernández-González, Daniel",Neurocomputing,524,43,2023,2023/03/01/,0925-2312,https://doi.org/10.1016/j.neucom.2022.12.045,https://www.sciencedirect.com/science/article/pii/S092523122201551X,Natural language processing,"In order to achieve deep natural language understanding, syntactic constituent parsing is a vital step, highly demanded by many artificial intelligence systems to process both text and speech. One of the most recent proposals is the use of standard sequence-to-sequence models to perform constituent parsing as a machine translation task, instead of applying task-specific parsers. While they show a competitive performance, these text-to-parse transducers are still lagging behind classic techniques in terms of accuracy, coverage and speed. To close the gap, we here extend the framework of sequence-to-sequence models for constituent parsing, not only by providing a more powerful neural architecture for improving their performance, but also by enlarging their coverage to handle the most complex syntactic phenomena: discontinuous structures. To that end, we design several novel linearizations that can fully produce discontinuities and, for the first time, we test a sequence-to-sequence model on the main discontinuous benchmarks, obtaining competitive results on par with task-specific discontinuous constituent parsers and achieving state-of-the-art scores on the (discontinuous) English Penn Treebank.",,58,,,
,,"Gómez-Rodríguez, Carlos",,,,,,,,,Computational linguistics,,,,,,
,,,,,,,,,,,Parsing,,,,,,
,,,,,,,,,,,Discontinuous constituent parsing,,,,,,
,,,,,,,,,,,Neural network,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Sequence-to-sequence model,,,,,,
JOUR,SemAGR: semantic method for accurate geolocations reconstruction within extensive urban sites,"Sergeeva, Julia",Procedia Computer Science,212,409,2022,2022/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2022.11.025,https://www.sciencedirect.com/science/article/pii/S1877050922017161,urban study,"Accurate geolocation of users’ posts in social networks plays a vital role in a wide area of research devoted to analysing the urban environment based on social media data. It allows us to effectively correlate information about a real urban object with its description in social networks. Information analysis about extensive urban sites is challenging in such studies because publications correlate extremely unevenly with marked geotags relating to such objects. In addition, users of social networks often intentionally indicate incorrect geolocations to increase their publications’ popularity. This article proposes a solution to the problem of accurate geolocation reconstruction for extensive urban sites. It is proposed to use a combined method that enriches the list of initial geolocations of the social network with the help of external sources. The transformer model is then applied to recognise named entities that help to detect mentions of events or locations in user posts. After detection, posts are redefined to the new locations from the extended list according to the semantic similarity calculated between publications and locations.",,417,11th International Young Scientist Conference on Computational Science,,
,,"Filatova, Anastasiia",,,,,,,,,spatial analysis,,,,,,
,,"Kovalchuk, Mikhail",,,,,,,,,text mining,,,,,,
,,"Teryoshkin, Sergey",,,,,,,,,named entity recognition,,,,,,
,,,,,,,,,,,social networks,,,,,,
JOUR,Identification of Multilingual Offense and Troll from Social Media Memes Using Weighted Ensemble of Multimodal Features,"Hossain, Eftekhar",Journal of King Saud University - Computer and Information Sciences,34,6605,2022,2022/10/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2022.06.010,https://www.sciencedirect.com/science/article/pii/S1319157822002166,Multimodal learning,"In recent years, memes have become a common medium of promulgating offensive views by the content polluters in social media. Due to their multimodal nature, memes can easily evade the content regulators’ eyes. The proliferation of these undesired or harmful memes can cause a detrimental impact on social harmony. Therefore, restraining offensive memes on social media is of utmost importance. However, analyzing memes is very complicated as they implicitly express human emotions. Previous studies have not explored the joint modelling of multimodal features and their counteractive unimodal features (i.e., image, text) to classify undesired memes. This paper presents a framework that utilizes the weighted ensemble technique to assign weights to the participating visual, textual and multimodal models. The state-of-the-art visual (i.e., VGG19, VGG16, ResNet50) and textual (i.e., multilingual-BERT, multilingual-DistilBERT, XLM-R) models are employed to make the constituent modules of the framework. Moreover, two fusion approaches (i.e., early fusion and late fusion) are used to combine the visual and textual features for developing the multimodal models. The evaluations have demonstrated that the proposed weighted ensemble technique improves the performance over the investigated unimodal, multimodal, and ensemble models. The result shows that the proposed approach achieves superior outcomes on two multilingual benchmark datasets (MultiOFF and TamilMemes), with 66.73% and 58.59% weighted f1-scores, respectively. Furthermore, the comparative analysis reveals that the proposed approach outdoes other existing works by improving approximately 13% and 2% weighted f1-score gain.",9,6623,,,
,,"Sharif, Omar",,,,,,,,,Multimodal data,,,,,,
,,"Hoque, Mohammed Moshiul",,,,,,,,,Multilingual offense detection,,,,,,
,,"Akber Dewan, M. Ali",,,,,,,,,Ensemble,,,,,,
,,"Siddique, Nazmul",,,,,,,,,Multimodal fusion,,,,,,
,,"Hossain, Md. Azad",,,,,,,,,,,,,,,
JOUR,Pseudo-labeling with transformers for improving Question Answering systems,"Kuligowska, Karolina",Procedia Computer Science,192,1162,2021,2021/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2021.08.119,https://www.sciencedirect.com/science/article/pii/S1877050921016082,Natural Language Processing,"Advances in neural networks contributed to the fast development of Natural Language Processing systems. As a result, Question Answering systems have evolved and can classify and answer questions in an intuitive yet communicative way. However, the lack of large volumes of labeled data prevents large-scale training and development of Question Answering systems, confirming the need for further research. This paper aims to handle this real-world problem of lack of labeled datasets by applying a pseudo-labeling technique relying on a neural network transformer model DistilBERT. In order to evaluate our contribution, we examined the performance of a text classification transformer model that was fine-tuned on the data subject to prior pseudo-labeling. Research has shown the usefulness of the applied pseudo-labeling technique on a neural network text classification transformer model DistilBERT. The results of our analysis indicated that the model with additional pseudo-labeled data achieved the best results among other compared neural network architectures. Based on that result, Question Answering systems may be directly improved by enriching their training steps with additional data acquired cost-effectively.",,1169,Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021,,
,,"Kowalczuk, Bartłomiej",,,,,,,,,Question Answering systems,,,,,,
,,,,,,,,,,,pseudo-labeling,,,,,,
,,,,,,,,,,,neural networks,,,,,,
,,,,,,,,,,,transfer learning,,,,,,
,,,,,,,,,,,knowledge distillation,,,,,,
JOUR,A deep penetration network for sentence classification,"Chen, Yanping",Information Fusion,95,174,2023,2023/07/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.02.015,https://www.sciencedirect.com/science/article/pii/S1566253523000556,Sentence classification,"Sentence classification is an important task in natural language processing. The task makes use of deep networks to enclose a mass of features with different granularities in a sentence. However, the classification usually suffer from severe performance degradation when stacking a large number of networks. The main reason is that, in a deep architecture, the silent feature representations are easily weakened and mixed with noisy information, which is not effective in learning contextual features and constructing semantic dependencies in a sentence. In this paper, a deep penetration network (DPN) is designed to improve deep architectures’ ability to preserve the favourable semantic features. The DPN enables salient features to penetrate through a deeper architecture and to construct long semantic dependencies between them. This approach is evaluated on seven public datasets. Our experiments show that the DPN exhibits a stable performance with deeper architectures. It improves the performance on three types of sentence classification tasks, outperforming the existing state-of-the-art models.",,185,,,
,,"Zhong, Xinyang",,,,,,,,,Text classification,,,,,,
,,"Liu, Yijun",,,,,,,,,Natural language processing,,,,,,
,,"Dong, Bo",,,,,,,,,Feature extraction,,,,,,
,,"Zheng, Qinghua",,,,,,,,,,,,,,,
JOUR,Tackling the infodemic during a pandemic: A comparative study on algorithms to deal with thematically heterogeneous fake news,"Vasist, Pramukh Nanjundaswamy",International Journal of Information Management Data Insights,2,100133,2022,2022/11/01/,2667-0968,https://doi.org/10.1016/j.jjimei.2022.100133,https://www.sciencedirect.com/science/article/pii/S2667096822000763,Ensemble,"Fake news poses a grave threat with devastating consequences in this information-centric age. While advances in data science undeniably hold the key to accurately detecting and curtailing the unfettered spread of fake news, guidance on the selection of algorithms and models that are best suited to a specific fake news scenario leaves much to be desired. Most studies have focused on fake news in a specific domain and employed a limited range of algorithmic techniques. In contrast, the thematic diversity of fake news raises questions over the comprehensiveness of such techniques, whose performance drops when exposed to fake news from a different domain. The current study responds to this call for guidance by focusing on thematically diverse datasets, applying a series of complex algorithms, and performing topic modeling on them. The results demonstrate that ensemble techniques outperform other algorithms, achieving high levels of accuracy of over 98 percent and 95 percent on thematically diverse and pandemic-related datasets, respectively. The study also demonstrates that neural networks are not a panacea for all situations, while topic modeling helps illustrate the lack of coherence in fake news articles. The study offers a distinct perspective on the accuracy of a diverse set of algorithmic approaches and their ability to adapt to an ever-evolving multi-domain world of fake news. A key implication of the study is the unique and comprehensive view of classification performance when exposed to diverse datasets, including pandemic-related news and data from other disciplines, as opposed to its performance on pandemic-related data alone. Our practical contribution is truly the comparative perspective we offer to practitioners when a choice of algorithm is to be made to accurately detect fake news with thematic heterogeneity.",2,,,,
,,"Sebastian, M.P.",,,,,,,,,Fake news,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Neural networks,,,,,,
,,,,,,,,,,,Text classification,,,,,,
JOUR,Towards finding the lost generation of autistic adults: A deep and multi-view learning approach on social media,"Khorasani, Mahsa",Knowledge-Based Systems,276,110724,2023,2023/09/27/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110724,https://www.sciencedirect.com/science/article/pii/S0950705123004744,Autism disorder,"The detection of mental disorders through social media has received significant attention. With the growing prevalence of Autism Spectrum Disorder (ASD) and the inherent difficulties in diagnosing adults, researchers have attempted to identify undiagnosed adults. Previous studies have primarily concentrated on analyzing ASD characteristics rather than directly detecting ASD. The current study aims to propose a novel framework to assist in identifying the “lost generation” of ASD adults using their social media posts. Combining traditional and deep learning methods makes it possible to model complex aspects of ASD diagnostic characteristics, which have been relatively overlooked in previous studies. To accomplish this, specific formalizations for users’ patterns of interest as a main ASD diagnostic characteristic are proposed first. The latent linguistic and semantic features of ASD users’ postings are then modeled using deep and transformer-based language models. Finally, all these different aspects are considered together to train a detection model by employing the multi-view learning approach. The experiments show that the feature of idiosyncratic interests has more discriminative power than limited and repetitive interests. The results also indicate that the early fusion of interest-related features along with deep linguistic features outperforms the other examined feature combinations. Additionally, the proposed ‘if−iuf’ fusion model demonstrates improved performance in capturing patterns of interests, compared to baselines. These findings suggest the potential application of the proposed framework towards indirectly identifying ASD users on social media, as evidenced by achieving precision and recall rates of 85% and 82% respectively on the used sampled dataset.",,,,,
,,"Kahani, Mohsen",,,,,,,,,Mental health,,,,,,
,,"Yazdi, Seyed Amir Amin",,,,,,,,,Social media mining,,,,,,
,,"Hajiaghaei-Keshteli, Mostafa",,,,,,,,,Multi-view learning,,,,,,
,,,,,,,,,,,Transformer learning,,,,,,
JOUR,CJE-TIG: Zero-shot cross-lingual text-to-image generation by Corpora-based Joint Encoding,"Zhang, Han",Knowledge-Based Systems,239,108006,2022,2022/03/05/,0950-7051,https://doi.org/10.1016/j.knosys.2021.108006,https://www.sciencedirect.com/science/article/pii/S0950705121011138,Cross-lingual pre-training,"Recently, text-to-Image (T2I) generation has been well developed by improving synthesis authenticity, text-consistency and generation diversity. However, large amount of pairwise image–text data required restricts generalization of synthesis models only to its pre-trained language. In this paper, a cross-lingual pre-training method is proposed to adapt target low-resource language to pre-trained generative models. As far as we known, this is the first time that arbitrary input languages could access T2I generation. This joint encoding scheme fulfills both universal and visual semantic alignment. With any prepared GAN-based T2I framework, pre-trained source encoder model could be easily fine-tuned to construct target encoder model and hence entirely enable transfer of T2I synthesis ability between languages. After that, a semantic-level alignment independent of source T2I structure is established to guarantee optimal text consistency and detail generation. Different from monolingual T2I methods that apply discriminator to enhance generation quality, we use an adversarial training scheme that optimizes the sentence-level alignment along with the word-level alignment with a self-attention mechanism. Considering of training for low-resource languages lack of parallel texts in practice, target input embedding is designed available for zero-shot learning. Experimental results prove robustness of the proposed cross-lingual T2I pre-training on multiple downstream generative models and target languages applied.",,,,,
,,"Yang, Suyi",,,,,,,,,Text-to-image synthesis,,,,,,
,,"Zhu, Hongqing",,,,,,,,,Universal contextual word vector space,,,,,,
,,,,,,,,,,,Semantic alignment,,,,,,
,,,,,,,,,,,Joint adversarial training,,,,,,
JOUR,"ISE-Hate: A benchmark corpus for inter-faith, sectarian, and ethnic hatred detection on social media in Urdu","Akram, Muhammad Hammad",Information Processing & Management,60,103270,2023,2023/05/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103270,https://www.sciencedirect.com/science/article/pii/S0306457323000079,Hateful content detection,"Social media has become the most popular platform for free speech. This freedom of speech has given opportunities to the oppressed to raise their voice against injustices, but on the other hand, this has led to a disturbing trend of spreading hateful content of various kinds. Pakistan has been dealing with the issue of sectarian and ethnic violence for the last three decades and now due to freedom of speech, there is a growing trend of disturbing content about religion, sect, and ethnicity on social media. This necessitates the need for an automated system for the detection of controversial content on social media in Urdu which is the national language of Pakistan. The biggest hurdle that has thwarted the Urdu language processing is the scarcity of language resources, annotated datasets, and pretrained language models. In this study, we have addressed the problem of detecting Interfaith, Sectarian, and Ethnic hatred on social media in Urdu language using machine learning and deep learning techniques. In particular, we have: (1) developed and presented guidelines for annotating Urdu text with appropriate labels for two levels of classification, (2) developed a large dataset of 21,759 tweets using the developed guidelines and made it publicly available, and (3) conducted experiments to compare the performance of eight supervised machine learning and deep learning techniques, for the automated identification of hateful content. In the first step, experiments are performed for the hateful content detection as a binary classification task, and in the second step, the classification of Interfaith, Sectarian and Ethnic hatred detection is performed as a multiclass classification task. Overall, Bidirectional Encoder Representation from Transformers (BERT) proved to be the most effective technique for hateful content identification in Urdu tweets.",3,,,,
,,"Shahzad, Khurram",,,,,,,,,Urdu,,,,,,
,,"Bashir, Maryam",,,,,,,,,Corpus generation,,,,,,
,,,,,,,,,,,Sectarian,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Ethnic,,,,,,
,,,,,,,,,,,Hatred,,,,,,
JOUR,Cross-domain knowledge distillation for text classification,"Zhang, Shaokang",Neurocomputing,509,11,2022,2022/10/14/,0925-2312,https://doi.org/10.1016/j.neucom.2022.08.061,https://www.sciencedirect.com/science/article/pii/S092523122201058X,Knowledge distillation,"Most text classification methods achieve great success based on the large-scale annotated data and the pre-trained language models. However, the labeled data is insufficient in practice, and the pre-trained language models are difficult to deploy due to their high computing resources and slow inference speed. In this paper, we propose cross-domain knowledge distillation, where the teacher and student tasks belong to different domains. It not only acquires knowledge from multiple teachers but also accelerates inference and reduces model size. Specifically, we train the pre-trained language models on factual knowledge obtained by aligned Wikipedia text to Wikidata triplets and fine-tune it as the teacher model. Then we use the heterogeneous multi-teacher knowledge distillation to transfer knowledge from the multiple teacher models to the student model. Multi-teacher knowledge vote can distill knowledge related to the target domain. Moreover, we also introduce the teacher assistant to help distill large pre-trained language models. Finally, we reduce the difference between the source domain and target domain by multi-source domain adaptation to solve the domain shift problem. Experiments on the multiple public datasets demonstrate that our method can achieve competitive performance while having fewer parameters and less inference time.",,20,,,
,,"Jiang, Lei",,,,,,,,,Domain adaptation,,,,,,
,,"Tan, Jianlong",,,,,,,,,Multi-teacher knowledge vote,,,,,,
,,,,,,,,,,,Text classification,,,,,,
JOUR,"Computational approaches to Explainable Artificial Intelligence: Advances in theory, applications and trends","Górriz, J.M.",Information Fusion,100,101945,2023,2023/12/01/,1566-2535,https://doi.org/10.1016/j.inffus.2023.101945,https://www.sciencedirect.com/science/article/pii/S1566253523002610,Explainable Artificial Intelligence,"Deep Learning (DL), a groundbreaking branch of Machine Learning (ML), has emerged as a driving force in both theoretical and applied Artificial Intelligence (AI). DL algorithms, rooted in complex and non-linear artificial neural systems, excel at extracting high-level features from data. DL has demonstrated human-level performance in real-world tasks, including clinical diagnostics, and has unlocked solutions to previously intractable problems in virtual agent design, robotics, genomics, neuroimaging, computer vision, and industrial automation. In this paper, the most relevant advances from the last few years in Artificial Intelligence (AI) and several applications to neuroscience, neuroimaging, computer vision, and robotics are presented, reviewed and discussed. In this way, we summarize the state-of-the-art in AI methods, models and applications within a collection of works presented at the 9th International Conference on the Interplay between Natural and Artificial Computation (IWINAC). The works presented in this paper are excellent examples of new scientific discoveries made in laboratories that have successfully transitioned to real-life applications.",,,,,
,,"Álvarez-Illán, I.",,,,,,,,,Data science,,,,,,
,,"Álvarez-Marquina, A.",,,,,,,,,Computational approaches,,,,,,
,,"Arco, J.E.",,,,,,,,,Machine learning,,,,,,
,,"Atzmueller, M.",,,,,,,,,Deep learning,,,,,,
,,"Ballarini, F.",,,,,,,,,Neuroscience,,,,,,
,,"Barakova, E.",,,,,,,,,Robotics,,,,,,
,,"Bologna, G.",,,,,,,,,Biomedical applications,,,,,,
,,"Bonomini, P.",,,,,,,,,Computer-aided diagnosis systems,,,,,,
,,"Castellanos-Dominguez, G.",,,,,,,,,,,,,,,
,,"Castillo-Barnes, D.",,,,,,,,,,,,,,,
,,"Cho, S.B.",,,,,,,,,,,,,,,
,,"Contreras, R.",,,,,,,,,,,,,,,
,,"Cuadra, J.M.",,,,,,,,,,,,,,,
,,"Domínguez, E.",,,,,,,,,,,,,,,
,,"Domínguez-Mateos, F.",,,,,,,,,,,,,,,
,,"Duro, R.J.",,,,,,,,,,,,,,,
,,"Elizondo, D.",,,,,,,,,,,,,,,
,,"Fernández-Caballero, A.",,,,,,,,,,,,,,,
,,"Fernandez-Jover, E.",,,,,,,,,,,,,,,
,,"Formoso, M.A.",,,,,,,,,,,,,,,
,,"Gallego-Molina, N.J.",,,,,,,,,,,,,,,
,,"Gamazo, J.",,,,,,,,,,,,,,,
,,"González, J. García",,,,,,,,,,,,,,,
,,"Garcia-Rodriguez, J.",,,,,,,,,,,,,,,
,,"Garre, C.",,,,,,,,,,,,,,,
,,"Garrigós, J.",,,,,,,,,,,,,,,
,,"Gómez-Rodellar, A.",,,,,,,,,,,,,,,
,,"Gómez-Vilda, P.",,,,,,,,,,,,,,,
,,"Graña, M.",,,,,,,,,,,,,,,
,,"Guerrero-Rodriguez, B.",,,,,,,,,,,,,,,
,,"Hendrikse, S.C.F.",,,,,,,,,,,,,,,
,,"Jimenez-Mesa, C.",,,,,,,,,,,,,,,
,,"Jodra-Chuan, M.",,,,,,,,,,,,,,,
,,"Julian, V.",,,,,,,,,,,,,,,
,,"Kotz, G.",,,,,,,,,,,,,,,
,,"Kutt, K.",,,,,,,,,,,,,,,
,,"Leming, M.",,,,,,,,,,,,,,,
,,"de Lope, J.",,,,,,,,,,,,,,,
,,"Macas, B.",,,,,,,,,,,,,,,
,,"Marrero-Aguiar, V.",,,,,,,,,,,,,,,
,,"Martinez, J.J.",,,,,,,,,,,,,,,
,,"Martinez-Murcia, F.J.",,,,,,,,,,,,,,,
,,"Martínez-Tomás, R.",,,,,,,,,,,,,,,
,,"Mekyska, J.",,,,,,,,,,,,,,,
,,"Nalepa, G.J.",,,,,,,,,,,,,,,
,,"Novais, P.",,,,,,,,,,,,,,,
,,"Orellana, D.",,,,,,,,,,,,,,,
,,"Ortiz, A.",,,,,,,,,,,,,,,
,,"Palacios-Alonso, D.",,,,,,,,,,,,,,,
,,"Palma, J.",,,,,,,,,,,,,,,
,,"Pereira, A.",,,,,,,,,,,,,,,
,,"Pinacho-Davidson, P.",,,,,,,,,,,,,,,
,,"Pinninghoff, M.A.",,,,,,,,,,,,,,,
,,"Ponticorvo, M.",,,,,,,,,,,,,,,
,,"Psarrou, A.",,,,,,,,,,,,,,,
,,"Ramírez, J.",,,,,,,,,,,,,,,
,,"Rincón, M.",,,,,,,,,,,,,,,
,,"Rodellar-Biarge, V.",,,,,,,,,,,,,,,
,,"Rodríguez-Rodríguez, I.",,,,,,,,,,,,,,,
,,"Roelofsma, P.H.M.P.",,,,,,,,,,,,,,,
,,"Santos, J.",,,,,,,,,,,,,,,
,,"Salas-Gonzalez, D.",,,,,,,,,,,,,,,
,,"Salcedo-Lagos, P.",,,,,,,,,,,,,,,
,,"Segovia, F.",,,,,,,,,,,,,,,
,,"Shoeibi, A.",,,,,,,,,,,,,,,
,,"Silva, M.",,,,,,,,,,,,,,,
,,"Simic, D.",,,,,,,,,,,,,,,
,,"Suckling, J.",,,,,,,,,,,,,,,
,,"Treur, J.",,,,,,,,,,,,,,,
,,"Tsanas, A.",,,,,,,,,,,,,,,
,,"Varela, R.",,,,,,,,,,,,,,,
,,"Wang, S.H.",,,,,,,,,,,,,,,
,,"Wang, W.",,,,,,,,,,,,,,,
,,"Zhang, Y.D.",,,,,,,,,,,,,,,
,,"Zhu, H.",,,,,,,,,,,,,,,
,,"Zhu, Z.",,,,,,,,,,,,,,,
,,"Ferrández-Vicente, J.M.",,,,,,,,,,,,,,,
JOUR,"Unveiling Security, Privacy, and Ethical Concerns of ChatGPT","Wu, Xiaodong",Journal of Information and Intelligence,,,2023,2023/10/31/,2949-7159,https://doi.org/10.1016/j.jiixd.2023.10.007,https://www.sciencedirect.com/science/article/pii/S2949715923000707,ChatGPT,"This paper delves into the realm of ChatGPT, an AI-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. Although ChatGPT holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. By exploring the upgrade path from GPT-1 to GPT-4, discussing the model’s features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives. Focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. Finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.",,,,,
,,"Duan, Ran",,,,,,,,,Large Language Model (LLM),,,,,,
,,"Ni, Jianbing",,,,,,,,,Security,,,,,,
,,,,,,,,,,,Privacy,,,,,,
,,,,,,,,,,,Ethics,,,,,,
JOUR,JointMatcher: Numerically-aware entity matching using pre-trained language models with attention concentration,"Ye, Chen",Knowledge-Based Systems,251,109033,2022,2022/09/05/,0950-7051,https://doi.org/10.1016/j.knosys.2022.109033,https://www.sciencedirect.com/science/article/pii/S0950705122005044,Entity matching,"Entity matching (EM) aims to identify whether two records refer to the same underlying real-world entity. Traditional entity matching methods mainly focus on structured data, where the attribute values are short and atomic. Recently, there has been an increasing demand for matching textual records, such as matching descriptions of products that correspond to long spans of text, which challenges the applications of these methods. Although a few deep learning (DL) solutions have been proposed, these solutions tend to “directly” use the DL techniques and treat the EM as NLP tasks without determining the unique demand for the EM task. Thus, the performance of these DL-based solutions is still far from satisfactory. In this paper, we present JointMatcher, a novel EM method based on the pre-trained Transformer-based language models so that the generated features of the textual records contain the context information. We realize that more attention paid to the similar segments and number-contained segments of the record pair is crucial for accurate matching. To integrate the high-contextualized features with the consideration of paying more attention to the similar segments and the number-contained segments, JointMatcher is equipped with the relevance-aware encoder and the numerically-aware encoder. Extensive experiments using structured and real-world textual datasets demonstrated that JointMatcher outperforms the previous state-of-the-art (SOTA) results without injecting any domain knowledge when small or medium size training sets are used.",,,,,
,,"Jiang, Shihao",,,,,,,,,Pre-trained language model,,,,,,
,,"Zhang, Hua",,,,,,,,,Attention concentration,,,,,,
,,"Wu, Yifan",,,,,,,,,,,,,,,
,,"Shi, Jiankai",,,,,,,,,,,,,,,
,,"Wang, Hongzhi",,,,,,,,,,,,,,,
,,"Dai, Guojun",,,,,,,,,,,,,,,
JOUR,"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI","Ballout, Mohamad",Procedia Computer Science,222,94,2023,2023/01/01/,1877-0509,https://doi.org/10.1016/j.procs.2023.08.147,https://www.sciencedirect.com/science/article/pii/S1877050923009122,transfer learning,"Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7%, compared to transformers trained from scratch, which have an average accuracy of 29.0%. The significant improvement demonstrated across three types of datasets suggests that pre-training on language helps the models to acquire general knowledge, bringing us a step closer to general AI. We also showed that reducing the number of parameters in pre-trained language models does not have a great impact as the performance drops slightly when using T5-Small instead of T5-Base. In fact, when using only 2% of the parameters, we achieved a great improvement compared to training from scratch. Finally, in contrast to prior work, we find out that using pre-trained embeddings for the input layer is necessary to achieve the desired results.",,103,International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023),,
,,"Krumnack, Ulf",,,,,,,,,natural language processing,,,,,,
,,"Heidemann, Gunther",,,,,,,,,multi-modal learning,,,,,,
,,"Kühnberger, Kai-Uwe",,,,,,,,,deep learning,,,,,,
,,,,,,,,,,,pre-trained transformers,,,,,,
JOUR,A simple and efficient text matching model based on deep interaction,"Yu, Chuanming",Information Processing & Management,58,102738,2021,2021/11/01/,0306-4573,https://doi.org/10.1016/j.ipm.2021.102738,https://www.sciencedirect.com/science/article/pii/S030645732100220X,Text matching,"In recent years, text matching has gained increasing research focus and shown great improvements. However, due to the long-distance dependency and polysemy, existing text matching models cannot effectively capture the contextual and implicit semantic information of texts. Additionally, existing models are lack of generalization ability when applied to different scenarios. In this study, we propose a novel Deep Interactive Text Matching (DITM) model by integrating the encoder layer, the co-attention layer, and the fusion layer as an interaction module, based on a matching-aggregation framework. In particular, the interaction process is iterated multiple times to obtain the in-depth interaction information, and the relationship between the text pair is extracted through the multi-perspective pooling. We conduct extensive experiments on four text matching tasks, i.e., opinion retrieval, answer selection, paraphrase identification and natural language inference. Compared with the state-of-the-art text matching methods, the proposed model achieved the best results on most of the tasks, which proves that our model could effectively capture the interactive information between text pairs, and has a high generalization ability among different tasks. Further multi-lingual investigations show the similarities of the performance between English and Chinese, which suggest that our model could be ported to other languages. The research contributes a simple and efficient implementation of text matching in a situation where there is limited computing capacity, and sheds light on leveraging text matching models to facilitate a range of downstream tasks.",6,,,,
,,"Xue, Haodong",,,,,,,,,Deep learning,,,,,,
,,"Jiang, Yifan",,,,,,,,,Deep interaction,,,,,,
,,"An, Lu",,,,,,,,,Attention mechanism,,,,,,
,,"Li, Gang",,,,,,,,,Neural network,,,,,,
JOUR,MRC-Sum: An MRC framework for extractive summarization of academic articles in natural sciences and medicine,"Li, Shuaimin",Information Processing & Management,60,103467,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103467,https://www.sciencedirect.com/science/article/pii/S0306457323002042,Extractive summarization,"Extractive summarization for academic articles in natural sciences and medicine has attracted attention for a long time. However, most existing extractive summarization models often process academic articles with sentence classification models, which are hard to produce comprehensive summaries. To address this issue, we explore a new view to solve the extractive summarization of academic articles in natural sciences and medicine by taking it as a question-answering process. We propose a novel framework, MRC-Sum, where the extractive summarization for academic articles in natural sciences and medicine is cast as an MRC (Machine Reading Comprehension) task. To instantiate MRC-Sum, article-summary pairs in the summarization datasets are firstly reconstructed into (Question, Answer, Context) triples in the MRC task. Several questions are designed to cover the main aspects (e.g. Background, Method, Result, Conclusion) of the articles in natural sciences and medicine. A novel strategy is proposed to solve the problem of the non-existence of the ground truth answer spans. Then MRC-Sum is trained on the reconstructed datasets and large-scale pre-trained models. During the inference stage, four answer spans of the predefined questions are given by MRC-Sum and concatenated to form the final summary for each article. Experiments on three publicly available benchmarks, i.e., the Covid, PubMed, and arXiv datasets, demonstrate the effectiveness of MRC-Sum. Specifically, MRC-Sum outperforms advanced extractive summarization baselines on the Covid dataset and achieves competitive results on the PubMed and arXiv datasets. We also propose a novel metric, COMPREHS, to automatically evaluate the comprehensiveness of the system summaries for academic articles in natural sciences and medicine. Abundant experiments are conducted and verified the reliability of the proposed metric. And the results of the COMPREHS metric show that MRC-Sum is able to generate more comprehensive summaries than the baseline models.",5,,,,
,,"Xu, Jungang",,,,,,,,,Machine reading comprehension,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Academic articles,,,,,,
JOUR,Explainable machine learning multi-label classification of Spanish legal judgements,"de Arriba-Pérez, Francisco",Journal of King Saud University - Computer and Information Sciences,34,10180,2022,2022/11/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2022.10.015,https://www.sciencedirect.com/science/article/pii/S1319157822003664,Machine learning,"Artificial Intelligence techniques such as Machine Learning (ml) have not been exploited to their maximum potential in the legal domain. This has been partially due to the insufficient explanations they provided about their decisions. Automatic expert systems with explanatory capabilities can be specially useful when legal practitioners search jurisprudence to gather contextual knowledge for their cases. Therefore, we propose a hybrid system that applies ml for multi-label classification of judgements (sentences) and visual and natural language descriptions for explanation purposes, boosted by Natural Language Processing techniques and deep legal reasoning to identify the entities, such as the parties, involved. We are not aware of any prior work on automatic multi-label classification of legal judgements also providing natural language explanations to the end-users with comparable overall quality. Our solution achieves over 85% micro precision on a labelled data set annotated by legal experts. This endorses its interest to relieve human experts from monotonous labour-intensive legal classification tasks.","10, Part B",10192,,,
,,"García-Méndez, Silvia",,,,,,,,,Natural language processing,,,,,,
,,"González-Castaño, Francisco J.",,,,,,,,,Multi-label text classification,,,,,,
,,"González-González, Jaime",,,,,,,,,Interpretability and explainability,,,,,,
,,,,,,,,,,,Legal texts,,,,,,
JOUR,Joint structured pruning and dense knowledge distillation for efficient transformer model compression,"Cui, Baiyun",Neurocomputing,458,56,2021,2021/10/11/,0925-2312,https://doi.org/10.1016/j.neucom.2021.05.084,https://www.sciencedirect.com/science/article/pii/S0925231221008390,Transformer Model Compression,"In this paper, we develop a novel Joint Model Compression (referred to as JMC) method by combining structured pruning and dense knowledge distillation techniques to significantly compress original large language model into a deep compressed shallow network. In particular, a new Direct Importance-aware Structured Pruning (referred as DISP) approach is proposed to structurally prune the redundant structures in the Transformer networks directly based on the corresponding parameter matrices in the model. Besides, a Dense Knowledge Distillation (referred to as DKD) method is developed with a many-to-one layer mapping strategy to leverage more comprehensive layer-wise linguistic knowledge for the distillation. Further, the proposed structured pruning and dense knowledge distillation are integrated together to perform the joint compression, which enables us to achieve a significant compression without sacrificing model accuracy. The extensive experimental results across four NLP tasks on seven datasets demonstrate its effectiveness and superiority to the baselines, while maintaining similar performance to original large model with further remarkable benefits for inference-time speedup and memory efficiency.",,69,,,
,,"Li, Yingming",,,,,,,,,Structured Pruning,,,,,,
,,"Zhang, Zhongfei",,,,,,,,,Knowledge Distillation,,,,,,
JOUR,Legal Information Retrieval systems: State-of-the-art and open issues,"Sansone, Carlo",Information Systems,106,101967,2022,2022/05/01/,0306-4379,https://doi.org/10.1016/j.is.2021.101967,https://www.sciencedirect.com/science/article/pii/S0306437921001551,Legal Information Retrieval,"In the last years, the legal domain has been revolutionized by the use of Information and Communication Technologies, producing large amount of digital information. Legal practitioners’ needs, then, in browsing these repositories has required to investigate more efficient retrieval methods, that assume more relevance because digital information is mostly unstructured. In this paper we analyze the state-of-the-art of artificial intelligence approaches for legal domain, focusing on Legal Information Retrieval systems based on Natural Language Processing, Machine Learning and Knowledge Extraction techniques. Finally, we also discuss challenges – mainly focusing on retrieving similar cases, statutes or paragraph for supporting latest cases’ analysis – and open issues about Legal Information Retrieval systems.",,,,,
,,"Sperlí, Giancarlo",,,,,,,,,Artificial Intelligence,,,,,,
,,,,,,,,,,,Natural Processing Language,,,,,,
,,,,,,,,,,,Ontology,,,,,,
JOUR,A commonsense-infused language-agnostic learning framework for enhancing prediction of political bias in multilingual news headlines,"Swati, Swati",Knowledge-Based Systems,277,110838,2023,2023/10/09/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110838,https://www.sciencedirect.com/science/article/pii/S0950705123005889,News,"Predicting the political bias of news headlines is a challenging task that becomes even more challenging in a multilingual setting with low-resource languages. To deal with this, we propose to utilise Inferential Commonsense Knowledge via a Translate–Retrieve–Translate strategy to introduce a learning framework. To begin with, we use the translate–retrieve–translate strategy to acquire inferential knowledge in the target language. We then employ an attention mechanism to emphasise important inferences. We finally integrate the attended inferences into a multilingual, pre-trained language model for the task of bias prediction. To evaluate the effectiveness of our framework, we present a dataset of over 62.6K multilingual news headlines annotated with their respective political biases in five low-resource European languages. We evaluate several state-of-the-art multilingual pre-trained language models since their performance tends to vary across languages (low or high resource). Evaluation results demonstrate that our proposed framework is effective regardless of the models employed. Overall, the best-performing model trained with only headlines shows 0.90 accuracy and F1 and a 0.83 Jaccard score. With attended knowledge in our framework, the same model shows an increase in 2.2% accuracy and F1 and a 3.6% Jaccard score. Extending our experiments to individual languages reveals that the models we analyse for Slovenian perform significantly worse than other languages in our dataset. To investigate this, we assess the effect of translation quality on prediction performance. It indicates that the disparity in performance is most likely due to poor translation quality. We release our dataset and scripts at https://github.com/Swati17293/KG-Multi-Bias for future research. Our framework has the potential to benefit journalists, social scientists, news producers, and consumers.",,,,,
,,"Grobelnik, Adrian Mladenić",,,,,,,,,Bias,,,,,,
,,"Mladenić, Dunja",,,,,,,,,NLP,,,,,,
,,"Grobelnik, Marko",,,,,,,,,Commonsense,,,,,,
,,,,,,,,,,,Inferential commonsense knowledge,,,,,,
,,,,,,,,,,,Multilingual,,,,,,
,,,,,,,,,,,Headline,,,,,,
,,,,,,,,,,,Low-resource,,,,,,
,,,,,,,,,,,Imbalanced sample distribution,,,,,,
,,,,,,,,,,,Pre-trained language models,,,,,,
JOUR,Detecting malicious activity in Twitter using deep learning techniques,"Ilias, Loukas",Applied Soft Computing,107,107360,2021,2021/08/01/,1568-4946,https://doi.org/10.1016/j.asoc.2021.107360,https://www.sciencedirect.com/science/article/pii/S1568494621002830,Social media,"Undoubtedly, social media, such as Facebook and Twitter, constitute a major part of our everyday life due to the incredible possibilities they offer to their users. However, Twitter and generally online social networks (OSNs) are increasingly used by automated accounts, widely known as bots, due to their immense popularity across a wide range of user categories. Their main purpose is the dissemination of fake news, the promotion of specific ideas and products, the manipulation of the stock market and even the diffusion of sexually explicit material. Therefore, the early detection of bots in social media is quite essential. In this paper, two methods are introduced targeting this that are mainly based on Natural Language Processing (NLP) to distinguish legitimate users from bots. In the first method, a feature extraction approach is proposed for identifying accounts posting automated messages. After applying feature selection techniques and dealing with imbalanced datasets, the subset of features selected is fed in machine learning algorithms. In the second method, a deep learning architecture is proposed to identify whether tweets have been posted by real users or generated by bots. To the best of the authors’ knowledge, there is no prior work on using an attention mechanism for identifying bots. The introduced approaches have been evaluated over a series of experiments using two large real Twitter datasets and demonstrate valuable advantages over other existing techniques targeting the identification of malicious users in social media.",,,,,
,,"Roussaki, Ioanna",,,,,,,,,Twitter,,,,,,
,,,,,,,,,,,Bot detection,,,,,,
,,,,,,,,,,,Feature extraction,,,,,,
,,,,,,,,,,,Feature selection,,,,,,
,,,,,,,,,,,Machine learning,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Attention mechanism,,,,,,
,,,,,,,,,,,Natural Language Processing,,,,,,
JOUR,Weak-PMLC: A large-scale framework for multi-label policy classification based on extremely weak supervision,"Zhao, Jiufeng",Information Processing & Management,60,103442,2023,2023/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103442,https://www.sciencedirect.com/science/article/pii/S0306457323001796,Natural language processing,"With the development of e-government, multiple local governments in China are developing Internet-based open policy platforms, and these online platforms need to automatically classify policies. Current policy classification methods are usually supervised models that require massive amounts of annotated policies, which can be expensive and difficult to obtain in practice. To alleviate the burden of human experts annotating a large number of policies, we propose a large-scale framework (Weak-PMLC) for multi-label policy classification based on extremely weak supervision, which does not rely on any labeled documents and uses only the label names of each category. Specifically, we first pre-train a language model (LM) on a given dataset to extend the LM from general to domain-specific. We then utilize the domain-specific LM to generate seed words semantically related to label names. Finally, following the category-related seed words, we generate massive pseudo-labeled policies as training data for high-performance supervised models. To verify the effectiveness of our proposed method, we created two new human-labeled datasets containing about 56k and 37k policies, respectively. We also define 59 label names, which are several key feature words used to summarize all collected policies. We show that Weak-PMLC achieves around 90% F1-scores on these two datasets and improves performance by 4% over state-of-the-art weakly supervised methods. Further experiments show that the proposed Weak-PMLC is even comparable to some supervised models.",5,,,,
,,"Song, Rui",,,,,,,,,Weak supervision,,,,,,
,,"Yue, Chitao",,,,,,,,,Policy text classification,,,,,,
,,"Wang, Zhenxin",,,,,,,,,Domain-specific pre-training,,,,,,
,,"Xu, Hao",,,,,,,,,,,,,,,
JOUR,BCMF: A bidirectional cross-modal fusion model for fake news detection,"Yu, Chuanming",Information Processing & Management,59,103063,2022,2022/09/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.103063,https://www.sciencedirect.com/science/article/pii/S0306457322001662,Fake news detection,"In recent years, fake news detection has been a significant task attracting much attention. However, most current approaches utilize the features from a single modality, such as text or image, while the comprehensive fusion between features of different modalities has been ignored. To deal with the above problem, we propose a novel model named Bidirectional Cross-Modal Fusion (BCMF), which comprehensively integrates the textual and visual representations in a bidirectional manner. Specifically, the proposed model is decomposed into four submodules, i.e., the input embedding, the image2text fusion, the text2image fusion, and the prediction module. We conduct intensive experiments on four real-world datasets, i.e., Weibo, Twitter, Politi, and Gossip. The results show 2.2, 2.5, 4.9, and 3.1 percentage points of improvements in classification accuracy compared to the state-of-the-art methods on Weibo, Twitter, Politi, and Gossip, respectively. The experimental results suggest that the proposed model could better capture integrated information of different modalities and has high generalizability among different datasets. Further experiments suggest that the bidirectional fusions, the number of multi-attention heads, and the aggregating function could impact the performance of the cross-modal fake news detection. The research sheds light on the role of bidirectional cross-modal fusion in leveraging multi-modal information to improve the effect of fake news detection.",5,,,,
,,"Ma, Yinxue",,,,,,,,,Cross-modal fusion,,,,,,
,,"An, Lu",,,,,,,,,Contextualized embedding,,,,,,
,,"Li, Gang",,,,,,,,,Deep learning,,,,,,
JOUR,SEEC and CHASE: An emotion-cause pair-oriented approach and conversational dataset with heterogeneous emotions for empathetic response generation,"Gupta, Srishti",Knowledge-Based Systems,280,111039,2023,2023/11/25/,0950-7051,https://doi.org/10.1016/j.knosys.2023.111039,https://www.sciencedirect.com/science/article/pii/S095070512300789X,Empathetic chatbots,"Empathetic Response Generation (ERG) in dialog agents has gained tremendous attention in the recent past. Among various methods used to impart empathy in the generated response, the contemporary one is using the emotion cause. Previous works of ERG have 2 major flaws: (1) usage of the emotion detected using the entire conversation to generate the empathetic response is heavily misleading as the speaker might be going through a completely different emotion in his last utterance. This erroneous emotion detection, in turn, leads to incorrect detection of the cause of that emotion. (2) Usage of entire utterance has proven to be inefficient in the cause extraction task. Consequently, existing works fail to capture multiple emotion clauses and their corresponding emotion causes for efficient ERG. In our work, we introduce a new dataset,11The dataset will be made available at https://drive.google.com/drive/folders/1ozYeuACo_He75w0XoNWZntMZ3FkiTHPB?usp=sharing. CHASE, which is a compilation of conversations extracted from various plays to highlight the above-mentioned change in emotion and how one can show empathy in such a case. In this dataset, we use Dr. Brene Brown’s 22https://brenebrown.com/. notion of empathy on how to administer this change in emotion in the response and still sound empathetic by generating golden responses for each conversation. To address the aforementioned flaws, we also propose a model, SEEC, that utilizes the Emotion-Cause Pair Extraction task on the conversation to find various {emotion clause, cause clause} pairs and use these to impart empathy appropriately to the responses. Our qualitative and quantitative results prove the efficiency in generating enhanced empathetic responses of both SEEC and CHASE.",,,,,
,,"Dandapat, Sourav Kumar",,,,,,,,,Response generation,,,,,,
,,,,,,,,,,,Dialogue systems,,,,,,
,,,,,,,,,,,Dataset creation,,,,,,
JOUR,Towards Improvement of Grounded Cross-lingual Natural Language Inference with VisioTextual Attention,"De, Arkadipta",Natural Language Processing Journal,4,100023,2023,2023/09/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100023,https://www.sciencedirect.com/science/article/pii/S2949719123000201,Attention mechanism,"Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal cross-lingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI.",,,,,
,,"Desarkar, Maunendra Sankar",,,,,,,,,Textual entailment,,,,,,
,,"Ekbal, Asif",,,,,,,,,Natural Language Inference,,,,,,
,,,,,,,,,,,Grounded textual entailment,,,,,,
,,,,,,,,,,,Cross-lingual textual entailment,,,,,,
JOUR,Using Bidirectional Encoder Representations from Transformers (BERT) to classify traffic crash severity types,"Oliaee, Amir Hossein",Natural Language Processing Journal,3,100007,2023,2023/06/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100007,https://www.sciencedirect.com/science/article/pii/S2949719123000043,Crashes,"Traffic crashes are a critical safety concern. Many studies have attempted to improve traffic safety by performing a wide range of studies on safety topics with the application of diverse statistical and machine learning models. The data elements contained in police-reported crash narrative information are not routinely analyzed with coded and structured crash data. In the recent years, unstructured textual contents in traffic crash narratives have been investigated by many researchers. However, most of these studies are basic text mining applications and often the dataset is limited in size. This study applied an advanced language model Bidirectional Encoder Representations from Transformers (BERT) to classify traffic injury types by using a dataset of over 750,000 unique crash narrative reports. The models have an 84.2% ±0.5 predictive accuracy and an Area Under the receiver operating Curve (AUC) of 0.93 ± 0.06 per class. Overall, the findings can assist safety engineers and analysts in determining the causes of a crash. The classification of crash injury types using a language model like BERT is a valuable tool for identifying additional factors that contribute to crashes, which can identify new areas for safety countermeasures and support the development of new safety strategies.",,,,,
,,"Das, Subasish",,,,,,,,,Safety,,,,,,
,,"Liu, Jinli",,,,,,,,,Severity,,,,,,
,,"Rahman, M. Ashifur",,,,,,,,,Text mining,,,,,,
,,,,,,,,,,,BERT,,,,,,
,,,,,,,,,,,Language model,,,,,,
JOUR,A novel hybrid approach for text encoding: Cognitive Attention To Syntax model to detect online misinformation,"Faye, Géraud",Data & Knowledge Engineering,148,102230,2023,2023/11/01/,0169-023X,https://doi.org/10.1016/j.datak.2023.102230,https://www.sciencedirect.com/science/article/pii/S0169023X23000903,Misinformation detection,"Most approaches for text encoding rely on the attention mechanism, at the core of the transformers architecture and large language models. The understanding of this mechanism is still limited and present inconvenients such as lack of interpretability, large requirements of data and low generalization. Based on current understanding of the attention mechanism, we propose CATS (Cognitive Attention To Syntax), a neurosymbolic attention encoding approach based on the syntactic understanding of texts. This approach has on-par to better performance compared to classical attention and displays expected advantages of neurosymbolic AI such as better functioning with little data and better explainability. This layer has been tested on the task of misinformation detection but is general and could be used in any task involving natural language processing.",,,,,
,,"Ouerdane, Wassila",,,,,,,,,Hybrid AI,,,,,,
,,"Gadek, Guillaume",,,,,,,,,Neurosymbolism,,,,,,
,,"Gahbiche, Souhir",,,,,,,,,Natural Language Processing,,,,,,
,,"Gatepaille, Sylvain",,,,,,,,,Classification,,,,,,
,,,,,,,,,,,Web information systems,,,,,,
JOUR,Tackling cyber-aggression: Identification and fine-grained categorization of aggressive texts on social media using weighted ensemble of transformers,"Sharif, Omar",Neurocomputing,490,462,2022,2022/06/14/,0925-2312,https://doi.org/10.1016/j.neucom.2021.12.022,https://www.sciencedirect.com/science/article/pii/S0925231221018567,Natural language processing,"The pervasiveness of aggressive content in social media has become a serious concern for government organizations and tech companies because of its pernicious societal effects. In recent years, social media has been repeatedly used as a tool to incite communal aggression, spread distorted propaganda, damage social harmony and demean the identity of individuals or a community in the public spaces. Therefore, restraining the proliferation of aggressive content and detecting them has become an urgent duty. Studies of the identification of aggressive content have mostly been done for English and other high-resource languages. Automatic systems developed for those languages can not accurately identify detrimental contents written in regional languages like Bengali. To compensate this insufficiency, this work presents a novel Bengali aggressive text dataset (called ‘BAD’) with two-level annotation. In level-A, 14158 texts are labeled as either aggressive or non-aggressive. While in level-B, 6807 aggressive texts are categorized into religious, political, verbal and gendered aggression classes each having 2217, 2085, 2043 and 462 texts respectively. This paper proposes a weighted ensemble technique including m-BERT, distil-BERT, Bangla-BERT and XLM-R as the base classifiers to identify and classify the aggressive texts in Bengali. The proposed model can readdress the softmax probabilities of the participating classifiers depending on their primary outcomes. This weighting technique has enabled the model to outdo the simple average ensemble and all other machine learning (ML), deep learning (DL) baselines. It has acquired the highest weighted f1-score of 93.43% in the identification task and 93.11% in the categorization task. Dataset developed as the part of this work is available at https://github.com/BAD-Bangla-Aggressive-Text-Dataset",,481,,,
,,"Hoque, Mohammed Moshiul",,,,,,,,,Aggressive text classification,,,,,,
,,,,,,,,,,,Low resource language,,,,,,
,,,,,,,,,,,Bengali aggressive text corpus,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Transformers,,,,,,
,,,,,,,,,,,Ensemble,,,,,,
JOUR,Contextual Embeddings based on Fine-tuned Urdu-BERT for Urdu threatening content and target identification,"Malik, Muhammad Shahid Iqbal",Journal of King Saud University - Computer and Information Sciences,35,101606,2023,2023/07/01/,1319-1578,https://doi.org/10.1016/j.jksuci.2023.101606,https://www.sciencedirect.com/science/article/pii/S131915782300160X,Threatening content,"Identification of threatening text on social media platforms is a challenging task. Contrary to the high-resource languages, the Urdu language has very limited such approaches and the benchmark approach has an issue of inappropriate data annotation. Therefore, we present robust threatening content and target identification as a hierarchical classification model for Urdu tweets. This study investigates the potential of the Urdu-BERT (Bidirectional Encoder Representations from Transformer) language model to learn universal contextualized representations aiming to showcase its usefulness for binary classification tasks of threatening content and target identification. We propose to exploit a pre-trained Urdu-BERT as a transfer learning model after fine-tuning its parameters on a newly designed Urdu corpus from the Twitter platform. The proposed dataset contains 2,400 tweets manually annotated as threatening or non-threatening at the first level and threatening tweets are further categorized into individual or group at the second level. The performance of fine-tuned Urdu-BERT is compared with the benchmark study and other feature models. Experimental results show that the fine-tuned Urdu-BERT model achieves state-of-the-art performance by obtaining 87.5% accuracy and 87.8% F1-score for threatening content identification and 82.5% accuracy and 83.2% F1-score for target identification task. Furthermore, the proposed model outperforms the benchmark study.",7,,,,
,,"Cheema, Uswa",,,,,,,,,Target identification,,,,,,
,,"Ignatov, Dmitry I.",,,,,,,,,Fine-tuned BERT,,,,,,
,,,,,,,,,,,Urdu,,,,,,
,,,,,,,,,,,Twitter,,,,,,
,,,,,,,,,,,Text representation,,,,,,
JOUR,Fusing topology contexts and logical rules in language models for knowledge graph completion,"Lin, Qika",Information Fusion,90,253,2023,2023/02/01/,1566-2535,https://doi.org/10.1016/j.inffus.2022.09.020,https://www.sciencedirect.com/science/article/pii/S1566253522001592,Knowledge graph completion,"Knowledge graph completion (KGC) aims to infer missing facts based on the observed ones, which is significant for many downstream applications. Given the success of deep learning and pre-trained language models (LMs), some LM-based methods are proposed for the KGC task. However, most of them focus on modeling the text of fact triples and ignore the deeper semantic information (e.g., topology contexts and logical rules) that is significant for KG modeling. For such a reason, we propose a unified framework FTL-LM to Fuse Topology contexts and Logical rules in Language Models for KGC, which mainly contains a novel path-based method for topology contexts learning and a variational expectation–maximization (EM) algorithm for soft logical rule distilling. The former utilizes a heterogeneous random-walk to generate topology paths and further reasoning paths that can represent topology contexts implicitly and can be modeled by a LM explicitly. The strategies of mask language modeling and contrastive path learning are introduced to model these topology contexts. The latter implicitly fuses logical rules by a variational EM algorithm with two LMs. Specifically, in the E-step, the triple LM is updated under the supervision of observed triples and valid hidden triples verified by the fixed rule LM. And in the M-step, we fix the triple LM and fine-tune the rule LM to update logical rules. Experiments on three common KGC datasets demonstrate the superiority of the proposed FTL-LM, e.g., it achieves 2.1% and 3.1% Hits@10 improvement over the state-of-the-art LM-based model LP-BERT in the WN18RR and FB15k-237, respectively.",,264,,,
,,"Mao, Rui",,,,,,,,,Information fusion,,,,,,
,,"Liu, Jun",,,,,,,,,Topology context,,,,,,
,,"Xu, Fangzhi",,,,,,,,,Logical rule,,,,,,
,,"Cambria, Erik",,,,,,,,,Language model,,,,,,
JOUR,Exploring unsupervised textual representations generated by neural language models in the context of automatic tweet stream summarization,"Dusart, Alexis",Online Social Networks and Media,37-38,100272,2023,2023/09/01/,2468-6964,https://doi.org/10.1016/j.osnem.2023.100272,https://www.sciencedirect.com/science/article/pii/S2468696423000319,Incremental summarization,"Users are often overwhelmed by the amount of information generated on online social networks and media (OSNEM), in particular Twitter, during particular events. Summarizing the information streams would help them be informed in a reasonable time. In parallel, recent state of the art in summarization has a special focus on deep neural models and pre-trained language models. In this context, we aim at (i) evaluating different pre-trained language model (PLM) to represent microblogs (i.e., tweets), and (ii) to identify the most suitable ones in a summarization context, as well as (iii) to see how neural models can be used knowing the issue of input size limitation of such models. For this purpose, we divided the problem into 3 questions and made experiments on 3 different datasets. Using a simple greedy algorithm, we first compared several pre-trained models for single tweet representation. We then evaluated the quality of the average representation of the stream and sought to use it as a starting point for a neural approach. First results show the interest of using USE and Sentence-BERT representations for tweet stream summarization, as well as the great potential of using the average representation of the stream.",,,,,
,,"Pinel-Sauvagnat, Karen",,,,,,,,,Tweet stream,,,,,,
,,"Hubert, Gilles",,,,,,,,,Twitter,,,,,,
,,,,,,,,,,,Pre-trained language models,,,,,,
JOUR,Offensive language identification in dravidian languages using MPNet and CNN,"Chakravarthi, Bharathi Raja",International Journal of Information Management Data Insights,3,100151,2023,2023/04/01/,2667-0968,https://doi.org/10.1016/j.jjimei.2022.100151,https://www.sciencedirect.com/science/article/pii/S2667096822000945,Offensive language identification,"Social media has effectively replaced traditional forms of communication and marketing. As these platforms allow for the free expression of ideas and facts through text, images, and videos, there exists a significant need to screen them to safeguard people and organisations from objectionable information directed at them. Our work aims to categorise code-mixed social media comments and posts in Tamil, Malayalam, and Kannada into offensive or not offensive at different levels. We present a multilingual MPNet and CNN fusion model for detecting offensive language content directed at an individual (or group) in low-resource Dravidian languages at different levels. Our model is capable of handling data that has been code-mixed, such as Tamil and Latin scripts. The model was successfully validated on the datasets, achieving offensive language detection results better than those of other baseline models with weighted average F1-score of 0.85, 0.98, and 0.76, and performed better than the baseline models EWDT, and EWODT by 0.02, 0.02, 0.04 for Tamil, Malayalam, and Kannada respectively.",1,,,,
,,"Jagadeeshan, Manoj Balaji",,,,,,,,,Dravidian languages,,,,,,
,,"Palanikumar, Vasanth",,,,,,,,,Code-mixing,,,,,,
,,"Priyadharshini, Ruba",,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,MPNet,,,,,,
,,,,,,,,,,,CNN,,,,,,
JOUR,Analyzing Chinese text with clause relevance structure,"Lyu, Chen",Neurocomputing,519,82,2023,2023/01/28/,0925-2312,https://doi.org/10.1016/j.neucom.2022.10.077,https://www.sciencedirect.com/science/article/pii/S0925231222013649,Discourse structure,"Discourse structure is generally represented as hierarchical structure, the two most well known representations are rhetorical structure theory (RST) and Penn discourse treebank (PDTB). The main problem of the hierarchical structure is that it can not describe the direct semantic relevance between the elementary discourse units (EDU), especially the non-adjacent and cross-level EDUs. Discourse dependency structure (DDS) has been put forward in recent years to describe the head-dependent relation between the EDUs. However, the judgment process of the head can not be answered theoretically. This problem is particularly serious in Chinese discourse analysis, because Chinese lacks the form differences between the main clauses and the subordinate clauses. In this paper, we propose clause relevance structure to represent the discourse structure. Compared with the hierarchical discourse structure and DDS, the clause relevance structure can effectively describe the direct semantic association between discontinuous and cross-level clauses in a text, and the construction of structure is not presupposed by the head recognition. We propose the judgment criteria and formal constraints of the clause relevance structure, and built a human-annotated corpus on Chinese text. Based on the Chinese corpus, we explore the automatic recognition of clause relevance structure. The clause relevance recognition task is formalized as a classification problem and performed by the BERT-based model. A bidirectional LSTM layer is added on the top of the BERT to improve the performance, and the recognition accuracy (90.77%) is achieved by the BERT-LSTM model. Experimental results show that the long distance clause pairs are the main difficulties in the clause relevance recognition, and these difficulties mainly focus on the positive examples, while the clause pairs with short distance are especially difficult to be correctly recognized as negative relevance.",,93,,,
,,"Feng, Wenhe",,,,,,,,,Clause relevance structure,,,,,,
,,,,,,,,,,,Corpus construction,,,,,,
,,,,,,,,,,,Clause relevance recognition,,,,,,
JOUR,Transfer language selection for zero-shot cross-lingual abusive language detection,"Eronen, Juuso",Information Processing & Management,59,102981,2022,2022/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2022.102981,https://www.sciencedirect.com/science/article/pii/S0306457322000978,Abusive language detection,"We study the selection of transfer languages for automatic abusive language detection. Instead of preparing a dataset for every language, we demonstrate the effectiveness of cross-lingual transfer learning for zero-shot abusive language detection. This way we can use existing data from higher-resource languages to build better detection systems for low-resource languages. Our datasets are from seven different languages from three language families. We measure the distance between the languages using several language similarity measures, especially by quantifying the World Atlas of Language Structures. We show that there is a correlation between linguistic similarity and classifier performance. This discovery allows us to choose an optimal transfer language for zero shot abusive language detection.",4,,,,
,,"Ptaszynski, Michal",,,,,,,,,Zero-shot learning,,,,,,
,,"Masui, Fumito",,,,,,,,,Transfer learning,,,,,,
,,"Arata, Masaki",,,,,,,,,Linguistics,,,,,,
,,"Leliwa, Gniewosz",,,,,,,,,,,,,,,
,,"Wroczynski, Michal",,,,,,,,,,,,,,,
JOUR,Improving question answering performance using knowledge distillation and active learning,"Boreshban, Yasaman",Engineering Applications of Artificial Intelligence,123,106137,2023,2023/08/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.106137,https://www.sciencedirect.com/science/article/pii/S0952197623003214,Natural language processing,"Contemporary question answering (QA) systems, including Transformer-based architectures, suffer from increasing computational and model complexity which render them inefficient for real-world applications with limited resources. Furthermore, training or even fine-tuning such models requires a vast amount of labeled data which is often not available for the task at hand. In this manuscript, we conduct a comprehensive analysis of the mentioned challenges and introduce suitable countermeasures. We propose a novel knowledge distillation (KD) approach to reduce the parameter and model complexity of a pre-trained bidirectional encoder representations from transformer (BERT) system and utilize multiple active learning (AL) strategies for immense reduction in annotation efforts. We show the efficacy of our approach by comparing it with four state-of-the-art (SOTA) Transformers-based systems, namely KroneckerBERT, EfficientBERT, TinyBERT, and DistilBERT. Specifically, we outperform KroneckerBERT21 and EfficientBERTTINY by 4.5 and 0.4 percentage points in EM, despite having 75.0% and 86.2% fewer parameters, respectively. Additionally, our approach achieves comparable performance to 6-layer TinyBERT and DistilBERT while using only 2% of their total trainable parameters. Besides, by the integration of our AL approaches into the BERT framework, we show that SOTA results on the QA datasets can be achieved when we only use 40% of the training data. Overall, all results demonstrate the effectiveness of our approaches in achieving SOTA performance, while extremely reducing the number of parameters and labeling efforts. Finally, we make our code publicly available at https://github.com/mirbostani/QA-KD-AL.",,,,,
,,"Mirbostani, Seyed Morteza",,,,,,,,,Question answering,,,,,,
,,"Ghassem-Sani, Gholamreza",,,,,,,,,Deep learning,,,,,,
,,"Mirroshandel, Seyed Abolghasem",,,,,,,,,Knowledge distillation,,,,,,
,,"Amiriparian, Shahin",,,,,,,,,Active learning,,,,,,
,,,,,,,,,,,Performance,,,,,,
JOUR,FALCoN: Detecting and classifying abusive language in social networks using context features and unlabeled data,"Tuarob, Suppawong",Information Processing & Management,60,103381,2023,2023/07/01/,0306-4573,https://doi.org/10.1016/j.ipm.2023.103381,https://www.sciencedirect.com/science/article/pii/S0306457323001188,Abusive language detection,"Social networks have grown into a widespread form of communication that allows a large number of users to participate in conversations and consume information at any time. The casual nature of social media allows for nonstandard terminology, some of which may be considered rude and derogatory. As a result, a significant portion of social media users is found to express disrespectful language. This problem may intensify in certain developing countries where young children are granted unsupervised access to social media platforms. Furthermore, the sheer amount of social media data generated daily by millions of users makes it impractical for humans to monitor and regulate inappropriate content. If adolescents are exposed to these harmful language patterns without adequate supervision, they may feel obliged to adopt them. In addition, unrestricted aggression in online forums may result in cyberbullying and other dreadful occurrences. While computational linguistics research has addressed the difficulty of detecting abusive dialogues, issues remain unanswered for low-resource languages with little annotated data, leading the majority of supervised techniques to perform poorly. In addition, social media content is often presented in complex, context-rich formats that encourage creative user involvement. Therefore, we propose to improve the performance of abusive language detection and classification in a low-resource setting, using both the abundant unlabeled data and the context features via the co-training protocol that enables two machine learning models, each learning from an orthogonal set of features, to teach each other, resulting in an overall performance improvement. Empirical results reveal that our proposed framework achieves F1 values of 0.922 and 0.827, surpassing the state-of-the-art baselines by 3.32% and 45.85% for binary and fine-grained classification tasks, respectively. In addition to proving the efficacy of co-training in a low-resource situation for abusive language detection and classification tasks, the findings shed light on several opportunities to use unlabeled data and contextual characteristics of social networks in a variety of social computing applications.",4,,,,
,,"Satravisut, Manisa",,,,,,,,,Social networking service,,,,,,
,,"Sangtunchai, Pochara",,,,,,,,,Co-training,,,,,,
,,"Nunthavanich, Sakunrat",,,,,,,,,Ethical natural language processing,,,,,,
,,"Noraset, Thanapon",,,,,,,,,,,,,,,
JOUR,Improving task-agnostic BERT distillation with layer mapping search,"Jiao, Xiaoqi",Neurocomputing,461,194,2021,2021/10/21/,0925-2312,https://doi.org/10.1016/j.neucom.2021.07.050,https://www.sciencedirect.com/science/article/pii/S0925231221010948,Pre-trained language models,"Knowledge distillation (KD) which transfers the knowledge from a large teacher model to a small student model, has been widely used to compress the BERT model recently. Besides the supervision in the output in the original KD, recent works show that layer-level supervision is crucial to the performance of the student BERT model. However, previous works designed the layer mapping strategy heuristically (e.g., uniform or last-layer), which can lead to inferior performance. In this paper, we propose to use the genetic algorithm (GA) to search for the optimal layer mapping automatically. To accelerate the search process, we further propose a proxy setting where a small portion of the training corpus are sampled for distillation, and three representative tasks are chosen for evaluation. After obtaining the optimal layer mapping, we perform the task-agnostic BERT distillation with it on the whole corpus to build a compact student model, which can be directly fine-tuned on downstream tasks. Comprehensive experiments on the evaluation benchmarks demonstrate that 1) layer mapping strategy has a significant effect on task-agnostic BERT distillation and different layer mappings can result in quite different performances; 2) the optimal layer mapping strategy from the proposed search process consistently outperforms the other heuristic ones; 3) with the optimal layer mapping, our student model achieves state-of-the-art performance on the GLUE tasks.",,203,,,
,,"Chang, Huating",,,,,,,,,Bert,,,,,,
,,"Yin, Yichun",,,,,,,,,Knowledge distillation,,,,,,
,,"Shang, Lifeng",,,,,,,,,Task-agnostic,,,,,,
,,"Jiang, Xin",,,,,,,,,Layer mapping,,,,,,
,,"Chen, Xiao",,,,,,,,,,,,,,,
,,"Li, Linlin",,,,,,,,,,,,,,,
,,"Wang, Fang",,,,,,,,,,,,,,,
,,"Liu, Qun",,,,,,,,,,,,,,,
JOUR,Textual entailment as an evaluation metric for abstractive text summarization,"Bhuyan, Swagat Shubham",Natural Language Processing Journal,4,100028,2023,2023/09/01/,2949-7191,https://doi.org/10.1016/j.nlp.2023.100028,https://www.sciencedirect.com/science/article/pii/S2949719123000250,Automatic evaluation,"Automated text summarization systems require to be heedful of the reader and the communication goals since it may be the determining component of whether the original textual content is actually worth reading in full. The summary can also assist enhance document indexing for information retrieval, and it is generally much less biased than a human-written summary. A crucial part while building intelligent systems is evaluating them. Consequently, the choice of evaluation metric(s) is of utmost importance. Current standard evaluation metrics like BLEU and ROUGE, although fairly effective for evaluation of extractive text summarization systems, become futile when it comes to comparing semantic information between two texts, i.e in abstractive summarization. We propose textual entailment as a potential metric to evaluate abstractive summaries. The results show the contribution of text entailment as a strong automated evaluation model for such summaries. The textual entailment scores between the text and generated summaries, and between the reference and predicted summaries were calculated, and an overall summarizer score was generated to give a fair idea of how efficient the generated summaries are. We put forward some novel methods that use the entailment scores and the final summarizer scores for a reasonable evaluation of the same across various scenarios. A Final Entailment Metric Score (FEMS) was generated to get an insightful idea in order to compare both the generated summaries.",,,,,
,,"Mahanta, Saranga Kingkor",,,,,,,,,Abstractive text summarization,,,,,,
,,"Pakray, Partha",,,,,,,,,Text entailment,,,,,,
,,"Favre, Benoit",,,,,,,,,Natural language processing,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
JOUR,Negation-based transfer learning for improving biomedical Named Entity Recognition and Relation Extraction,"Fabregat, Hermenegildo",Journal of Biomedical Informatics,138,104279,2023,2023/02/01/,1532-0464,https://doi.org/10.1016/j.jbi.2022.104279,https://www.sciencedirect.com/science/article/pii/S1532046422002842,Transfer learning,Background and Objectives:,,,,,
,,"Duque, Andres",,,,,,,,,Named Entity Recognition,"Named Entity Recognition (NER) and Relation Extraction (RE) are two of the most studied tasks in biomedical Natural Language Processing (NLP). The detection of specific terms and entities and the relationships between them are key aspects for the development of more complex automatic systems in the biomedical field. In this work, we explore transfer learning techniques for incorporating information about negation into systems performing NER and RE. The main purpose of this research is to analyse to what extent the successful detection of negated entities in separate tasks helps in the detection of biomedical entities and their relationships.",,,,,
,,"Martinez-Romo, Juan",,,,,,,,,Negation detection,Methods:,,,,,
,,"Araujo, Lourdes",,,,,,,,,Relation Extraction,"Three neural architectures are proposed in this work, all of them mainly based on Bidirectional Long Short-Term Memory (Bi-LSTM) networks and Conditional Random Fields (CRFs). While the first architecture is devoted to detecting triggers and scopes of negated entities in any domain, two specific models are developed for performing isolated NER tasks and joint NER and RE tasks in the biomedical domain. Then, weights related to negation detection learned by the first architecture are incorporated into those last models. Two different languages, Spanish and English, are taken into account in the experiments.",,,,,
,,,,,,,,,,,,Results:,,,,,
,,,,,,,,,,,,"Performance of the biomedical models is analysed both when the weights of the neural networks are randomly initialized, and when weights from the negation detection model are incorporated into them. Improvements of around 3.5% of F-Measure in the English language and more than 7% in the Spanish language are achieved in the NER task, while the NER+RE task increases F-Measure scores by more than 13% for the NER submodel and around 2% for the RE submodel.",,,,,
,,,,,,,,,,,,Conclusions:,,,,,
,,,,,,,,,,,,The obtained results allow us to conclude that negation-based transfer learning techniques are appropriate for performing biomedical NER and RE tasks. These results highlight the importance of detecting negation for improving the identification of biomedical entities and their relationships. The explored techniques show robustness by maintaining consistent results and improvements across different tasks and languages.,,,,,
JOUR,Judicial nested named entity recognition method with MRC framework,"Zhang, Hu",International Journal of Cognitive Computing in Engineering,4,118,2023,2023/06/01/,2666-3074,https://doi.org/10.1016/j.ijcce.2023.03.002,https://www.sciencedirect.com/science/article/pii/S2666307423000128,Named entity recognition,"Judicial named entity recognition (JNER) is a basic task of judicial intelligence and judicial service informatization. At present, the research of JNER has attracted extensive attention. However, the existing JNER methods usually can only assign a single label to a token in the input sequence, which is not applicable to nested entities where a token may be assigned two or more different labels at the same time. Therefore, this paper introduces the machine reading comprehension (MRC) framework into JNER, and proposes a judicial nested NER method based on the MRC. Firstly, we design the question template according to the characteristics of judicial nested named entities, and construct the legal text named entity dataset in MRC format. Next, we introduce the span extraction MRC model based on the pre-trained to encode the question and text, and learn the context knowledge of the entity in the question. Finally, we extract the starting and end positions of the matching span respectively through two classifiers, to get the corresponding entities. The experimental results on the information extraction dataset in “CAIL2021” show, compared with the existing baseline models, the proposed method effectively improves the recognition effect of nested entities commonly existing in the judicial field.",,126,,,
,,"Guo, Jiayu",,,,,,,,,Machine reading comprehension,,,,,,
,,"Wang, Yujie",,,,,,,,,Nested named entities,,,,,,
,,"Zhang, Zhen",,,,,,,,,Wisdom justice,,,,,,
,,"Zhao, Hansen",,,,,,,,,,,,,,,
JOUR,Phrase embedding learning from internal and external information based on autoencoder,"Li, Rongsheng",Information Processing & Management,58,102422,2021,2021/01/01/,0306-4573,https://doi.org/10.1016/j.ipm.2020.102422,https://www.sciencedirect.com/science/article/pii/S030645732030916X,Phrase embedding,"Phrase embedding can improve the performance of multiple NLP tasks. Most of the previous phrase-embedding methods that only use the external or internal semantic information of phrases to learn phrase embedding are challenging to solve the problem of data sparseness and have poor semantic presentation ability. To solve the above issues, in this paper, we propose an autoencoder-based method to combine pre-trained phrase embeddings and phrase component word embeddings into new phrase embeddings through complex non-linear transformations. This method uses both internal and external semantic information of phrases to generate new phrases with better semantic expression capabilities. This method can also generate well-represented phrase embeddings when only pre-trained component word embeddings are used as input to solve the problem of data sparseness effectively. We have designed two models for this method. The first one uses an FCNN(Fully Connected Neural Network) as the encoder and decoder, which we call AE-F. The second one uses the attention mechanism shared by the parameters of encoder and decoder to proportionally allocate the outputs of an LSTM and an FCNN, which we call it AE-ALF. We evaluated them in terms of phrase similarity and phrase classification and used two English datasets and two Chinese datasets. Experimental results show that AE-F and AE-ALF methods using pre-trained phrase embeddings and component word embeddings exceed 17 baseline methods, and AE-F and AE-ALF perform similarly. With only pre-trained component word embeddings, AE-F and AE-ALF also exceed most baseline methods, and AE-ALF performs better than AE-F.",1,,,,
,,"Yu, Qinyong",,,,,,,,,Aautoencoder,,,,,,
,,"Huang, Shaobin",,,,,,,,,LSTM,,,,,,
,,"Shen, Linshan",,,,,,,,,Attention mechanism,,,,,,
,,"Wei, Chi",,,,,,,,,,,,,,,
,,"Sun, Xuewei",,,,,,,,,,,,,,,
JOUR,DNCP: An attention-based deep learning approach enhanced with attractiveness and timeliness of News for online news click prediction,"Xiong, Jie",Information & Management,58,103428,2021,2021/03/01/,0378-7206,https://doi.org/10.1016/j.im.2021.103428,https://www.sciencedirect.com/science/article/pii/S0378720621000021,News click prediction,"Predicting news clicks or popularity is of great importance to news providers and recommender systems. Attractiveness and timeliness of news are two prominent drivers of news clicks. Attractiveness represents the appealingness or interestingness of news to an individual, while timeliness indicates the recency of news. Existing research on news click prediction models has ignored those two important variables. There is a lack of exploration and understanding of how to represent them in a predictive model and how effective and valuable they are to the prediction performance of a model. To fill these gaps, in this research, we propose a deep news click prediction (DNCP) model that integrates attractiveness and timeliness of news in an attention-based deep neural network for predicting news clicks. We also propose new measures for those two variables. Empirical evaluation using two real-world datasets shows that the DNCP model outperforms a variety of baseline models. The findings of this research provide several novel research contributions and practical implications for improving news click prediction.",2,,,,
,,"Yu, Li",,,,,,,,,Deep learning,,,,,,
,,"Zhang, Dongsong",,,,,,,,,Attention mechanism,,,,,,
,,"Leng, Youfang",,,,,,,,,Topic model,,,,,,
JOUR,Metapath and syntax-aware heterogeneous subgraph neural networks for spam review detection,"Zhang, Zhiqiang",Applied Soft Computing,128,109438,2022,2022/10/01/,1568-4946,https://doi.org/10.1016/j.asoc.2022.109438,https://www.sciencedirect.com/science/article/pii/S1568494622005609,Spam detection,"Spam Review Detection is a subclass of text classification that aims to distinguish genuine reviews from spam reviews (e.g., irrelevant reviews, deceptive reviews, machine-generated reviews, and non-review messages). Previous studies have focused on review text analysis, abnormal behavior detection, and intrinsic relationship identification. However, these methods ignore different fraudulent camouflages and writing styles. In this paper, we instead design a Spam detection model Metapath-based Subgraph Aggregated Neural Network (Spam-MSANN) integrating three metapath-based subgraphs (i.e. User-Item Subgraph, Review Subgraph, and User-Review-Item Subgraph) and syntactic information to enhance the relevant representation of the review information with subgraph aggregation operations. Experimental results on benchmarking datasets (i.e. YelpCHI and Amazon) demonstrate that our Spam-MSANN model significantly improves the state-of-the-art models. Specifically, Spam-MSANN outperforms 11 of the 12 advanced benchmark models on these two datasets, which further manifests that the fusion of different metapath-based subgraphs and syntactic information is adequate for the spam review detection task.",,,,,
,,"Dong, Yuhang",,,,,,,,,Syntactic parse,,,,,,
,,"Wu, Haiyan",,,,,,,,,Semantic parse,,,,,,
,,"Song, Haiyu",,,,,,,,,Metapath,,,,,,
,,"Deng, Shengchun",,,,,,,,,Heterogeneous graph,,,,,,
,,"Chen, Yanhong",,,,,,,,,,,,,,,
JOUR,Prompt learning for metonymy resolution: Enhancing performance with internal prior knowledge of pre-trained language models,"Zhao, Biao",Knowledge-Based Systems,279,110928,2023,2023/11/04/,0950-7051,https://doi.org/10.1016/j.knosys.2023.110928,https://www.sciencedirect.com/science/article/pii/S0950705123006780,Metonymy resolution,"Linguistic metonymy is a common type of figurative language in natural language processing (NLP), where a concept is represented by a closely associated word or phrase, for example “business executives suits”. As a result, metonymy resolution has become an important NLP task aimed at correctly identifying metonymic expressions within sentences. Previous approaches to this task have typically relied on pre-trained language models (PLMs) using a fine-tuning process. However, this can be time-consuming and resource-intensive, and may lead to a loss of factual prior knowledge. The emergence of a novel learning paradigm termed “prompt learning” or “prompt-tuning” has recently sparked widespread interest and captured considerable attention, as it has proven to yield remarkable results and surpass previous benchmarks. This approach uses a “pre-train→prompt→predict” paradigm and has been shown to better utilize the internal prior knowledge of a PLM, especially in situations with limited supervised resources. Inspired by this success, we investigated how prompt learning could improve metonymy resolution. We have developed a series of prompt learning approaches, called PromptMR, for metonymy resolution, and applied them to several widely-used metonymy resolution datasets. We also designed additional prompt-tuning augmentation strategies to further enhance the potential of prompt learning. Our experiments demonstrated that our method achieved state-of-the-art performance over multiple competitive baselines in both data-sufficient and data-scarce scenarios. The code implementations for PromptMR are accessible on GitHub via the URL: https://github.com/albert-jin/PromptTuning2MetonymyResolution.",,,,,
,,"Jin, Weiqiang",,,,,,,,,Prompt learning,,,,,,
,,"Zhang, Yu",,,,,,,,,Pre-trained language model,,,,,,
,,"Huang, Subin",,,,,,,,,Prompting template engineering,,,,,,
,,"Yang, Guang",,,,,,,,,Answer engineering,,,,,,
JOUR,"Pre-trained models: Past, present and future","Han, Xu",AI Open,2,225,2021,2021/01/01/,2666-6510,https://doi.org/10.1016/j.aiopen.2021.08.002,https://www.sciencedirect.com/science/article/pii/S2666651021000231,Pre-trained models,"Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.",,250,,,
,,"Zhang, Zhengyan",,,,,,,,,Language models,,,,,,
,,"Ding, Ning",,,,,,,,,Transfer learning,,,,,,
,,"Gu, Yuxian",,,,,,,,,Self-supervised learning,,,,,,
,,"Liu, Xiao",,,,,,,,,Natural language processing,,,,,,
,,"Huo, Yuqi",,,,,,,,,Multimodal processing,,,,,,
,,"Qiu, Jiezhong",,,,,,,,,Artificial intelligence,,,,,,
,,"Yao, Yuan",,,,,,,,,,,,,,,
,,"Zhang, Ao",,,,,,,,,,,,,,,
,,"Zhang, Liang",,,,,,,,,,,,,,,
,,"Han, Wentao",,,,,,,,,,,,,,,
,,"Huang, Minlie",,,,,,,,,,,,,,,
,,"Jin, Qin",,,,,,,,,,,,,,,
,,"Lan, Yanyan",,,,,,,,,,,,,,,
,,"Liu, Yang",,,,,,,,,,,,,,,
,,"Liu, Zhiyuan",,,,,,,,,,,,,,,
,,"Lu, Zhiwu",,,,,,,,,,,,,,,
,,"Qiu, Xipeng",,,,,,,,,,,,,,,
,,"Song, Ruihua",,,,,,,,,,,,,,,
,,"Tang, Jie",,,,,,,,,,,,,,,
,,"Wen, Ji-Rong",,,,,,,,,,,,,,,
,,"Yuan, Jinhui",,,,,,,,,,,,,,,
,,"Zhao, Wayne Xin",,,,,,,,,,,,,,,
,,"Zhu, Jun",,,,,,,,,,,,,,,
JOUR,Multi-hop question answering using sparse graphs,"Hemmati, Nima",Engineering Applications of Artificial Intelligence,126,107128,2023,2023/11/01/,0952-1976,https://doi.org/10.1016/j.engappai.2023.107128,https://www.sciencedirect.com/science/article/pii/S095219762301312X,Natural language processing,"Multi-hop question answering (QA) across multiple documents requires a deep understanding of relationships between entities in documents, questions, and answer candidates. Graph Neural Networks (GNNs) have emerged as a promising tool for multi-hop QA tasks. These models often suffer from increasing computational and model complexity, which makes them inefficient for real-world applications with limited resources. In this paper, we propose a graph-based approach called Sparse Graph-based Multi-hop Question Answering system (SG-MQA), which provides a throughout examination of the mentioned challenges and presents appropriate measures to address them. We propose a novel approach based on the Relational Graph Convolutional Network (R-GCN) that reduces the model complexity and improves its performance. We have utilized various strategies and conducted multiple experiments to achieve this goal. We show the efficacy of the proposed approach by examining the results of experiments on two QA datasets, namely WikiHop and HotpotQA. The SG-MQA model outperforms all the state-of-the-art (SOTA) methods on WikiHop and increases the accuracy of the best previous approach from 74.4% to 78.3%. Additionally, it achieves acceptable performance on HotpotQA. Although, according to the F1 measure, the performance of SG-MQA is inferior to that of the SOTA model, it is comparable to that of all other approaches. On the other hand, based on the Exact Match (EM) measure, SG-MQA shows comparable performance to that of the SOTA model and outperforms all other approaches.",,,,,
,,"Ghassem-Sani, Gholamreza",,,,,,,,,Multi-hop question answering,,,,,,
,,,,,,,,,,,Deep learning,,,,,,
,,,,,,,,,,,Graph convolutional network,,,,,,
,,,,,,,,,,,Attention mechanism,,,,,,